<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>L10178 on XLabs</title><link>https://www.xlabs.club/contributors/l10178/</link><description>Recent content in L10178 on XLabs</description><generator>Hugo</generator><language>zh</language><copyright>Copyright (c) 2020-2024 XLabs Club</copyright><lastBuildDate>Fri, 10 May 2024 23:11:22 +0800</lastBuildDate><atom:link href="https://www.xlabs.club/contributors/l10178/index.xml" rel="self" type="application/rss+xml"/><item><title>使用 Pulumi 部署 cert-manager 创建 K8S 自签名证书并信任证书</title><link>https://www.xlabs.club/blog/trust-cert-manager-selfsigned-tls/</link><pubDate>Mon, 29 Apr 2024 21:49:22 +0800</pubDate><guid>https://www.xlabs.club/blog/trust-cert-manager-selfsigned-tls/</guid><description>在搭建本地 Kubernetus 集群后，由于环境在内网，做不了域名验证，无法使用 Let&amp;rsquo;s Encrypt 颁发和自动更新证书，然而很多应用要求必须启用 HTTPS，只能用自签名 CA 证书，并由此 CA 继续颁发其他证书。
所以我们准备了以下工具，开始搭建。
Pulumi: 当前非常流行的 IaC 工具，值得一试。 cert-manager: 云原生证书管理，用于自动管理和颁发各种发行来源的 TLS 证书。它将确保证书有效并定期更新，并尝试在到期前的适当时间更新证书。 核心步骤和相关代码如下，更多源码请参考我们的 GitHub 项目 xlabs-ops。
使用 Pulumi 安装 cert-manager，生成自签名 CA 证书，根据自签名 CA 证书生成 cert-manager ClusterIssuer，都在如下代码了。
import * as pulumi from &amp;#34;@pulumi/pulumi&amp;#34;; import * as kubernetes from &amp;#34;@pulumi/kubernetes&amp;#34;; import * as tls from &amp;#34;@pulumi/tls&amp;#34;; // 部署 cert-manager Helm chart const certManagerRelease = new kubernetes.helm.v3.Release(&amp;#34;cert-manager&amp;#34;, { name: &amp;#34;cert-manager&amp;#34;, chart: &amp;#34;cert-manager&amp;#34;, version: &amp;#34;1.14.5&amp;#34;, namespace: &amp;#34;cert-manager&amp;#34;, createNamespace: true, timeout: 600, repositoryOpts: { repo: &amp;#34;https://charts.</description></item><item><title>Mac 搭建本地 K8S 开发环境方案选型</title><link>https://www.xlabs.club/blog/easiest-k8s-on-macos/</link><pubDate>Sat, 13 Apr 2024 15:20:43 +0800</pubDate><guid>https://www.xlabs.club/blog/easiest-k8s-on-macos/</guid><description>因为工作经常需要用到 K8S，而且有时因网络原因不能完全依赖公司网络，或者因为测试新功能不能直接发布到公司集群，所以就有了本地搭建 K8S 的需求。
另外如果你有以下需求，此文档中提到的方案也许有所帮助：
开发机器模拟 Arm、AMD64 等不同架构。 完全隔离的不同环境，比如为测试 docker、podman、buildkit、containd 等不同软件设置的独立环境。 CI/CD 流程中即用即消的轻量级虚拟机替代方案。 有限的资源模拟大批量的 K8S 节点。 以下介绍一下我用过的几种不同方案，有些纯属个人观点仅供参考。
Docker Desktop 并启用 Kubernetes 功能。
优点：最简单，开箱即用。
缺点：只支持单节点 K8S，且 K8S 部分功能不支持，不易定制。
Docker run K3D, K3D run K3S。
优点：简单，任何支持 docker 的工具（Rancher Desktop、Podman） 启动一个容器即可。
缺点：只支持 K3S。
multipass 启动虚拟机安装 K8S 或 K3S。
优点：multipass 可启动空白 ubuntu 虚拟机，或者启动已经安装好 minikube 的虚拟机。
缺点：只支持 ubuntu，虚拟机与宿主机同架构。
lima 启动虚拟机安装 K8S 或 K3S。
优点：支持虚拟多种 Linux，支持异构虚拟机，支持 contained 可代替 docker。
缺点：架构稍复杂，启动略慢，不如 multipass 稳定，不支持运行在 Windows。
以上方案，在网络畅通的情况下，均能在 10 分钟内启动一个单节点 K8S，所以整体方案都不复杂。</description></item><item><title>K8S 服务长连接负载不均衡问题分析和解决办法</title><link>https://www.xlabs.club/blog/tomcat-keepalive-load-balancer/</link><pubDate>Thu, 11 Apr 2024 21:05:46 +0800</pubDate><guid>https://www.xlabs.club/blog/tomcat-keepalive-load-balancer/</guid><description>问题背景，我们有一个 Http 服务在 K8S 内部署了 3 个 Pod，客户端使用 Service NodePort 进行连接，发现流量几乎都集中到了一个 Pod 上。
已知的情况是：
K8S Service 使用 round-robin 负载均衡策略。 客户端和服务端都启用了 Keep-Alive 长连接。 经过抓包分析，负载较高的 Pod 保持着较多 KeepAlive 长连接。将 kube-proxy 的 ipvs 转发模式设置为 Least-Connection，即倾向转发给连接数少的 Pod，可能会有所缓解，但也不一定，因为 ipvs 的负载均衡状态是分散在各个节点的，并没有收敛到一个地方，也就无法在全局层面感知哪个 Pod 上的连接数少，并不能真正做到 Least-Connection。
服务端主动要求断开长连接 客户端连接我们可能无法控制，那么如何从服务端主动断开长连接。
以 Tomcat 为例，它提供了 maxKeepAliveRequests 参数，到达此参数阈值后，Tomcat 会在 Response Header 中主动加一个 Connection: close，正常情况下客户端接收到此响应后会主动断开长连接。
对于其他不支持此参数的服务器，可以自定义 Filter 或者自定代码，到达某阈值后在 Response Header 中主动追加 Connection: close。
对于 Spring Boot 可通过 properties 配置。
# Spring Boot Tomcat server.tomcat.max-keep-alive-requests=100 # Spring Boot WebFlux server.</description></item><item><title>MacOS 固化配置，彻底解决 too many open files in system 的问题</title><link>https://www.xlabs.club/blog/macos-too-many-open-files/</link><pubDate>Tue, 19 Mar 2024 23:05:07 +0800</pubDate><guid>https://www.xlabs.club/blog/macos-too-many-open-files/</guid><description>作为一个开发者，经常在 MacOS 遇到 Too many open files in system 的报错，尤其是碰到黑洞 node_modules 时，如何固化配置彻底解决，直接上代码。
输入 launchctl limit 即可看到当前的限制，我这里 maxfiles 是改过以后的。
$ launchctl limit cpu unlimited unlimited filesize unlimited unlimited data unlimited unlimited stack 8388608 67104768 core 0 unlimited rss unlimited unlimited memlock unlimited unlimited maxproc 1392 2088 maxfiles 10240 102400 开始创建文件 sudo vi /Library/LaunchDaemons/limit.maxfiles.plist ，内容如下，可根据自己爱好改后面的两个数字值。
&amp;lt;?xml version=&amp;#34;1.0&amp;#34; encoding=&amp;#34;UTF-8&amp;#34;?&amp;gt; &amp;lt;!DOCTYPE plist PUBLIC &amp;#34;-//Apple//DTD PLIST 1.0//EN&amp;#34; &amp;#34;http://www.apple.com/DTDs/PropertyList-1.0.dtd&amp;#34;&amp;gt; &amp;lt;plist version=&amp;#34;1.0&amp;#34;&amp;gt; &amp;lt;dict&amp;gt; &amp;lt;key&amp;gt;Label&amp;lt;/key&amp;gt; &amp;lt;string&amp;gt;limit.maxfiles&amp;lt;/string&amp;gt; &amp;lt;key&amp;gt;ProgramArguments&amp;lt;/key&amp;gt; &amp;lt;array&amp;gt; &amp;lt;string&amp;gt;launchctl&amp;lt;/string&amp;gt; &amp;lt;string&amp;gt;limit&amp;lt;/string&amp;gt; &amp;lt;string&amp;gt;maxfiles&amp;lt;/string&amp;gt; &amp;lt;string&amp;gt;10240&amp;lt;/string&amp;gt; &amp;lt;string&amp;gt;102400&amp;lt;/string&amp;gt; &amp;lt;/array&amp;gt; &amp;lt;key&amp;gt;RunAtLoad&amp;lt;/key&amp;gt; &amp;lt;true/&amp;gt; &amp;lt;key&amp;gt;ServiceIPC&amp;lt;/key&amp;gt; &amp;lt;false/&amp;gt; &amp;lt;/dict&amp;gt; &amp;lt;/plist&amp;gt;验证文件格式和内容，并应用生效。</description></item><item><title>Spring Boot Start 脚手架定制开发和快速入门</title><link>https://www.xlabs.club/blog/spring-boot-start-site/</link><pubDate>Sat, 09 Mar 2024 14:29:03 +0800</pubDate><guid>https://www.xlabs.club/blog/spring-boot-start-site/</guid><description>介绍基于 start.spring.io 快速定制自己的 Spring Boot 脚手架，主要应用场景：
规范公司自己的 parent pom，增加特定的依赖项。 根据公司规范生成统一的包结构，统一命名。 根据需要增加特定代码或文件，比如根据公司要求统一 logback.xml、 application.properties 文件。 提供公司自研的二方 jar 包。 快速开始 基本步骤：
对于 spring.initializr 我们没有定制的需求，直接引用官方的。 拷贝一份 start.spring.io，直接基于这个项目开发、部署、运行。以下都是关于如何修改 start.spring.io。 start.spring.io 主要关注两个模块：
start-client：前端页面，可以定制些自己的 logo、title 等。 start-site：是一个标准的 spring boot 项目，实际 run 起来的服务，引用了 start-client，直接 run 这个项目的 main 方法就能看到效果。 主要配置文件：start-site/src/main/resources/application.yml，通过修改这个配置文件可以达到的效果如下。
修改 start 启动时默认 group，把 com.example 改为公司自己的 group。
initializr: group-id: value: com.yourgroup修改父 pom，使用公司自己的 pom。
initializr: env: maven: # use your parent pom parent: groupId: com.yourself artifactId: your-parent version: 1.</description></item><item><title>基于 Alibaba Sentinel 实现的分布式限流中间件服务以及遇到的坑和注意事项</title><link>https://www.xlabs.club/blog/sentinel/</link><pubDate>Thu, 07 Mar 2024 21:06:10 +0800</pubDate><guid>https://www.xlabs.club/blog/sentinel/</guid><description>基于 Alibaba Sentinel 实现的分布式限流中间件服务。主要对服务提供者提供限流、系统保护，对服务调用者提供熔断降级、限流排队等待效果。
实现目标：
作为服务提供者，保护自己不被打死，服务可以慢不可以挂。 作为客户端及时限速和熔断，防止对服务提供方包含 Http、数据库、MQ 等造成太大压力，防止把糟糕的情况变得更糟。 以用户、租户、对象等更细粒度进行流量精细控制。 服务预热，应用新发布上线，缓存尚未完全建立，防止流量一下子把服务打死。 能够根据 Prometheus、ClickHouse、Elasticsearch 提供的监控指标，动态生成规则，自适应调整规则。 概述 Sentinel 的基础知识请参考官方文档描述，这里单独介绍一些与我们定制相关的内容。
限流简单来说就三个点：资源、规则、效果。
资源：就是一个字符串，这个字符串可以自己定义、可以用注解自动生成、可以通过拦截器按规则生成。
规则：Sentinel 定义的一系列限流保护规则，比如流量控制规则、自适应保护规则。
效果：实际上“效果”也是“规则”定义的一部分。任何一条请求，命中某些资源规则后产生的效果，比如直接抛出异常、匀速等待。
Sentinel 全局注意事项和使用限制 使用开源默认 Sentinel 组件，有一些坑，或者说需要关注的注意事项：
单个进程内资源数量阈值是 6000，多出的资源规则将不会生效（因为是懒加载，资源先到先得），也不提示错误而是直接忽略，资源数量太多建议使用热点参数控制。 对于限流的链路模式，context 阈值是 2000，所以默认的 WEB_CONTEXT_UNIFY 为 true，如果需要链路限流需要把这个改为 false。 自定义时，资源名中不要带 | 线， 这个日志中要用，日志以此作为分割符。 Sentinel 支持按来源限流，注意 origin 数量不能太多，否则会导致内存暴涨。 一个资源可以有多个规则，一条请求能否通过，取决于规则里阈值最小的限制条件。 限流的目的是保护系统，计数计量并不准确，所以不要拿限流做计量或配额控制。 增加限流一定程度上通过时间换空间，降低了 CPU、内存负载，对 K8S HPA 策略会有一定影响。后续我们也会考虑根据 Sentinel 限流指标进行扩缩容。 限流中如果有增加等待效果会使接口变慢，各调用链需要关注调用超时和事务配置。 目前 sentinel-web-servlet 和 sentinel-spring-webmvc-adapter 均不支持热点参数限流。为了支持热点参数需要自行扩展。 sentinel-web-servlet 和 sentinel-spring-webmvc-adapter 会将每个到来的不同的 URL 都作为不同的资源处理，因此对于 REST 风格的 API，需要自行实现 UrlCleaner 接口清洗一下资源（比如将满足 /foo/:id 的 URL 都归到 /foo/* 资源下）。否则会导致资源数量过多，超出资源数量阈值（目前是 6000）时多出的资源的规则将不会生效。 一些文档中尚未更新但是大家可能关心的内容：</description></item><item><title>Windows 提权和设置环境变量</title><link>https://www.xlabs.club/blog/windows-takeown/</link><pubDate>Mon, 26 Feb 2024 23:25:29 +0800</pubDate><guid>https://www.xlabs.club/blog/windows-takeown/</guid><description>背景：公司 Windows 办公机受域控安全策略限制，部分文件无权直接修改，另外开发常用的设置系统环境变量也变灰无法设置。此问题解决方式如下。
提升文件权限 点击 Windows + X 快捷键 – 选择「命令提示符（管理员）。
在 CDM 窗口中执行如下命令。
takeown /f C:\要修复的文件路径在拿到文件所有权后，还需要使用如下命令获取文件的完全控制权限。
icacls C:\要修复的文件路径 /Grant Administrators:F命令行设置环境变量 Windows 下命令行设置环境变量，方式为 setx 变量名 变量值，变量值带空格等特殊符号的，用引号引起来。
# 通过命令行设置 Java Home setx JAVA_HOME &amp;#34;C:\Program Files\Java\jdk-11.0.2&amp;#34; # 设置 GO Path setx GOPATH &amp;#34;D:\workspace\go&amp;#34;</description></item><item><title>Git SSH 客户端同一机器多用户多仓库配置</title><link>https://www.xlabs.club/blog/git-multi-user/</link><pubDate>Mon, 26 Feb 2024 22:55:10 +0800</pubDate><guid>https://www.xlabs.club/blog/git-multi-user/</guid><description>Git 为不同目录配置不同的 config，比如在同一个电脑上区分个人开发账号和公司开发账号，开源项目放一个文件夹，公司项目放一个文件夹，这样在提交代码的时候就不会混乱。
为账户 B 准备一个单独的配置文件，比如： ~/.gitconfig-b，内容根据需要定义。
[user] name = userb-name email = userb-email@test.com修改 ~/.gitconfig 文件，增加以下配置，引用上面创建的配置文件，注意其中的路径用绝对路径，并且路径以 / 结尾。
[includeIf &amp;#34;gitdir:/project/path-b/&amp;#34;] path = /Users/xxxx/.gitconfig-b保存后，在 /project/path-b/ 下新的仓库都会以 .gitconfig-b 中的用户名和邮箱提交了。
注意如果使用 ssh key 方式，在生成 key 的时候 ssh-keygen 名字指定文件名，多个 key 不要覆盖了。</description></item><item><title>重复 Transfer-Encoding Response Header 引起的 Traefik 代理服务 500 问题</title><link>https://www.xlabs.club/blog/duplicate-transfer-encoding-chunked/</link><pubDate>Sun, 26 Nov 2023 10:21:44 +0800</pubDate><guid>https://www.xlabs.club/blog/duplicate-transfer-encoding-chunked/</guid><description>我有一个 Spring Boot 应用服务，提供了一些简单的查询接口，本身运行很正常，通过 curl 或其他 http 客户端 localhost 请求都没有问题。
某天通过 Traefik 代理了此服务，经过代理后再访问，某个接口一直都是 500 internal server error，其他接口都没有问题。通过 tcpdump 抓包发现，应用服务并没有返回任何 500 错误，而且响应时间和 Body 体大小都很正常。
根据网上经验排查了 Traefik SSL 证书问题、路径问题、消息体太大问题、请求 Header 不合规问题，都一一否定。最后无意间看了一眼 Response Header，发现 Spring Boot 应用返回了两个 Transfer-Encoding: chunked Header。
再根据此 Header 搜索，发现果然有人遇到过类似问题，请参考这几个链接。
https://github.com/traefik/traefik/issues/7741 https://github.com/spring-projects/spring-framework/issues/21523 https://github.com/spring-projects/spring-boot/issues/37646 https://stackoverflow.com/questions/77042701/nginx-upstream-sent-duplicate-header-line-transfer-encoding-chunked-previo 从上面链接描述中可知，不仅 Traefik 会出现此问题，nginx 包含以 nginx 为基础的 ingress 也会出现同样问题，不过 nginx 返回错误信息类似 Nginx: upstream sent duplicate header line: &amp;quot;Transfer-Encoding: chunked&amp;quot;, previous value: &amp;quot;Transfer-Encoding: chunked” ，返回错误码一般是 502 Bad Gateway。</description></item><item><title>K8S 容器 PID 限制引起的 Java OutOfMemoryError</title><link>https://www.xlabs.club/blog/k8s-pid-limiting-oom/</link><pubDate>Thu, 07 Sep 2023 16:21:44 +0800</pubDate><guid>https://www.xlabs.club/blog/k8s-pid-limiting-oom/</guid><description>问题描述：
一个 Java 应用跑在 K8S 容器内，Pod 内只有 Java 这一个进程。应用跑了一段时间后，CPU、内存占用都不高，但是却出现以下 OutOfMemoryError 错误。
Exception in thread &amp;#34;slow-fetch-15&amp;#34; java.lang.OutOfMemoryError: unable to create new native thread 428 at java.lang.Thread.start0(Native Method) 429 at java.lang.Thread.start(Thread.java:719) 430 at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:957) 431 at java.util.concurrent.ThreadPoolExecutor.processWorkerExit(ThreadPoolExecutor.java:1025) 432 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167) 433 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) 进入 Pod 内，尝试执行任何操作，又会出现 unable to start container process 错误。
一开始怀疑是内存不足，调大了内存，同时也缩小了 Java 的 xss，都不起作用。
真实原因： K8S 容器限制了 PID 数，无法创建新的线程，在 Pod 内 cat /sys/fs/cgroup/pids/pids.max 发现是 1024。
关于 K8S pid limit， 可参考此资料：https://kubernetes.io/zh-cn/docs/concepts/policy/pid-limiting/.</description></item></channel></rss>