<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Blog on XLabs</title><link>https://www.xlabs.club/blog/</link><description>Recent content in Blog on XLabs</description><generator>Hugo -- gohugo.io</generator><language>zh</language><copyright>Copyright (c) 2020-2024 XLabs Club</copyright><lastBuildDate>Sat, 09 Mar 2024 18:32:03 +0800</lastBuildDate><atom:link href="https://www.xlabs.club/blog/index.xml" rel="self" type="application/rss+xml"/><item><title>Spring Boot Start 脚手架定制开发和快速入门</title><link>https://www.xlabs.club/blog/spring-boot-start-site/</link><pubDate>Sat, 09 Mar 2024 14:29:03 +0800</pubDate><guid>https://www.xlabs.club/blog/spring-boot-start-site/</guid><description>介绍基于 start.spring.io 快速定制自己的 Spring Boot 脚手架，主要应用场景：
规范公司自己的 parent pom，增加特定的依赖项。 根据公司规范生成统一的包结构，统一命名。 根据需要增加特定代码或文件，比如根据公司要求统一 logback.xml、 application.properties 文件。 提供公司自研的二方 jar 包。 快速开始 基本步骤：
对于 spring.initializr 我们没有定制的需求，直接引用官方的。 拷贝一份 start.spring.io，直接基于这个项目开发、部署、运行。以下都是关于如何修改 start.spring.io。 start.spring.io 主要关注两个模块：
start-client：前端页面，可以定制些自己的 logo、title 等。 start-site：是一个标准的 spring boot 项目，实际 run 起来的服务，引用了 start-client，直接 run 这个项目的 main 方法就能看到效果。 主要配置文件：start-site/src/main/resources/application.yml，通过修改这个配置文件可以达到的效果如下。
修改 start 启动时默认 group，把 com.example 改为公司自己的 group。
initializr: group-id: value: com.yourgroup修改父 pom，使用公司自己的 pom。
initializr: env: maven: # use your parent pom parent: groupId: com.yourself artifactId: your-parent version: 1.</description></item><item><title>使用 Sentinel 实现分布式应用限流</title><link>https://www.xlabs.club/blog/sentinel/</link><pubDate>Thu, 07 Mar 2024 21:06:10 +0800</pubDate><guid>https://www.xlabs.club/blog/sentinel/</guid><description>基于 Alibaba Sentinel 实现的分布式限流中间件服务。主要对服务提供者提供限流、系统保护，对服务调用者提供熔断降级、限流排队等待效果。
实现目标：
作为服务提供者，保护自己不被打死，服务可以慢不可以挂。 作为客户端及时限速和熔断，防止对服务提供方包含 Http、数据库、MQ 等造成太大压力，防止把糟糕的情况变得更糟。 以用户、租户、对象等更细粒度进行流量精细控制。 服务预热，应用新发布上线，缓存尚未完全建立，防止流量一下子把服务打死。 能够根据 Prometheus、ClickHouse、Elasticsearch 提供的监控指标，动态生成规则，自适应调整规则。 概述 Sentinel 的基础知识请参考官方文档描述，这里单独介绍一些与我们定制相关的内容。
限流简单来说就三个点：资源、规则、效果。
资源：就是一个字符串，这个字符串可以自己定义、可以用注解自动生成、可以通过拦截器按规则生成。 规则：Sentinel 定义的一系列限流保护规则，比如流量控制规则、自适应保护规则。 效果：实际上“效果”也是“规则”定义的一部分。任何一条请求，命中某些资源规则后产生的效果，比如直接抛出异常、匀速等待。
Sentinel 全局注意事项和使用限制 使用开源默认 Sentinel 组件，可关注以下注意事项：
单个进程内资源数量阈值是 6000，多出的资源规则将不会生效，也不提示错误而是直接忽略，资源数量太多建议使用热点参数控制。 对于限流的链路模式，context 阈值是 2000，所以默认的 WEB_CONTEXT_UNIFY 为 true，如果需要链路限流需要把这个改为 false。 自定义时，资源名中不要带 | 线， 这个日志中要用，日志以此作为分割符。 Sentinel 支持按来源限流，注意 origin 数量不能太多，否则会导致内存暴涨，并且目前不支持模式匹配。 一个资源可以有多个规则，一条请求能否通过，取决于规则里阈值最小的限制条件。 限流的目的是保护系统，计数计量并不准确，所以不要拿限流做计量或配额控制。 增加限流一定程度上通过时间换空间，降低了 CPU、内存负载，对 K8S HPA 策略会有一定影响。后续我们也会考虑根据 Sentinel 限流指标进行扩缩容。 接口变慢，各调用链需要关注调用超时和事务配置。 目前 sentinel-web-servlet 和 sentinel-spring-webmvc-adapter 均不支持热点参数限流。为了支持热点参数需要自行扩展。 接入指导 总体架构图。
我们所有组件，规则加载都是由 Datasource 组件统一加载，配置是懒加载的，在第一次访问的时候加载，如果需要定义规则请在配置中心定义。这是由 Sentinel 在第一次初始化的时候通过 SPI 加载的，所以在咱们的代码里看不到主动加载的动作。 注意：如果你有自编码使用 Sentinel SDK 自带的 XxxRuleManager.</description></item><item><title>Windows 提权和设置环境变量</title><link>https://www.xlabs.club/blog/windows-takeown/</link><pubDate>Mon, 26 Feb 2024 23:25:29 +0800</pubDate><guid>https://www.xlabs.club/blog/windows-takeown/</guid><description>背景：公司 Windows 办公机受域控安全策略限制，部分文件无权直接修改，另外开发常用的设置系统环境变量也变灰无法设置。此问题解决方式如下。
提升文件权限 点击 Windows + X 快捷键 – 选择「命令提示符（管理员）。
在 CDM 窗口中执行如下命令。
takeown /f C:\要修复的文件路径在拿到文件所有权后，还需要使用如下命令获取文件的完全控制权限。
icacls C:\要修复的文件路径 /Grant Administrators:F命令行设置环境变量 Windows 下命令行设置环境变量，方式为 setx 变量名 变量值，变量值带空格等特殊符号的，用引号引起来。
# 通过命令行设置 Java Home setx JAVA_HOME &amp;#34;C:\Program Files\Java\jdk-11.0.2&amp;#34; # 设置 GO Path setx GOPATH &amp;#34;D:\workspace\go&amp;#34;</description></item><item><title>Git 多用户配置</title><link>https://www.xlabs.club/blog/git-multi-user/</link><pubDate>Mon, 26 Feb 2024 22:55:10 +0800</pubDate><guid>https://www.xlabs.club/blog/git-multi-user/</guid><description>Git 为不同目录配置不同的 config，比如在同一个电脑上区分个人开发账号和公司开发账号，开源项目放一个文件夹，公司项目放一个文件夹，这样在提交代码的时候就不会混乱。
为账户 B 准备一个单独的配置文件，比如： ~/.gitconfig-b，内容根据需要定义。
[user] name = userb-name email = userb-email@test.com修改 ~/.gitconfig 文件，增加以下配置，引用上面创建的配置文件，注意其中的路径用绝对路径，并且路径以 / 结尾。
[includeIf &amp;#34;gitdir:/project/path-b/&amp;#34;] path = /Users/xxxx/.gitconfig-b保存后，在 /project/path-b/ 下新的仓库都会以 .gitconfig-b 中的用户名和邮箱提交了。
注意如果使用 ssh key 方式，在生成 key 的时候 ssh-keygen 名字指定文件名，多个 key 不要覆盖了。</description></item><item><title>重复 Transfer-Encoding Response Header 引起的 Traefik 代理服务 500 问题</title><link>https://www.xlabs.club/blog/duplicate-transfer-encoding-chunked/</link><pubDate>Sun, 26 Nov 2023 10:21:44 +0800</pubDate><guid>https://www.xlabs.club/blog/duplicate-transfer-encoding-chunked/</guid><description>我有一个 Spring Boot 应用服务，提供了一些简单的查询接口，本身运行很正常，通过 curl 或其他 http 客户端 localhost 请求都没有问题。
某天通过 Traefik 代理了此服务，经过代理后再访问，某个接口一直都是 500 internal server error，其他接口都没有问题。通过 tcpdump 抓包发现，应用服务并没有返回任何 500 错误，而且响应时间和 Body 体大小都很正常。
根据网上经验排查了 Traefik SSL 证书问题、路径问题、消息体太大问题、请求 Header 不合规问题，都一一否定。最后无意间看了一眼 Response Header，发现 Spring Boot 应用返回了两个 Transfer-Encoding: chunked Header。
再根据此 Header 搜索，发现果然有人遇到过类似问题，请参考这几个链接。
https://github.com/traefik/traefik/issues/7741 https://github.com/spring-projects/spring-framework/issues/21523 https://github.com/spring-projects/spring-boot/issues/37646 https://stackoverflow.com/questions/77042701/nginx-upstream-sent-duplicate-header-line-transfer-encoding-chunked-previo 从上面链接描述中可知，不仅 Traefik 会出现此问题，nginx 包含以 nginx 为基础的 ingress 也会出现同样问题，不过 nginx 返回错误信息类似 Nginx: upstream sent duplicate header line: &amp;quot;Transfer-Encoding: chunked&amp;quot;, previous value: &amp;quot;Transfer-Encoding: chunked” ，返回错误码一般是 502 Bad Gateway。</description></item><item><title>K8S 容器 PID 限制引起的 Java OutOfMemoryError</title><link>https://www.xlabs.club/blog/k8s-pid-limiting-oom/</link><pubDate>Thu, 07 Sep 2023 16:21:44 +0800</pubDate><guid>https://www.xlabs.club/blog/k8s-pid-limiting-oom/</guid><description>问题描述：
一个 Java 应用跑在 K8S 容器内，Pod 内只有 Java 这一个进程。应用跑了一段时间后，CPU、内存占用都不高，但是却出现以下 OutOfMemoryError 错误。
Exception in thread &amp;#34;slow-fetch-15&amp;#34; java.lang.OutOfMemoryError: unable to create new native thread 428 at java.lang.Thread.start0(Native Method) 429 at java.lang.Thread.start(Thread.java:719) 430 at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:957) 431 at java.util.concurrent.ThreadPoolExecutor.processWorkerExit(ThreadPoolExecutor.java:1025) 432 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167) 433 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) 进入 Pod 内，尝试执行任何操作，又会出现 unable to start container process 错误。
一开始怀疑是内存不足，调大了内存，同时也缩小了 Java 的 xss，都不起作用。
真实原因： K8S 容器限制了 PID 数，无法创建新的线程，在 Pod 内 cat /sys/fs/cgroup/pids/pids.max 发现是 1024。
关于 K8S pid limit， 可参考此资料：https://kubernetes.io/zh-cn/docs/concepts/policy/pid-limiting/.</description></item><item><title>K8S 容器内 Java 进程内存分析</title><link>https://www.xlabs.club/blog/java-memory/</link><pubDate>Sat, 07 Jan 2023 10:54:37 +0800</pubDate><guid>https://www.xlabs.club/blog/java-memory/</guid><description>故事背景：
一个 K8S Pod，里面只有一个 Java 进程，K8S request 和 limit memory 都是 2G，Java 进程核心参数包括：-XX:+UseZGC -Xmx1024m -Xms768m -XX:SoftMaxHeapSize=512m。
服务启动一段时间后，查看 Grafana 监控数据，Pod 内存使用量约 1.5G，JVM 内存使用量约 500M，通过 jvm dump 分析没有任何大对象，运行三五天后出现 ContainerOOM。
首先区分下 ContainerOOM 和 JvmOOM，ContainerOOM 是 Pod 内存不够，Java 向操作系统申请内存时内存不足导致。
问题来了：
Pod 2G 内存，JVM 设置了 Xmx 1G，已经预留了 1G 内存，为什么还会 ContainerOOM，这预留的 1G 内存被谁吃了。 正常情况下（无 ContainerOOM），Grafana 看到的监控数据，Pod 内存使用量 1.5G， JVM 内存使用量 500M，差别为什么这么大。 Grafana 看到的监控数据，内存使用量、提交量各是什么意思，这些值是怎么算出来的，和 Pod 进程中如何对应，为什么提交量一直居高不小。 Grafana 监控图。
统计指标 Pod 内存使用量统计的指标是 container_memory_working_set_bytes：
container_memory_usage_bytes = container_memory_rss + container_memory_cache + kernel memory container_memory_working_set_bytes = container_memory_usage_bytes - total_inactive_file（未激活的匿名缓存页） container_memory_working_set_bytes 是容器真实使用的内存量，也是资源限制 limit 时的 OOM 判断依据。</description></item><item><title>使用 Visual Studio Code 搭建多用户远程 IDE</title><link>https://www.xlabs.club/blog/code-server/</link><pubDate>Wed, 07 Sep 2022 16:21:44 +0800</pubDate><guid>https://www.xlabs.club/blog/code-server/</guid><description>为 VS Code Web 版 code-server 增加外部认证，并支持多用户，不同用户的 code-server 实例完全隔离。
主要为了解决问题：
code-server 本身只支持配置文件形式的用户名密码认证（截止目前，以后也许会改进）。所以引入了外部认证系统，Google、GitHub、 okta、CAS、Keycloak 等理论上都是支持的。
code-server 默认没有数据隔离，所以又加了一层 auth proxy，为每个用户创建一个（或多个）code-server 实例，通过 proxy 代理到各自的实例，以实现用户间的数据隔离。
使用开源 Auth Proxy，无需自己编码即可实现认证授权流程，比如 code flow with pkce 对大部分人来说读懂这个协议都很困难。
此文档源码请参考：architecture-diagram
使用组件 keycloak
Redhat 开源 IAM 系统，目前也是 CNCF 项目，提供用户、组织服务，提供标准 OIDC。
oauth2-proxy
认证代理，配合 keycloak 提供完整 OAuth2 Code Flow 认证流程。也可以试试 pomerium，看样子也不错。
架构图如下。
核心逻辑 架构图简单解读，所有过程官方文档都有详细说明，都是配置，以官方配置为准。
keycloak 创建 client，使用 OIDC 协议，作为 oauth2-proxy 的 provider。
ingress(nginx) 使用 auth_request 指令拦截所有请求，从 oauth2-proxy 进行代理认证，配置可参考 oauth2-proxy auth_request 指导。
nginx.ingress.kubernetes.io/auth-signin: https://$host/oauth2/start?rd=$escaped_request_uri nginx.</description></item><item><title>备考 CKA 过程，CKA 真题分享</title><link>https://www.xlabs.club/blog/kubernetes-cka/</link><pubDate>Sat, 26 Feb 2022 23:42:48 +0800</pubDate><guid>https://www.xlabs.club/blog/kubernetes-cka/</guid><description>备考 CKA （Certified Kubernetes Administrator）过程，心得，遇见问题，CKA 真题。
一句话总结：按照教程多练习，把控好时间就能通过，期望通过刷题通过考试的年代已经过去了，而且多练习对平时工作真的有用。
备考环境 备考使用的系统和软件版本如下。
Ubuntu：20.04 Focal Fossa Kubernetes：1.20.7 kubeadm：1.20.7 安装和使用问题记录 kubeadm 安装问题 安装 kubeadm，国内安装使用阿里镜像源。
$ cat /etc/apt/sources.list.d/kubernetes.list deb https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial main 踩坑：因为使用的是 ubuntu 20.04，代号 focal，专门去各个代理镜像源找kubernetes-focal都没有找到，后来发现 google 官方根本没发布对应的版本，只有kubernetes-xenial， k8s 官方文档里 ubuntu 也是用的这一个版本。可以用，就用他吧。
kubeadm init 时指定使用阿里镜像源（解决国内连不上 k8s.gcr.io 的问题）、指定版本号（安装考试对应的版本，不一定是最新版本）。
通过指定--image-repository，不需要手动下载镜像重新打 tag，kubeadm 自动使用指定的 repository。
kubeadm init --image-repository=registry.aliyuncs.com/google_containers \ --pod-network-cidr=10.244.0.0/16 \ --kubernetes-version=v1.20.7解决 scheduler Unhealthy，controller-manager Unhealthy 第一次安装完成后通过 kubectl get cs命令，发现 scheduler Unhealthy，controller-manager Unhealthy。
$ kubectl get cs NAME STATUS MESSAGE scheduler Unhealthy Get &amp;#34;http://127.</description></item><item><title>从 Spring 到 Spring Boot</title><link>https://www.xlabs.club/blog/migrating-spring-to-spring-boot/</link><pubDate>Sat, 07 Jan 2023 10:54:37 +0800</pubDate><guid>https://www.xlabs.club/blog/migrating-spring-to-spring-boot/</guid><description>从 Spring 到 Spring Boot，迁移升级快速入门以及各种踩坑记录。
概述 从 Spring 到 Spring Boot，整体开发、运行方式主要变化。
- 当前（老）模式 新模式（本地） 新模式（线上） 开发习惯 Spring + 外置 Tomcat Spring Boot（embed tomcat） Spring Boot War or Jar Java 版本 8、11、16、17 11、17（推荐）、21 11、17（推荐）、21 Tomcat 版本 8.x、9.x 9.x 9.x（推荐）、10.x 说明：
理论上支持 Java11，但是要求业务方尽量使用 Java17。其他版本都是实验性质尽量兼容。 线上运行支持 Spring Boot jar 直接运行，但主要业务仍推荐以 war + tomcat 为主。如果希望以 java -jar 方式运行，参考下面的章节“jar 方式运行”描述。 目前 Spring Boot 主要推行版本是 2.7.x。 3.x 版本逐渐适配中。 快速开始 线下支撑系统导航，点击 脚手架 进入 spring start 页面，按自己需求选择模块，生成自己业务模式初始化代码。 写（Copy）业务代码到项目里，修改 pom.</description></item></channel></rss>