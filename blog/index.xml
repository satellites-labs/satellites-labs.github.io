<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Blog on XLabs</title><link>https://www.xlabs.club/blog/</link><description>Recent content in Blog on XLabs</description><generator>Hugo</generator><language>zh</language><copyright>Copyright (c) 2020-2024 XLabs Club</copyright><lastBuildDate>Sun, 30 Jun 2024 18:48:43 +0800</lastBuildDate><atom:link href="https://www.xlabs.club/blog/index.xml" rel="self" type="application/rss+xml"/><item><title>Koupleless 试用报告总结，踩坑记录分享</title><link>https://www.xlabs.club/blog/koupleless-first-boot/</link><pubDate>Mon, 27 May 2024 14:20:24 +0800</pubDate><guid>https://www.xlabs.club/blog/koupleless-first-boot/</guid><description>我们公司的主要应用都是以 Java 作为开发语言，这几年随着业务的高速增长，应用数目越来越多，CPU 内存资源占用越来越多，项目组之间开发合作效率也越来越低。
顺应这个时代降本增效的目的，我们希望寻找一个能解决当前几个核心问题的框架：
模块化开发、部署、资源共享的能力，减少 Cache、Class 等资源占用，有效降低内存占用。 更快更轻的依赖，应用能够更快的启动。 能够让各个项目组不改代码或少改代码即可接入，控制开发迁移的成本，毕竟很多历史老应用不是那么容易迁移。 基于以上背景，我们在 2022 年基于 SOFAArk 运行了一个版本，效果不太理想暂时搁置。今年 Koupleless 重新开源后做了一些增强和变更，开源社区活跃度有所提升，看宣传效果很好，我们决定重新评估是否可在公司内推广。
什么是 Koupleless Koupleless 是一种模块化 Serverless 技术解决方案，它能让普通应用低成本演进为 Serverless 研发模式，让代码与资源解耦，轻松独立维护， 与此同时支持秒级构建部署、合并部署、动态伸缩等能力为用户提供极致的研发运维体验，最终帮助企业实现降本增效。
Koupleless 是蚂蚁集团内部经过 5 年打磨成熟的研发框架和运维调度平台能力，相较于传统镜像化的应用模式研发、运维、运行阶段都有 10 倍左右的提升，总结起来 5 大特点：快、省、灵活部署、平滑演进、生产规模化验证。
以上都是官网的宣传，更多介绍请链接到官网查看。
在整个 Koupleless 平台里，需要四个组件：
研发工具 Arkctl, 提供模块创建、快速联调测试等能力。 运行组件 SOFAArk, Arklet，提供模块运维、模块生命周期管理，多模块运行环境。（这算两个组件？） 控制面组件 ModuleController，本质上是一个 K8S Operator，提供模块发布与运维能力。 我们公司有自己的发布系统、应用管理平台，很少允许运行额外的控制面组件，那么除去 ModuleController，我个人认为，Koupleless 约等于 SOFAArk。
Koupleless 增强了 SOFAArk 运维部署相关的功能，解决了 SOFAArk 在企业内无法开箱即用的问题。
应用接入遇见问题 基于官方文档我们改造接入了几个应用，分享几个我们遇见的问题。
对 Java 17 或 21 的支持不好。 虽然官方已经声称支持 Java 17，但是若用了 Java 17 的语法或新特性，无法编译通过。最后只好自编译 SOFAArk plugin 修改相关依赖解决。</description></item><item><title>容器镜像制作最佳实践，Dockerfile 实践经验和踩坑记录</title><link>https://www.xlabs.club/blog/docker-best-practices/</link><pubDate>Fri, 24 May 2024 20:56:08 +0800</pubDate><guid>https://www.xlabs.club/blog/docker-best-practices/</guid><description>整理了由 Docker 官方和社区推荐的用于构建高效镜像的最佳实践和方法，当然有些可能并不适用于你，请注意分辨。
使用官方镜像作为基础镜像。官方镜像经过了充分验证并集成了最佳实践。
# 正例： FROM node # 反例： FROM ubuntu RUN apt-get install -y node保持尽可能小的镜像大小，绝不安装无关依赖。
严格的版本化管理，使用确定性的标签，基础镜像禁用 latest。
使用 .dockerignore 文件排除文件干扰。
最经常变化的命令越往后执行，充分利用分层缓存机制。
Dockerfile 中每行命令产生一层，请合并命令执行，最大限度减少层数。
使用多阶段构建，减少所构建镜像的大小。
禁用 root 用户，使用独立的 use 和 group。
启用镜像安全扫描，并及时更新。
一个容器只专注做一件事情。
Java 应用程序不要使用 PID 为 1 的进程，使用 tini 或 dump-init 管理进程，避免僵尸进程。
以上都是一些基本的原则，但是实际工作的过程中，大家可能会像我一样纠结几个问题。
关于第 1 点，一定要使用官方镜像吗。未必，看情况。比如我们作为平台，涉及很多种开发语言，很多种组合场景，每个官方基础镜像可能都不同，就会自建基础镜像，以便统一操作系统、统一脚本和安全维护。为什么要统一操作系统，操作系统投的毒，就像出骨鱼片里未净的刺，给人一种不期待的伤痛。 为了镜像大小和安全，一定要使用 Alpine 或 distroless 镜像吗。我的建议是不要使用 Alpine 镜像，如有能力才使用 distroless 镜像。毕竟 libc 的坑，谁痛谁知道。 我们的镜像策略 Dockerfile 编写小技巧 使用 Heredocs 语法代替又长又臭的字符串拼接，当然 Heredocs 支持更多功能比如 run python、多文件内容拷贝，以下举例只是最常用的。</description></item><item><title>使用 Pulumi 部署 cert-manager 创建 K8S 自签名证书并信任证书</title><link>https://www.xlabs.club/blog/trust-cert-manager-selfsigned-tls/</link><pubDate>Mon, 29 Apr 2024 21:49:22 +0800</pubDate><guid>https://www.xlabs.club/blog/trust-cert-manager-selfsigned-tls/</guid><description>在搭建本地 Kubernetus 集群后，由于环境在内网，做不了域名验证，无法使用 Let&amp;rsquo;s Encrypt 颁发和自动更新证书，然而很多应用要求必须启用 HTTPS，只能用自签名 CA 证书，并由此 CA 继续颁发其他证书。
所以我们准备了以下工具，开始搭建。
Pulumi: 当前非常流行的 IaC 工具，值得一试。 cert-manager: 云原生证书管理，用于自动管理和颁发各种发行来源的 TLS 证书。它将确保证书有效并定期更新，并尝试在到期前的适当时间更新证书。 核心步骤和相关代码如下，更多源码请参考我们的 GitHub 项目 xlabs-ops。
使用 Pulumi 安装 cert-manager，生成自签名 CA 证书，根据自签名 CA 证书生成 cert-manager ClusterIssuer，都在如下代码了。
import * as pulumi from &amp;#34;@pulumi/pulumi&amp;#34;; import * as kubernetes from &amp;#34;@pulumi/kubernetes&amp;#34;; import * as tls from &amp;#34;@pulumi/tls&amp;#34;; // 部署 cert-manager Helm chart const certManagerRelease = new kubernetes.helm.v3.Release(&amp;#34;cert-manager&amp;#34;, { name: &amp;#34;cert-manager&amp;#34;, chart: &amp;#34;cert-manager&amp;#34;, version: &amp;#34;1.14.5&amp;#34;, namespace: &amp;#34;cert-manager&amp;#34;, createNamespace: true, timeout: 600, repositoryOpts: { repo: &amp;#34;https://charts.</description></item><item><title>Mac 搭建本地 K8S 开发环境方案选型</title><link>https://www.xlabs.club/blog/easiest-k8s-on-macos/</link><pubDate>Sat, 13 Apr 2024 15:20:43 +0800</pubDate><guid>https://www.xlabs.club/blog/easiest-k8s-on-macos/</guid><description>因为工作经常需要用到 K8S，而且有时因网络原因不能完全依赖公司网络，或者因为测试新功能不能直接发布到公司集群，所以就有了本地搭建 K8S 的需求。
另外如果你有以下需求，此文档中提到的方案也许有所帮助：
开发机器模拟 Arm、AMD64 等不同架构。 完全隔离的不同环境，比如为测试 docker、podman、buildkit、containd 等不同软件设置的独立环境。 CI/CD 流程中即用即消的轻量级虚拟机替代方案。 有限的资源模拟大批量的 K8S 节点。 以下介绍一下我用过的几种不同方案，有些纯属个人观点仅供参考。
Docker Desktop 并启用 Kubernetes 功能。
优点：最简单，开箱即用。
缺点：只支持单节点 K8S，且 K8S 部分功能不支持，不易定制。
Docker run K3D, K3D run K3S。
优点：简单，任何支持 docker 的工具（Rancher Desktop、Podman） 启动一个容器即可。
缺点：只支持 K3S。
multipass 启动虚拟机安装 K8S 或 K3S。
优点：multipass 可启动空白 ubuntu 虚拟机，或者启动已经安装好 minikube 的虚拟机。
缺点：只支持 ubuntu，虚拟机与宿主机同架构。
lima 启动虚拟机安装 K8S 或 K3S。
优点：支持虚拟多种 Linux，支持异构虚拟机，支持 contained 可代替 docker。
缺点：架构稍复杂，启动略慢，不如 multipass 稳定，不支持运行在 Windows。
以上方案，在网络畅通的情况下，均能在 10 分钟内启动一个单节点 K8S，所以整体方案都不复杂。</description></item><item><title>K8S 服务长连接负载不均衡问题分析和解决办法</title><link>https://www.xlabs.club/blog/tomcat-keepalive-load-balancer/</link><pubDate>Thu, 11 Apr 2024 21:05:46 +0800</pubDate><guid>https://www.xlabs.club/blog/tomcat-keepalive-load-balancer/</guid><description>问题背景，我们有一个 Http 服务在 K8S 内部署了 3 个 Pod，客户端使用 Service NodePort 进行连接，发现流量几乎都集中到了一个 Pod 上。
已知的情况是：
K8S Service 使用 round-robin 负载均衡策略。 客户端和服务端都启用了 Keep-Alive 长连接。 经过抓包分析，负载较高的 Pod 保持着较多 KeepAlive 长连接。将 kube-proxy 的 ipvs 转发模式设置为 Least-Connection，即倾向转发给连接数少的 Pod，可能会有所缓解，但也不一定，因为 ipvs 的负载均衡状态是分散在各个节点的，并没有收敛到一个地方，也就无法在全局层面感知哪个 Pod 上的连接数少，并不能真正做到 Least-Connection。
服务端主动要求断开长连接 客户端连接我们可能无法控制，那么如何从服务端主动断开长连接。
以 Tomcat 为例，它提供了 maxKeepAliveRequests 参数，到达此参数阈值后，Tomcat 会在 Response Header 中主动加一个 Connection: close，正常情况下客户端接收到此响应后会主动断开长连接。
对于其他不支持此参数的服务器，可以自定义 Filter 或者自定代码，到达某阈值后在 Response Header 中主动追加 Connection: close。
对于 Spring Boot 可通过 properties 配置。
# Spring Boot Tomcat server.tomcat.max-keep-alive-requests=100 # Spring Boot WebFlux server.</description></item><item><title>K8S StatefulSet 应用 PV/PVC 平滑扩容</title><link>https://www.xlabs.club/blog/statefulset-resize-pvc/</link><pubDate>Sun, 31 Mar 2024 21:29:52 +0800</pubDate><guid>https://www.xlabs.club/blog/statefulset-resize-pvc/</guid><description>在 K8S 中使用 Helm 部署了一些有状态应用，并通过 Helm 自动生成了 PV 和 PVC，某天想扩容，竟然报错了。
以下以 bitnami zookeeper 为例，其他 StatefulSet 同理。
为了实现磁盘扩容，改大 persistence.size，比如由 8Gi 改为 10Gi，然后执行 helm upgrade，出现错误。
Error: UPGRADE FAILED: cannot patch &amp;#34;zookeeper&amp;#34; with kind StatefulSet: StatefulSet.apps &amp;#34;zookeeper&amp;#34; is invalid: spec: Forbidden: updates to statefulset spec for fields other than &amp;#39;replicas&amp;#39;, &amp;#39;template&amp;#39;, &amp;#39;updateStrategy&amp;#39;, &amp;#39;persistentVolumeClaimRetentionPolicy&amp;#39; and &amp;#39;minReadySeconds&amp;#39; are forbidden 实际上我们想更新的是 StatefulSet 的 spec.volumeClaimTemplates 中的 storage 大小，根据提示信息，StatefulSet 竟然不允许。
spec: volumeClaimTemplates: - apiVersion: v1 spec: resources: requests: storage: 8Gi查看 K8S 官方说明，果然当前版本 (1.</description></item><item><title>MacOS 固化配置，彻底解决 too many open files in system 的问题</title><link>https://www.xlabs.club/blog/macos-too-many-open-files/</link><pubDate>Tue, 19 Mar 2024 23:05:07 +0800</pubDate><guid>https://www.xlabs.club/blog/macos-too-many-open-files/</guid><description>作为一个开发者，经常在 MacOS 遇到 Too many open files in system 的报错，尤其是碰到黑洞 node_modules 时，如何固化配置彻底解决，直接上代码。
输入 launchctl limit 即可看到当前的限制，我这里 maxfiles 是改过以后的。
$ launchctl limit cpu unlimited unlimited filesize unlimited unlimited data unlimited unlimited stack 8388608 67104768 core 0 unlimited rss unlimited unlimited memlock unlimited unlimited maxproc 1392 2088 maxfiles 10240 102400 开始创建文件 sudo vi /Library/LaunchDaemons/limit.maxfiles.plist ，内容如下，可根据自己爱好改后面的两个数字值。
&amp;lt;?xml version=&amp;#34;1.0&amp;#34; encoding=&amp;#34;UTF-8&amp;#34;?&amp;gt; &amp;lt;!DOCTYPE plist PUBLIC &amp;#34;-//Apple//DTD PLIST 1.0//EN&amp;#34; &amp;#34;http://www.apple.com/DTDs/PropertyList-1.0.dtd&amp;#34;&amp;gt; &amp;lt;plist version=&amp;#34;1.0&amp;#34;&amp;gt; &amp;lt;dict&amp;gt; &amp;lt;key&amp;gt;Label&amp;lt;/key&amp;gt; &amp;lt;string&amp;gt;limit.maxfiles&amp;lt;/string&amp;gt; &amp;lt;key&amp;gt;ProgramArguments&amp;lt;/key&amp;gt; &amp;lt;array&amp;gt; &amp;lt;string&amp;gt;launchctl&amp;lt;/string&amp;gt; &amp;lt;string&amp;gt;limit&amp;lt;/string&amp;gt; &amp;lt;string&amp;gt;maxfiles&amp;lt;/string&amp;gt; &amp;lt;string&amp;gt;10240&amp;lt;/string&amp;gt; &amp;lt;string&amp;gt;102400&amp;lt;/string&amp;gt; &amp;lt;/array&amp;gt; &amp;lt;key&amp;gt;RunAtLoad&amp;lt;/key&amp;gt; &amp;lt;true/&amp;gt; &amp;lt;key&amp;gt;ServiceIPC&amp;lt;/key&amp;gt; &amp;lt;false/&amp;gt; &amp;lt;/dict&amp;gt; &amp;lt;/plist&amp;gt;验证文件格式和内容，并应用生效。</description></item><item><title>文件压缩解压缩快速参考</title><link>https://www.xlabs.club/blog/tar-gz/</link><pubDate>Sun, 10 Mar 2024 15:22:20 +0800</pubDate><guid>https://www.xlabs.club/blog/tar-gz/</guid><description>文件压缩解压缩快速参考。
常用文件格式 .tar：tar 其实打包（或翻译为归档）文件，本身并没有压缩。在 Linux 里 man tar 可以看到它的描述也是“manipulate tape archives”（tar 最初被用来在磁带上创建档案，现在，用户可以在任何设备上创建档案，只是它的描述还没有改）。
.gz：gzip 是 GNU 组织开发的一个压缩程序，.gz 结尾的文件就是 gzip 压缩的结果。
.bz2：bzip2 是一个压缩能力更强的压缩程序，.bz2 结尾的文件就是 bzip2 压缩的结果。
.Z：compress 也是一个压缩程序。.Z 结尾的文件就是 compress 压缩的结果。
.zip：使用 zip 软件压缩的文件。
.tar.gz、.tar.bz2、.tar.xz 等可以理解为打包+压缩的效果，用软件解压可以发现比。gz 多了一层包。gzip 和 bzip2，不能同时压缩多个文件，tar 相当于开个挂加上同时压缩的特效，tar 先归档为一个大文件，而归档为大文件的速度是很快的，测试了一下几乎可以忽略不计。
除了这些格式外，常见的 deb、exe、msi、rpm、dmg、iso 等安装软件，其实都是经过压缩的，一般情况下没有必要再压缩。而 rar 基本认为是 Windows 平台专属的压缩算法了，各个 Linux 发行版都不自带 rar 压缩解压缩软件，所以可以看到很多软件发行的格式都是 .tar.gz 或 .zip。
解压缩 根据文件名后缀自行选择解压缩命令。
tar -xf test.tar gzip -d test.gz gunzip test.gz # -C 直接解压到指定目录 tar -xzf test.tar.gz -C /home bzip2 -d test.</description></item><item><title>MySQL 大文件导入优化，提高速度，提升性能</title><link>https://www.xlabs.club/blog/mysql-large-import/</link><pubDate>Sun, 10 Mar 2024 15:18:22 +0800</pubDate><guid>https://www.xlabs.club/blog/mysql-large-import/</guid><description>项目中需要根据 SQL 文件导入数据，文件大约 20G，正常导入约需要 2 小时，如何提高导入速度。
经过实验测试，如果一个 SQL 文件只有一个表的数据，可以直接使用 mysql load data infile 语法，速度比较快。
我们是一个 SQL 文件包含了很多表，mysql load data infile 就不支持了，可考虑在导入过程中设置如下参数，经过测试 20G 大约需要 40 分钟，比之前快了很多。
# 进入 mysql mysql -u root -p # 创建数据库（如果已经有数据库忽略此步骤） CREATE DATABASE 数据库名； # 设置参数 set sql_log_bin=OFF;//关闭日志 set autocommit=0;//关闭 autocommit 自动提交模式 0 是关闭 1 是开启（默认） set global max_allowed_packet = 20 *1024* 1024 * 1024; # 使用数据库 use 数据库名； # 开启事务 START TRANSACTION; # 导入 SQL 文件并 COMMIT（因为导入比较耗时，导入和 COMMIT 一行命令，这样不用盯着屏幕等提交了） source /xxx.</description></item><item><title>Spring Boot Start 脚手架定制开发和快速入门</title><link>https://www.xlabs.club/blog/spring-boot-start-site/</link><pubDate>Sat, 09 Mar 2024 14:29:03 +0800</pubDate><guid>https://www.xlabs.club/blog/spring-boot-start-site/</guid><description>介绍基于 start.spring.io 快速定制自己的 Spring Boot 脚手架，主要应用场景：
规范公司自己的 parent pom，增加特定的依赖项。 根据公司规范生成统一的包结构，统一命名。 根据需要增加特定代码或文件，比如根据公司要求统一 logback.xml、 application.properties 文件。 提供公司自研的二方 jar 包。 快速开始 基本步骤：
对于 spring.initializr 我们没有定制的需求，直接引用官方的。 拷贝一份 start.spring.io，直接基于这个项目开发、部署、运行。以下都是关于如何修改 start.spring.io。 start.spring.io 主要关注两个模块：
start-client：前端页面，可以定制些自己的 logo、title 等。 start-site：是一个标准的 spring boot 项目，实际 run 起来的服务，引用了 start-client，直接 run 这个项目的 main 方法就能看到效果。 主要配置文件：start-site/src/main/resources/application.yml，通过修改这个配置文件可以达到的效果如下。
修改 start 启动时默认 group，把 com.example 改为公司自己的 group。
initializr: group-id: value: com.yourgroup修改父 pom，使用公司自己的 pom。
initializr: env: maven: # use your parent pom parent: groupId: com.yourself artifactId: your-parent version: 1.</description></item></channel></rss>