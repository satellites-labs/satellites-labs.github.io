<!doctype html><html lang=zh data-bs-theme=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1"><link rel=preload href=https://www.xlabs.club/fonts/vendor/jost/jost-v4-latin-regular.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=https://www.xlabs.club/fonts/vendor/jost/jost-v4-latin-500.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=https://www.xlabs.club/fonts/vendor/jost/jost-v4-latin-700.woff2 as=font type=font/woff2 crossorigin><script src=/js/color-mode.86a91f050a481d0a3f0c72ac26543cb6228c770875981c58dcbc008fd3f875c8.js integrity="sha256-hqkfBQpIHQo/DHKsJlQ8tiKMdwh1mBxY3LwAj9P4dcg="></script><link rel=stylesheet href=/main.148a18197d444fb79036cadcd893d81e6b0b42f3f991dfcda8d9b77669ec5fff9c7b76e7a68947225dd3e4db1ceffd3336dea9576e23ef9d61b26e431dd269c2.css integrity="sha512-FIoYGX1ET7eQNsrc2JPYHmsLQvP5kd/NqNm3dmnsX/+ce3bnpolHIl3T5Nsc7/0zNt6pV24j751hsm5DHdJpwg==" crossorigin=anonymous><noscript><style>img.lazyload{display:none}</style></noscript><base href=https://www.xlabs.club/blog/java-memory/><link rel=canonical href=https://www.xlabs.club/blog/java-memory/><title>K8S 容器内 Java 进程内存分析 | XLabs</title><meta name=description content="XLabs Club"><link rel=icon href=/favicon-16x16.png sizes=16x16 type=image/png><link rel=icon href=/favicon-32x32.png sizes=32x32 type=image/png><link rel=icon href=/favicon-150x150.png sizes=150x150 type=image/png><link rel=apple-touch-icon href=/apple-touch-icon.png sizes=180x180 type=image/png><link rel=icon href=/favicon-192x192.png sizes=192x192 type=image/png><link rel=icon href=/favicon-512x512.png sizes=512x512 type=image/png><link rel=icon href=/favicon.svg><link rel=mask-icon href=/mask-icon.svg color=white><meta property="og:title" content="K8S 容器内 Java 进程内存分析"><meta property="og:description" content="故事背景：
一个 K8S Pod，里面只有一个 Java 进程，K8S request 和 limit memory 都是 2G，Java 进程核心参数包括：-XX:+UseZGC -Xmx1024m -Xms768m -XX:SoftMaxHeapSize=512m。
服务启动一段时间后，查看 Grafana 监控数据，Pod 内存使用量约 1.5G，JVM 内存使用量约 500M，通过 jvm dump 分析没有任何大对象，运行三五天后出现 ContainerOOM。
首先区分下 ContainerOOM 和 JvmOOM，ContainerOOM 是 Pod 内存不够，Java 向操作系统申请内存时内存不足导致。
问题来了：
Pod 2G 内存，JVM 设置了 Xmx 1G，已经预留了 1G 内存，为什么还会 ContainerOOM，这预留的 1G 内存被谁吃了。 正常情况下（无 ContainerOOM），Grafana 看到的监控数据，Pod 内存使用量 1.5G， JVM 内存使用量 500M，差别为什么这么大。 Grafana 看到的监控数据，内存使用量、提交量各是什么意思，这些值是怎么算出来的，和 Pod 进程中如何对应，为什么提交量一直居高不小。 Grafana 监控图。
统计指标 Pod 内存使用量统计的指标是 container_memory_working_set_bytes：
container_memory_usage_bytes = container_memory_rss + container_memory_cache + kernel memory container_memory_working_set_bytes = container_memory_usage_bytes - total_inactive_file（未激活的匿名缓存页） container_memory_working_set_bytes 是容器真实使用的内存量，也是资源限制 limit 时的 OOM 判断依据。"><meta property="og:type" content="article"><meta property="og:url" content="https://www.xlabs.club/blog/java-memory/"><meta property="og:image" content="https://www.xlabs.club/cover.png"><meta property="article:section" content="blog"><meta property="article:published_time" content="2023-01-07T10:54:37+08:00"><meta property="article:modified_time" content="2024-01-21T19:14:37+08:00"><meta property="og:site_name" content="XLabs"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://www.xlabs.club/cover.png"><meta name=twitter:title content="K8S 容器内 Java 进程内存分析"><meta name=twitter:description content="故事背景：
一个 K8S Pod，里面只有一个 Java 进程，K8S request 和 limit memory 都是 2G，Java 进程核心参数包括：-XX:+UseZGC -Xmx1024m -Xms768m -XX:SoftMaxHeapSize=512m。
服务启动一段时间后，查看 Grafana 监控数据，Pod 内存使用量约 1.5G，JVM 内存使用量约 500M，通过 jvm dump 分析没有任何大对象，运行三五天后出现 ContainerOOM。
首先区分下 ContainerOOM 和 JvmOOM，ContainerOOM 是 Pod 内存不够，Java 向操作系统申请内存时内存不足导致。
问题来了：
Pod 2G 内存，JVM 设置了 Xmx 1G，已经预留了 1G 内存，为什么还会 ContainerOOM，这预留的 1G 内存被谁吃了。 正常情况下（无 ContainerOOM），Grafana 看到的监控数据，Pod 内存使用量 1.5G， JVM 内存使用量 500M，差别为什么这么大。 Grafana 看到的监控数据，内存使用量、提交量各是什么意思，这些值是怎么算出来的，和 Pod 进程中如何对应，为什么提交量一直居高不小。 Grafana 监控图。
统计指标 Pod 内存使用量统计的指标是 container_memory_working_set_bytes：
container_memory_usage_bytes = container_memory_rss + container_memory_cache + kernel memory container_memory_working_set_bytes = container_memory_usage_bytes - total_inactive_file（未激活的匿名缓存页） container_memory_working_set_bytes 是容器真实使用的内存量，也是资源限制 limit 时的 OOM 判断依据。"><meta name=twitter:site content="@XLabs Club"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","item":"https://www.xlabs.club/","name":"卫星实验室","position":1},{"@type":"ListItem","item":"https://www.xlabs.club/blog/","name":"Blog","position":2},{"@type":"ListItem","name":"K8 S 容器内 Java 进程内存分析","position":3}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@type":"BlogPosting","headline":"K8S 容器内 Java 进程内存分析","description":"","isPartOf":{"@id":"https://www.xlabs.club/blog/java-memory/"},"mainEntityOfPage":{"@id":"https://www.xlabs.club/blog/java-memory/"},"datePublished":"2023-01-07T10:54:37+08:00","dateModified":"2024-01-21T19:14:37+08:00","image":[{"@id":"https://www.xlabs.club/blog/java-memory/after-jemalloc.png"},{"@id":"https://www.xlabs.club/blog/java-memory/before-jemalloc.png"},{"@id":"https://www.xlabs.club/blog/java-memory/grafana-pod-jvm.png"},{"@id":"https://www.xlabs.club/blog/java-memory/java-heap-use.png"},{"@id":"https://www.xlabs.club/blog/java-memory/jemalloc-jvm.png"},{"@id":"https://www.xlabs.club/blog/java-memory/jvm-jemalloc.svg"},{"@id":"https://www.xlabs.club/blog/java-memory/jvm-memory-structure.png"},{"@id":"https://www.xlabs.club/blog/java-memory/memory-dance.png"}],"author":{"@type":"Organization","name":"XLabs Club","url":"https://www.xlabs.club"},"publisher":{"@type":"Organization","name":"XLabs Club"}}]}</script><meta name=bytedance-verification-code content="MLvJvgu8eINfBcFxGCcQ"></head><body class="single section blog" data-bs-spy=scroll data-bs-target=#toc data-bs-root-margin="0px 0px -60%" data-bs-smooth-scroll=true tabindex=0><div class=sticky-top><header class="navbar navbar-expand-lg"><div class=container-fluid><a class="navbar-brand me-auto me-lg-3" href=/>XLabs</a>
<button type=button id=searchToggleMobile class="btn btn-link nav-link mx-2 d-lg-none" aria-label="Search website"><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg></button>
<button class="btn btn-link nav-link mx-2 order-3 d-lg-none" type=button data-bs-toggle=offcanvas data-bs-target=#offcanvasNavMain aria-controls=offcanvasNavMain aria-label="Open main navigation menu"><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-menu" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><line x1="4" y1="8" x2="20" y2="8"/><line x1="4" y1="16" x2="20" y2="16"/></svg></button><div class="offcanvas offcanvas-end h-auto" tabindex=-1 id=offcanvasNavMain aria-labelledby=offcanvasNavMainLabel><div class=offcanvas-header><h5 class=offcanvas-title id=offcanvasNavMainLabel>XLabs</h5><button type=button class="btn btn-link nav-link p-0" data-bs-dismiss=offcanvas aria-label=Close><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-x" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M18 6 6 18"/><path d="M6 6l12 12"/></svg></button></div><div class="offcanvas-body d-flex flex-column flex-lg-row justify-content-between"><ul class="navbar-nav flex-grow-1"><li class=nav-item><a class=nav-link href=https://www.xlabs.club/docs/guides/introduction/>Docs</a></li><li class=nav-item><a class="nav-link active" href=https://www.xlabs.club/blog/ aria-current=true>Blog</a></li></ul><button type=button id=searchToggleDesktop class="btn btn-link nav-link p-2 d-none d-lg-block" aria-label="Search website"><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg></button>
<button id=buttonColorMode class="btn btn-link mx-auto nav-link p-0 ms-lg-2 me-lg-1" type=button aria-label="Toggle theme"><svg data-bs-theme-value="dark" xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-moon" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M12 3c.132.0.263.0.393.0a7.5 7.5.0 007.92 12.446A9 9 0 1112 2.992z"/></svg><svg data-bs-theme-value="light" xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-sun" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M12 12m-4 0a4 4 0 108 0 4 4 0 10-8 0m-5 0h1m8-9v1m8 8h1m-9 8v1M5.6 5.6l.7.7m12.1-.7-.7.7m0 11.4.7.7m-12.1-.7-.7.7"/></svg></button><ul id=socialMenu class="nav mx-auto flex-row order-lg-4"><li class=nav-item><a class="nav-link social-link" href=https://github.com/xlabs-club/xlabs-club.github.io><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg><small class="ms-2 visually-hidden">GitHub</small></a></li></ul></div></div></div></header></div><div class=modal id=searchModal tabindex=-1 aria-labelledby=searchModalLabel aria-hidden=true><div class="modal-dialog modal-dialog-scrollable modal-fullscreen-md-down"><div class=modal-content><div class=modal-header><h1 class="modal-title fs-5 visually-hidden" id=searchModalLabel></h1><button type=button class="btn-close visually-hidden" data-bs-dismiss=modal aria-label=Close></button><div class="search-input flex-grow-1 d-none"><form id=search-form class=search-form action=# method=post accept-charset=utf-8 role=search><label for=query class=visually-hidden></label><div class=d-flex><input type=search id=query name=query class="search-text form-control form-control-lg" placeholder aria-label maxlength=128 autocomplete=off>
<button type=button class="btn btn-link text-decoration-none px-0 ms-3 d-md-none" data-bs-dismiss=modal aria-label=Close>Cancel</button></div></form></div></div><div class=modal-body><p class="search-loading status message d-none mt-3 text-center"></p><p class="search-no-recent message d-none mt-3 text-center"></p><p class="search-no-results message d-none mt-3 text-center">for "<strong><span class=query-no-results>Query here</span></strong>"</p><div id=searchResults class=search-results></div><template><article class="search-result list-view"><div class="card my-3"><div class=card-body><header><h2 class="h5 title title-submitted mb-0"><a class="stretched-link text-decoration-none text-reset" href=#>Title here</a></h2><div class="submitted d-none"><time class=created-date>Date here</time></div></header><div class=content>Summary here</div></div></div></article></template></div><div class=modal-footer><ul class="list-inline me-auto d-none d-md-block"><li class=list-inline-item><kbd class=me-2><svg width="15" height="15" aria-label="Enter key" role="img"><g fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.2"><path d="M12 3.53088v3c0 1-1 2-2 2H4m3 3-3-3 3-3"/></g></svg></kbd><span class=DocSearch-Label>to select</span></li><li class=list-inline-item><kbd class=me-2><svg width="15" height="15" aria-label="Arrow down" role="img"><g fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.2"><path d="M7.5 3.5v8m3-3-3 3-3-3"/></g></svg></kbd><kbd class=me-2><svg width="15" height="15" aria-label="Arrow up" role="img"><g fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.2"><path d="M7.5 11.5v-8m3 3-3-3-3 3"/></g></svg></kbd><span class=DocSearch-Label>to navigate</span></li><li class=list-inline-item><kbd class=me-2><svg width="15" height="15" aria-label="Escape key" role="img"><g fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.2"><path d="M13.6167 8.936c-.1065.3583-.6883.962-1.4875.962-.7993.0-1.653-.9165-1.653-2.1258v-.5678c0-1.2548.7896-2.1016 1.653-2.1016s1.3601.4778 1.4875 1.0724M9 6c-.1352-.4735-.7506-.9219-1.46-.8972-.7092.0246-1.344.57-1.344 1.2166s.4198.8812 1.3445.9805C8.465 7.3992 8.968 7.9337 9 8.5s-.454 1.398-1.4595 1.398C6.6593 9.898 6 9 5.963 8.4851m-1.4748.5368c-.2635.5941-.8099.876-1.5443.876s-1.7073-.6248-1.7073-2.204v-.4603c0-1.0416.721-2.131 1.7073-2.131.9864.0 1.6425 1.031 1.5443 2.2492h-2.956"/></g></svg></kbd><span class=DocSearch-Label>to close</span></li></ul><p class=d-md-none>Search by <a class=text-decoration-none href=https://github.com/nextapps-de/flexsearch>FlexSearch</a></p></div></div></div></div><div class="wrap container-fluid" role=document><div class=content><div class="container p-0"><article><div class="row justify-content-center"><div class="col-md-12 col-lg-12"><div class=blog-header><h1>K8S 容器内 Java 进程内存分析</h1><p><small>2023年1月7日&nbsp;in&nbsp;<a class="stretched-link position-relative link-muted" href=/categories/java/>Java</a>&nbsp;by&nbsp;<a class="stretched-link position-relative" href=/contributors/l10178/>l10178</a><span class="stretched-link position-relative reading-time" title><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M3 12a9 9 0 1018 0A9 9 0 003 12"/><path d="M12 7v5l3 3"/></svg>8&nbsp;</span></small></p></div></div><div class="col-md-12 col-lg-12"><p>故事背景：</p><p>一个 K8S Pod，里面只有一个 Java 进程，K8S request 和 limit memory 都是 2G，Java 进程核心参数包括：<code>-XX:+UseZGC -Xmx1024m -Xms768m -XX:SoftMaxHeapSize=512m</code>。</p><p>服务启动一段时间后，查看 Grafana 监控数据，Pod 内存使用量约 1.5G，JVM 内存使用量约 500M，通过 jvm dump 分析没有任何大对象，运行三五天后出现 ContainerOOM。</p><p>首先区分下 ContainerOOM 和 JvmOOM，ContainerOOM 是 Pod 内存不够，Java 向操作系统申请内存时内存不足导致。</p><p>问题来了：</p><ol><li>Pod 2G 内存，JVM 设置了 <code>Xmx 1G</code>，已经预留了 1G 内存，为什么还会 ContainerOOM，这预留的 1G 内存被谁吃了。</li><li>正常情况下（无 ContainerOOM），Grafana 看到的监控数据，Pod 内存使用量 1.5G， JVM 内存使用量 500M，差别为什么这么大。</li><li>Grafana 看到的监控数据，内存使用量、提交量各是什么意思，这些值是怎么算出来的，和 Pod 进程中如何对应，为什么提交量一直居高不小。</li></ol><p>Grafana 监控图。</p><p><a href=./grafana-pod-jvm.png><img alt="Grafana 监控图" class="img-fluid lazyload blur-up" data-sizes=auto data-srcset="/blog/java-memory/grafana-pod-jvm_hu12a58f424aec7cdcc17f03f316157eb4_923623_61e6a508d0e55ab690f1732a8b8bad9a.webp 480w, /blog/java-memory/grafana-pod-jvm_hu12a58f424aec7cdcc17f03f316157eb4_923623_43cd1a61a8dc98723a954da95eb420e1.webp 640w, /blog/java-memory/grafana-pod-jvm_hu12a58f424aec7cdcc17f03f316157eb4_923623_cb462154965e78dcbb5423773130b6f7.webp 800w, /blog/java-memory/grafana-pod-jvm_hu12a58f424aec7cdcc17f03f316157eb4_923623_ee14b4f00035d4f54690835ae3dbdf57.webp 1024w" fetchpriority=low height=1360 id=h-rh-i-0 src=/blog/java-memory/grafana-pod-jvm_hu12a58f424aec7cdcc17f03f316157eb4_923623_0462e181b3402abf6de645df3b58ebff.webp width=765></a></p><h2 id=统计指标>统计指标</h2><p><code>Pod 内存使用量</code>统计的指标是 <code>container_memory_working_set_bytes</code>：</p><ul><li>container_memory_usage_bytes = container_memory_rss + container_memory_cache + kernel memory</li><li>container_memory_working_set_bytes = container_memory_usage_bytes - total_inactive_file（未激活的匿名缓存页）</li></ul><p>container_memory_working_set_bytes 是容器真实使用的内存量，也是资源限制 limit 时的 OOM 判断依据。</p><p>另外注意 cgroup 版本差异： <code>container_memory_cache</code> reflects <code>cache (cgroup v1)</code> or <code>file (cgroup v2)</code> entry in memory.stat.</p><p><code>JVM 内存使用量</code>统计的指标是 <code>jvm_memory_bytes_used</code>： heap、non-heap 以及<code>其他</code> 真实用量总和。下面解释其他。</p><p>对比一下 top 命令，使用 top 命令看一下 Java 进程真正占了多少。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>top -p $(pgrep java)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=gp>#</span> 注意下面的数据和截图不是同一时间的
</span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>   PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
</span></span></span><span class=line><span class=cl><span class=go>     7 root      20   0   58.2g   1.6g   1.0g S  17.3  2.3 659:42.46 java
</span></span></span></code></pre></div><p>VIRT：virtual memory usage 虚拟内存</p><ol><li>进程“需要的”虚拟内存大小，包括进程使用的库、代码、数据等</li><li>假如进程申请 100m 的内存，但实际只使用了 10m，那么它会增长 100m，而不是实际的使用量</li></ol><p>RES：resident memory usage 常驻内存</p><ol><li>进程当前使用的内存大小，但不包括被换出到交换区的部分</li><li>包含其他进程的共享</li><li>如果申请 100m 的内存，实际使用 10m，它只增长 10m，与 VIRT 相反</li><li>关于库占用内存的情况，它只统计加载的库文件所占内存大小</li></ol><p>SHR：shared memory 共享内存</p><ol><li>除了自身进程的共享内存，也包括其他进程的共享内存</li><li>虽然进程只使用了几个共享库的函数，但它包含了整个共享库的大小</li><li>计算某个进程所占的物理内存大小公式：RES – SHR。（JVM 的内存使用量等于 RES-SHR）</li></ol><p>container_memory_working_set_bytes 与 Top RES 相等吗。</p><p>为什么 container_memory_working_set_bytes 大于 top RES:</p><p>因为 container_memory_working_set_bytes 包含 container_memory_cache，这涉及到 <code>Linux 缓存机制</code>，延伸阅读：<a href=https://zhuanlan.zhihu.com/p/449630026>https://zhuanlan.zhihu.com/p/449630026</a>。遇到这种场景一般都是文件操作较多，可优先排除文件类操作。</p><p>为什么 container_memory_working_set_bytes 小于 top RES:</p><p>主要还是算法和数据来源不一样，top 的 <code>RES=Code + Data</code>，有些服务 Data 比较大。 当然实际测试会发现 RES!=Code + Data ，延伸阅读：<a href=https://liam.page/2020/07/17/memory-stat-in-TOP/>https://liam.page/2020/07/17/memory-stat-in-TOP/</a></p><p>另外可能看到的现象，top、granfana、docker stats、JMX 看到的使用量怎么都不一样，都是因为他们统计的维度不一样。</p><p>所以通过 top 命令看到的数据不一定是真实的，container_memory_working_set_bytes 指标来自 cadvisor，cadvisor 数据来源 cgroup，可以查看以下文件获取真实的内存情况。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=gp>#</span> 老版本使用 cgroup v1
</span></span><span class=line><span class=cl><span class=go>ll /sys/fs/cgroup/memory/
</span></span></span><span class=line><span class=cl><span class=go>total 0
</span></span></span><span class=line><span class=cl><span class=go>drwxr-xr-x.  2 root root   0 Dec 24 19:22 ./
</span></span></span><span class=line><span class=cl><span class=go>dr-xr-xr-x. 13 root root 340 Dec 24 19:22 ../
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r--.  1 root root   0 Dec 24 19:22 cgroup.clone_children
</span></span></span><span class=line><span class=cl><span class=go>--w--w--w-.  1 root root   0 Dec 24 19:22 cgroup.event_control
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r--.  1 root root   0 Dec 24 19:22 cgroup.procs
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r--.  1 root root   0 Dec 24 19:22 memory.failcnt
</span></span></span><span class=line><span class=cl><span class=go>--w-------.  1 root root   0 Dec 24 19:22 memory.force_empty
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r--.  1 root root   0 Dec 24 19:22 memory.kmem.failcnt
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r--.  1 root root   0 Dec 24 19:22 memory.kmem.limit_in_bytes
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r--.  1 root root   0 Dec 24 19:22 memory.kmem.max_usage_in_bytes
</span></span></span><span class=line><span class=cl><span class=go>-r--r--r--.  1 root root   0 Dec 24 19:22 memory.kmem.slabinfo
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r--.  1 root root   0 Dec 24 19:22 memory.kmem.tcp.failcnt
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r--.  1 root root   0 Dec 24 19:22 memory.kmem.tcp.limit_in_bytes
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r--.  1 root root   0 Dec 24 19:22 memory.kmem.tcp.max_usage_in_bytes
</span></span></span><span class=line><span class=cl><span class=go>-r--r--r--.  1 root root   0 Dec 24 19:22 memory.kmem.tcp.usage_in_bytes
</span></span></span><span class=line><span class=cl><span class=go>-r--r--r--.  1 root root   0 Dec 24 19:22 memory.kmem.usage_in_bytes
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r--.  1 root root   0 Dec 24 19:22 memory.limit_in_bytes
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r--.  1 root root   0 Dec 24 19:22 memory.max_usage_in_bytes
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r--.  1 root root   0 Dec 24 19:22 memory.memsw.failcnt
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r--.  1 root root   0 Dec 24 19:22 memory.memsw.limit_in_bytes
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r--.  1 root root   0 Dec 24 19:22 memory.memsw.max_usage_in_bytes
</span></span></span><span class=line><span class=cl><span class=go>-r--r--r--.  1 root root   0 Dec 24 19:22 memory.memsw.usage_in_bytes
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r--.  1 root root   0 Dec 24 19:22 memory.move_charge_at_immigrate
</span></span></span><span class=line><span class=cl><span class=go>-r--r--r--.  1 root root   0 Dec 24 19:22 memory.numa_stat
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r--.  1 root root   0 Dec 24 19:22 memory.oom_control
</span></span></span><span class=line><span class=cl><span class=go>----------.  1 root root   0 Dec 24 19:22 memory.pressure_level
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r--.  1 root root   0 Dec 24 19:22 memory.soft_limit_in_bytes
</span></span></span><span class=line><span class=cl><span class=go>-r--r--r--.  1 root root   0 Dec 24 19:22 memory.stat
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r--.  1 root root   0 Dec 24 19:22 memory.swappiness
</span></span></span><span class=line><span class=cl><span class=go>-r--r--r--.  1 root root   0 Dec 24 19:22 memory.usage_in_bytes
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r--.  1 root root   0 Dec 24 19:22 memory.use_hierarchy
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r--.  1 root root   0 Dec 24 19:22 notify_on_release
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r--.  1 root root   0 Dec 24 19:22 tasks
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=gp>#</span> 新版本使用 cgroup v2
</span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>ll /sys/fs/cgroup/memory.*
</span></span></span><span class=line><span class=cl><span class=go>-r--r--r-- 1 root root 0 Jan  6 16:25 /sys/fs/cgroup/memory.current
</span></span></span><span class=line><span class=cl><span class=go>-r--r--r-- 1 root root 0 Jan  6 16:25 /sys/fs/cgroup/memory.events
</span></span></span><span class=line><span class=cl><span class=go>-r--r--r-- 1 root root 0 Jan  6 16:25 /sys/fs/cgroup/memory.events.local
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r-- 1 root root 0 Jan  7 11:50 /sys/fs/cgroup/memory.high
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r-- 1 root root 0 Jan  7 11:50 /sys/fs/cgroup/memory.low
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r-- 1 root root 0 Jan  7 11:50 /sys/fs/cgroup/memory.max
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r-- 1 root root 0 Jan  6 16:25 /sys/fs/cgroup/memory.min
</span></span></span><span class=line><span class=cl><span class=go>-r--r--r-- 1 root root 0 Jan  6 16:25 /sys/fs/cgroup/memory.numa_stat
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r-- 1 root root 0 Jan  7 11:50 /sys/fs/cgroup/memory.oom.group
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r-- 1 root root 0 Jan  6 16:25 /sys/fs/cgroup/memory.pressure
</span></span></span><span class=line><span class=cl><span class=go>-r--r--r-- 1 root root 0 Jan  6 16:25 /sys/fs/cgroup/memory.stat
</span></span></span><span class=line><span class=cl><span class=go>-r--r--r-- 1 root root 0 Jan  6 16:25 /sys/fs/cgroup/memory.swap.current
</span></span></span><span class=line><span class=cl><span class=go>-r--r--r-- 1 root root 0 Jan  6 16:25 /sys/fs/cgroup/memory.swap.events
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r-- 1 root root 0 Jan  6 16:25 /sys/fs/cgroup/memory.swap.high
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r-- 1 root root 0 Jan  6 16:25 /sys/fs/cgroup/memory.swap.max
</span></span></span></code></pre></div><p>JVM 关于使用量和提交量的解释。</p><p><code>Used Size</code>：The used space is the amount of memory that is currently occupied by Java objects.
当前实际真的用着的内存，每个 bit 都对应了有值的。</p><p><code>Committed Size</code>：The committed size is the amount of memory guaranteed to be available for use by the Java virtual machine.<br>操作系统向 JVM 保证可用的内存大小，或者说 JVM 向操作系统已经要的内存。站在操作系统的角度，就是已经分出去（占用）的内存，保证给 JVM 用了，其他进程不能用了。 由于操作系统的内存管理是惰性的，对于已申请的内存虽然会分配地址空间，但并不会直接占用物理内存，真正使用的时候才会映射到实际的物理内存，所以 committed > res 也是很可能的。</p><h2 id=java-进程内存分析>Java 进程内存分析</h2><p>Pod 的内存使用量 1.5G，都包含哪些。</p><p>kernel memory 为 0，Cache 约 1100M，rss 约 650M，inactive_file 约 200M。可以看到 Cache 比较大，因为这个服务比较特殊有很多文件操作。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=gp>#</span> 这个数据和上面的 1.5G 不是同时的。
</span></span><span class=line><span class=cl><span class=go>cat  /sys/fs/cgroup/memory/memory.stat
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>cache 1455861760
</span></span></span><span class=line><span class=cl><span class=go>rss 685862912
</span></span></span><span class=line><span class=cl><span class=go>rss_huge 337641472
</span></span></span><span class=line><span class=cl><span class=go>mapped_file 504979456
</span></span></span><span class=line><span class=cl><span class=go>swap 0
</span></span></span><span class=line><span class=cl><span class=go>inactive_anon 805306368
</span></span></span><span class=line><span class=cl><span class=go>active_anon 685817856
</span></span></span><span class=line><span class=cl><span class=go>inactive_file 299671552
</span></span></span><span class=line><span class=cl><span class=go>active_file 350883840
</span></span></span><span class=line><span class=cl><span class=go>total_rss 685862912
</span></span></span><span class=line><span class=cl><span class=go>total_rss_huge 337641472
</span></span></span><span class=line><span class=cl><span class=go>total_mapped_file 504979456
</span></span></span><span class=line><span class=cl><span class=go>total_inactive_file 299671552
</span></span></span><span class=line><span class=cl><span class=go>total_active_file 350883840
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=gp>#</span> cgroup v2 变量变了
</span></span><span class=line><span class=cl><span class=go>cat /sys/fs/cgroup/memory.stat
</span></span></span><span class=line><span class=cl><span class=go>anon 846118912
</span></span></span><span class=line><span class=cl><span class=go>file 2321530880
</span></span></span><span class=line><span class=cl><span class=go>kernel_stack 10895360
</span></span></span><span class=line><span class=cl><span class=go>pagetables 15523840
</span></span></span><span class=line><span class=cl><span class=go>percpu 0
</span></span></span><span class=line><span class=cl><span class=go>sock 1212416
</span></span></span><span class=line><span class=cl><span class=go>shmem 1933574144
</span></span></span><span class=line><span class=cl><span class=go>file_mapped 1870290944
</span></span></span><span class=line><span class=cl><span class=go>file_dirty 12288
</span></span></span><span class=line><span class=cl><span class=go>file_writeback 0
</span></span></span><span class=line><span class=cl><span class=go>swapcached 0
</span></span></span><span class=line><span class=cl><span class=go>anon_thp 0
</span></span></span><span class=line><span class=cl><span class=go>file_thp 0
</span></span></span><span class=line><span class=cl><span class=go>shmem_thp 0
</span></span></span><span class=line><span class=cl><span class=go>inactive_anon 2602876928
</span></span></span><span class=line><span class=cl><span class=go>active_anon 176771072
</span></span></span><span class=line><span class=cl><span class=go>inactive_file 188608512
</span></span></span><span class=line><span class=cl><span class=go>active_file 199348224
</span></span></span><span class=line><span class=cl><span class=go>unevictable 0
</span></span></span><span class=line><span class=cl><span class=go>slab_reclaimable 11839688
</span></span></span><span class=line><span class=cl><span class=go>slab_unreclaimable 7409400
</span></span></span><span class=line><span class=cl><span class=go>slab 19249088
</span></span></span><span class=line><span class=cl><span class=go>workingset_refault_anon 0
</span></span></span><span class=line><span class=cl><span class=go>workingset_refault_file 318
</span></span></span><span class=line><span class=cl><span class=go>workingset_activate_anon 0
</span></span></span><span class=line><span class=cl><span class=go>workingset_activate_file 95
</span></span></span><span class=line><span class=cl><span class=go>workingset_restore_anon 0
</span></span></span><span class=line><span class=cl><span class=go>workingset_restore_file 0
</span></span></span><span class=line><span class=cl><span class=go>workingset_nodereclaim 0
</span></span></span><span class=line><span class=cl><span class=go>pgfault 2563565
</span></span></span><span class=line><span class=cl><span class=go>pgmajfault 15
</span></span></span><span class=line><span class=cl><span class=go>pgrefill 14672
</span></span></span><span class=line><span class=cl><span class=go>pgscan 25468
</span></span></span><span class=line><span class=cl><span class=go>pgsteal 25468
</span></span></span><span class=line><span class=cl><span class=go>pgactivate 106436
</span></span></span><span class=line><span class=cl><span class=go>pgdeactivate 14672
</span></span></span><span class=line><span class=cl><span class=go>pglazyfree 0
</span></span></span><span class=line><span class=cl><span class=go>pglazyfreed 0
</span></span></span><span class=line><span class=cl><span class=go>thp_fault_alloc 0
</span></span></span><span class=line><span class=cl><span class=go>thp_collapse_alloc 0
</span></span></span></code></pre></div><p>通过 Java 自带的 Native Memory Tracking 看下内存提交量。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Java 启动时先打开 NativeMemoryTracking，默认是关闭的。注意不要在生产环境长期开启，有性能损失</span>
</span></span><span class=line><span class=cl>java -XX:NativeMemoryTracking<span class=o>=</span>detail -jar
</span></span><span class=line><span class=cl><span class=c1># 查看详情</span>
</span></span><span class=line><span class=cl>jcmd <span class=k>$(</span>pgrep java<span class=k>)</span> VM.native_memory detail <span class=nv>scale</span><span class=o>=</span>MB
</span></span></code></pre></div><p>通过 Native Memory Tracking 追踪到的详情大致如下，关注其中每一项 <code>committed</code> 值。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>Native Memory Tracking:
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>(Omitting categories weighting less than 1MB)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>Total: reserved=68975MB, committed=1040MB
</span></span></span><span class=line><span class=cl><span class=go>-                 Java Heap (reserved=58944MB, committed=646MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (mmap: reserved=58944MB, committed=646MB)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>-                     Class (reserved=1027MB, committed=15MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (classes #19551)  #加载类的个数
</span></span></span><span class=line><span class=cl><span class=go>                            (  instance classes #18354, array classes #1197)
</span></span></span><span class=line><span class=cl><span class=go>                            (malloc=3MB #63653)
</span></span></span><span class=line><span class=cl><span class=go>                            (mmap: reserved=1024MB, committed=12MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (  Metadata:   )
</span></span></span><span class=line><span class=cl><span class=go>                            (    reserved=96MB, committed=94MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (    used=93MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (    waste=0MB =0.40%)
</span></span></span><span class=line><span class=cl><span class=go>                            (  Class space:)
</span></span></span><span class=line><span class=cl><span class=go>                            (    reserved=1024MB, committed=12MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (    used=11MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (    waste=1MB =4.63%)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>-                    Thread (reserved=337MB, committed=37MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (thread #335) #线程的个数
</span></span></span><span class=line><span class=cl><span class=go>                            (stack: reserved=336MB, committed=36MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (malloc=1MB #2018)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>-                      Code (reserved=248MB, committed=86MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (malloc=6MB #24750)
</span></span></span><span class=line><span class=cl><span class=go>                            (mmap: reserved=242MB, committed=80MB)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>-                        GC (reserved=8243MB, committed=83MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (malloc=19MB #45814)
</span></span></span><span class=line><span class=cl><span class=go>                            (mmap: reserved=8224MB, committed=64MB)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>-                  Compiler (reserved=3MB, committed=3MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (malloc=3MB #2212)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>-                  Internal (reserved=7MB, committed=7MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (malloc=7MB #31683)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>-                     Other (reserved=18MB, committed=18MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (malloc=18MB #663)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>-                    Symbol (reserved=19MB, committed=19MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (malloc=17MB #502325)
</span></span></span><span class=line><span class=cl><span class=go>                            (arena=2MB #1)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>-    Native Memory Tracking (reserved=12MB, committed=12MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (malloc=1MB #8073)
</span></span></span><span class=line><span class=cl><span class=go>                            (tracking overhead=11MB)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>-        Shared class space (reserved=12MB, committed=12MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (mmap: reserved=12MB, committed=12MB)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>-                    Module (reserved=1MB, committed=1MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (malloc=1MB #4996)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>-           Synchronization (reserved=1MB, committed=1MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (malloc=1MB #2482)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>-                 Metaspace (reserved=97MB, committed=94MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (malloc=1MB #662)
</span></span></span><span class=line><span class=cl><span class=go>                            (mmap: reserved=96MB, committed=94MB)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>-           Object Monitors (reserved=8MB, committed=8MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (malloc=8MB #39137)
</span></span></span></code></pre></div><p>一图解千愁，盗图。</p><p><a href=./jvm-memory-structure.png><img alt="JVM 内存结构" class="img-fluid lazyload blur-up" data-sizes=auto data-srcset="/blog/java-memory/jvm-memory-structure_hua94036a798b23daa5cb7c6e1e4bce7cb_32775_a428c8efaf2fb0b675217bdfebf664f1.webp 480w, /blog/java-memory/jvm-memory-structure_hua94036a798b23daa5cb7c6e1e4bce7cb_32775_0eda6dabb7ca9bc8dc811dcd3ce09242.webp 640w, /blog/java-memory/jvm-memory-structure_hua94036a798b23daa5cb7c6e1e4bce7cb_32775_1d3cf72143cc54ca153183e2fc055937.webp 800w, /blog/java-memory/jvm-memory-structure_hua94036a798b23daa5cb7c6e1e4bce7cb_32775_240674b6aaf4c2e88b3fbdaa83887413.webp 1024w" fetchpriority=low height=765 id=h-rh-i-1 src=/blog/java-memory/jvm-memory-structure_hua94036a798b23daa5cb7c6e1e4bce7cb_32775_c6ccc8006ea28a99025e51193c6a4439.webp width=1360></a></p><ul><li><p>Heap
Heap 是 Java 进程中使用量最大的一部分内存，是最常遇到内存问题的部分，Java 也提供了很多相关工具来排查堆内存泄露问题，这里不详细展开。Heap 与 RSS 相关的几个重要 JVM 参数如下：
Xms：Java Heap 初始内存大小。（目前我们用的百分比控制，MaxRAMPercentage)
Xmx：Java Heap 的最大大小。(InitialRAMPercentage)
XX:+UseAdaptiveSizePolicy：是否开启自适应大小策略。开启后，JVM 将动态判断是否调整 Heap size，来降低系统负载。</p></li><li><p>Metaspace
Metaspace 主要包含方法的字节码，Class 对象，常量池。一般来说，记载的类越多，Metaspace 使用的内存越多。与 Metaspace 相关的 JVM 参数有：
XX:MaxMetaspaceSize: 最大的 Metaspace 大小限制【默认无限制】
XX:MetaspaceSize=64M: 初始的 Metaspace 大小。如果 Metaspace 空间不足，将会触发 Full GC。
类空间占用评估，给两个数字可供参考：10K 个类约 90M，15K 个类约 100M。
什么时候回收：分配给一个类的空间，是归属于这个类的类加载器的，只有当这个类加载器卸载的时候，这个空间才会被释放。释放 Metaspace 的空间，并不意味着将这部分空间还给系统内存，这部分空间通常会被 JVM 保留下来。
扩展：参考资料中的<code>Java Metaspace 详解</code>，这里完美解释 Metaspace、Compressed Class Space 等。</p></li><li><p>Thread
NMT 中显示的 Thread 部分内存与线程数与 -Xss 参数成正比，一般来说 committed 内存等于 <code>Xss *线程数</code> 。</p></li><li><p>Code
JIT 动态编译产生的 Code 占用的内存。这部分内存主要由-XX:ReservedCodeCacheSize 参数进行控制。</p></li><li><p>Internal
Internal 包含命令行解析器使用的内存、JVMTI、PerfData 以及 Unsafe 分配的内存等等。
需要注意的是，Unsafe_AllocateMemory 分配的内存在 JDK11 之前，在 NMT 中都属于 Internal，但是在 JDK11 之后被 NMT 归属到 Other 中。</p></li><li><p>Symbol
Symbol 为 JVM 中的符号表所使用的内存，HotSpot 中符号表主要有两种：SymbolTable 与 StringTable。
大家都知道 Java 的类在编译之后会生成 Constant pool 常量池，常量池中会有很多的字符串常量，HotSpot 出于节省内存的考虑，往往会将这些字符串常量作为一个 Symbol 对象存入一个 HashTable 的表结构中即 SymbolTable，如果该字符串可以在 SymbolTable 中 lookup（SymbolTable::lookup）到，那么就会重用该字符串，如果找不到才会创建新的 Symbol（SymbolTable::new_symbol）。
当然除了 SymbolTable，还有它的双胞胎兄弟 StringTable（StringTable 结构与 SymbolTable 基本是一致的，都是 HashTable 的结构），即我们常说的字符串常量池。平时做业务开发和 StringTable 打交道会更多一些，HotSpot 也是基于节省内存的考虑为我们提供了 StringTable，我们可以通过 String.intern 的方式将字符串放入 StringTable 中来重用字符串。</p></li><li><p>Native Memory Tracking
Native Memory Tracking 使用的内存就是 JVM 进程开启 NMT 功能后，NMT 功能自身所申请的内存。</p></li></ul><p>观察上面几个区域的分配，没有明显的异常。</p><p>NMT 追踪到的 是 Committed，不一定是 Used，NMT 和 cadvisor 没有找到必然的对应的关系。可以参考 RSS，cadvisor 追踪到 RSS 是 650M，JVM Used 是 500M，还有大约 150M 浮动到哪里去了。</p><p>因为 NMT 只能 Track JVM 自身的内存分配情况，比如：Heap 内存分配，direct byte buffer 等。无法追踪的情况主要包括：</p><ul><li>使用 JNI 调用的一些第三方 native code 申请的内存，比如使用 System.Loadlibrary 加载的一些库。</li><li>标准的 Java Class Library，典型的，如文件流等相关操作（如：Files.list、ZipInputStream 和 DirectoryStream 等）。主要涉及到的调用是 Unsafe.allocateMemory 和 java.util.zip.Inflater.init(Native Method)。</li></ul><p>怎么追踪 NMT 追踪不到的<code>其他内存</code>，目前是安装了 jemalloc 内存分析工具，他能追踪底层内存的分配情况输出报告。</p><p>通过 jemalloc 内存分析工具佐证了上面的结论，Unsafe.allocateMemory 和 java.util.zip.Inflater.init 占了 30%，基本吻合。</p><p><a href=./jemalloc-jvm.png><img alt="Jemalloc 内存结果" class="img-fluid lazyload blur-up" data-sizes=auto data-srcset="/blog/java-memory/jemalloc-jvm_hub666aad971ed4b33a5c9e63efe387f78_114856_5f4522a620a89f3b9ce0b765bda7bc28.webp 480w, /blog/java-memory/jemalloc-jvm_hub666aad971ed4b33a5c9e63efe387f78_114856_5fe5b6c5666df7ed2a751bcb543f9386.webp 640w, /blog/java-memory/jemalloc-jvm_hub666aad971ed4b33a5c9e63efe387f78_114856_42de5f6b363301a83c89aea3cc279d52.webp 800w, /blog/java-memory/jemalloc-jvm_hub666aad971ed4b33a5c9e63efe387f78_114856_7d937a035bcd6129ea991cb884bf209f.webp 1024w" fetchpriority=low height=765 id=h-rh-i-2 src=/blog/java-memory/jemalloc-jvm_hub666aad971ed4b33a5c9e63efe387f78_114856_47bedc7fc51108a9fa6b6a3f87f0ce96.webp width=1360></a></p><p>启动 arthas 查看下类调用栈，在 arthas 里执行以下命令：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># 先设置 unsafe true</span>
</span></span><span class=line><span class=cl>options unsafe <span class=nb>true</span>
</span></span><span class=line><span class=cl><span class=c1># 这个没有</span>
</span></span><span class=line><span class=cl>stack sun.misc.Unsafe allocateMemory
</span></span><span class=line><span class=cl><span class=c1># 这个有</span>
</span></span><span class=line><span class=cl>stack jdk.internal.misc.Unsafe allocateMemory
</span></span><span class=line><span class=cl>stack java.util.zip.Inflater inflate
</span></span></code></pre></div><p>通过上面的命令，能看到 MongoDB 和 netty 一直在申请使用内存。注意：早期的 mongodb client 确实有无法释放内存的 bug，但是在我们场景，长期观察会发现内存申请了逐渐释放了，没有持续增长。回到开头的 ContainerOOM 问题，可能一个原因是流量突增，MongoDB 申请了更多的内存导致 OOM，而不是因为内存不释放。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=go>ts=2022-12-29 21:20:01;thread_name=ForkJoinPool.commonPool-worker-1;id=22;is_daemon=true;priority=1;TCCL=jdk.internal.loader.ClassLoaders$AppClassLoader@1d44bcfa
</span></span></span><span class=line><span class=cl><span class=go>    @jdk.internal.misc.Unsafe.allocateMemory()
</span></span></span><span class=line><span class=cl><span class=go>        at java.nio.DirectByteBuffer.&lt;init&gt;(DirectByteBuffer.java:125)
</span></span></span><span class=line><span class=cl><span class=go>        at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:332)
</span></span></span><span class=line><span class=cl><span class=go>        at sun.nio.ch.Util.getTemporaryDirectBuffer(Util.java:243)
</span></span></span><span class=line><span class=cl><span class=go>        at sun.nio.ch.NioSocketImpl.tryWrite(NioSocketImpl.java:394)
</span></span></span><span class=line><span class=cl><span class=go>        at sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:413)
</span></span></span><span class=line><span class=cl><span class=go>        at sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:440)
</span></span></span><span class=line><span class=cl><span class=go>        at sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:826)
</span></span></span><span class=line><span class=cl><span class=go>        at java.net.Socket$SocketOutputStream.write(Socket.java:1035)
</span></span></span><span class=line><span class=cl><span class=go>        at com.mongodb.internal.connection.SocketStream.write(SocketStream.java:99)
</span></span></span><span class=line><span class=cl><span class=go>        at com.mongodb.internal.connection.InternalStreamConnection.sendMessage(InternalStreamConnection.java:426)
</span></span></span><span class=line><span class=cl><span class=go>        at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:99)
</span></span></span><span class=line><span class=cl><span class=go>        at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:444)
</span></span></span><span class=line><span class=cl><span class=go>        ………………………………
</span></span></span><span class=line><span class=cl><span class=go>        at com.mongodb.MongoClientExt$1.execute(MongoClientExt.java:42)
</span></span></span><span class=line><span class=cl><span class=go>        at com.facishare.oms.thirdpush.dao.MongoDao.save(MongoDao.java:31)
</span></span></span><span class=line><span class=cl><span class=go>        ………………………………
</span></span></span></code></pre></div><p>总结 Java 进程内存占用：Total=heap + non-heap + 上面说的这个其他。</p><h2 id=jemalloc>jemalloc</h2><p>jemalloc 是一个比 glibc malloc 更高效的内存池技术，在 Facebook 公司被大量使用，在 FreeBSD 和 FireFox 项目中使用了 jemalloc 作为默认的内存管理器。使用 jemalloc 可以使程序的内存管理性能提升，减少内存碎片。</p><p>比如 Redis 内存分配默认使用的 jemalloc，早期版本安装 redis 是需要手动安装 jemalloc 的，现在 redis 应该是在编译期内置好了。</p><p>原来使用 jemalloc 是为了分析内存占用，通过 jemalloc 输出当前内存分配情况，或者通过 diff 分析前后内存差，大概能看出内存都分给睡了，占了多少，是否有内存无法释放的情况。</p><p>后来参考了这个文章，把 glibc 换成 jemalloc 带来性能提升，降低内存使用，决定一试。</p><p>how we’ve reduced memory usage without changing any code：<a href=https://blog.malt.engineering/java-in-k8s-how-weve-reduced-memory-usage-without-changing-any-code-cbef5d740ad>https://blog.malt.engineering/java-in-k8s-how-weve-reduced-memory-usage-without-changing-any-code-cbef5d740ad</a></p><p>Decreasing RAM Usage by 40% Using jemalloc with Python & Celery: <a href=https://zapier.com/engineering/celery-python-jemalloc/>https://zapier.com/engineering/celery-python-jemalloc/</a></p><p>一个服务，运行一周，观察效果。</p><p>使用 Jemalloc 之前：
<a href=./before-jemalloc.png><img alt=before class="img-fluid lazyload blur-up" data-sizes=auto data-srcset="/blog/java-memory/before-jemalloc_hubc7802af298d8edfcae1526727448899_739790_c6d9b2b7a014bd96cc89f9885ac5990b.webp 480w, /blog/java-memory/before-jemalloc_hubc7802af298d8edfcae1526727448899_739790_a9d6afddcbf8389f5667f3ac40eb4c49.webp 640w, /blog/java-memory/before-jemalloc_hubc7802af298d8edfcae1526727448899_739790_44a73c5bd03f19df79cf80144eaba77c.webp 800w, /blog/java-memory/before-jemalloc_hubc7802af298d8edfcae1526727448899_739790_5a13e87afa53b39e929d1ea7c75973c6.webp 1024w" fetchpriority=low height=1360 id=h-rh-i-3 src=/blog/java-memory/before-jemalloc_hubc7802af298d8edfcae1526727448899_739790_18387448d5f36bf80c77ced6d7e9aa7f.webp width=765></a></p><p>使用 Jemalloc 之后（同时调低了 Pod 内存）：
<a href=./after-jemalloc.png><img alt=after class="img-fluid lazyload blur-up" data-sizes=auto data-srcset="/blog/java-memory/after-jemalloc_hu87dee34f67a08dd4323707039b7e835c_819874_f411356698ed74f7be759c59609fc051.webp 480w, /blog/java-memory/after-jemalloc_hu87dee34f67a08dd4323707039b7e835c_819874_15bc2981a449ef9a1efd74f164a7882a.webp 640w, /blog/java-memory/after-jemalloc_hu87dee34f67a08dd4323707039b7e835c_819874_f444ae4b11b74cdece1833360af171d6.webp 800w, /blog/java-memory/after-jemalloc_hu87dee34f67a08dd4323707039b7e835c_819874_75187b40f37d182d108a84c607dcf72a.webp 1024w" fetchpriority=low height=1360 id=h-rh-i-4 src=/blog/java-memory/after-jemalloc_hu87dee34f67a08dd4323707039b7e835c_819874_6c771b2598da641cc52c962f18903b0f.webp width=765></a></p><p>注：以上结果未经生产长期检验。</p><h2 id=内存交还给操作系统>内存交还给操作系统</h2><p>注意：下面的操作，生产环境不建议这么干。</p><p>默认情况下，OpenJDK 不会主动向操作系统退还未用的内存（不严谨）。看第一张监控的图，会发现运行一段时间后，Pod 的内存使用量一直稳定在 80%&ndash;90%不再波动。</p><p>其实对于 Java 程序，浮动比较大的就是 heap 内存。其他区域 Code、Metaspace 基本稳定</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=gp>#</span> 执行命令获取当前 heap 情况
</span></span><span class=line><span class=cl><span class=go>jhsdb jmap --heap --pid $(pgrep java)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=gp>#</span>以下为输出
</span></span><span class=line><span class=cl><span class=go>Attaching to process ID 7, please wait...
</span></span></span><span class=line><span class=cl><span class=go>Debugger attached successfully.
</span></span></span><span class=line><span class=cl><span class=go>Server compiler detected.
</span></span></span><span class=line><span class=cl><span class=go>JVM version is 17.0.5+8-LTS
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>using thread-local object allocation.
</span></span></span><span class=line><span class=cl><span class=go>ZGC with 4 thread(s)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>Heap Configuration:
</span></span></span><span class=line><span class=cl><span class=go>   MinHeapFreeRatio         = 40
</span></span></span><span class=line><span class=cl><span class=go>   MaxHeapFreeRatio         = 70
</span></span></span><span class=line><span class=cl><span class=go>   MaxHeapSize              = 1287651328 (1228.0MB)
</span></span></span><span class=line><span class=cl><span class=go>   NewSize                  = 1363144 (1.2999954223632812MB)
</span></span></span><span class=line><span class=cl><span class=go>   MaxNewSize               = 17592186044415 MB
</span></span></span><span class=line><span class=cl><span class=go>   OldSize                  = 5452592 (5.1999969482421875MB)
</span></span></span><span class=line><span class=cl><span class=go>   NewRatio                 = 2
</span></span></span><span class=line><span class=cl><span class=go>   SurvivorRatio            = 8
</span></span></span><span class=line><span class=cl><span class=go>   MetaspaceSize            = 22020096 (21.0MB)
</span></span></span><span class=line><span class=cl><span class=go>   CompressedClassSpaceSize = 1073741824 (1024.0MB)
</span></span></span><span class=line><span class=cl><span class=go>   MaxMetaspaceSize         = 17592186044415 MB
</span></span></span><span class=line><span class=cl><span class=go>   G1HeapRegionSize         = 0 (0.0MB)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>Heap Usage:
</span></span></span><span class=line><span class=cl><span class=go> ZHeap          used 310M, capacity 710M, max capacity 1228M
</span></span></span></code></pre></div><p>Java 内存不交还，几种情况：</p><ul><li><p>Xms 大于实际需要的内存，比如我们服务设置了 Xms768M，但是实际上只需要 256，高峰期也就 512，到不了 Xms 的值也就无所谓归还。
<a href=./java-heap-use.png><img alt=Xms class="img-fluid lazyload blur-up" data-sizes=auto data-srcset="/blog/java-memory/java-heap-use_hu5ff2e654956fe5b4bffae42dc1968a6f_39757_2bbbb4bd0a08c780c6d28f0a10179f0b.webp 480w, /blog/java-memory/java-heap-use_hu5ff2e654956fe5b4bffae42dc1968a6f_39757_2698f63bd1f0d311b8dbe9284d51eb45.webp 640w, /blog/java-memory/java-heap-use_hu5ff2e654956fe5b4bffae42dc1968a6f_39757_8613359ae28e915ebb9acfb0d419124c.webp 800w, /blog/java-memory/java-heap-use_hu5ff2e654956fe5b4bffae42dc1968a6f_39757_3dc430e308312349e0ee2003ffafbb20.webp 1024w" fetchpriority=low height=765 id=h-rh-i-5 src=/blog/java-memory/java-heap-use_hu5ff2e654956fe5b4bffae42dc1968a6f_39757_29145ea8184b1f9fc65f61da3609ab08.webp width=1360></a></p></li><li><p>上面 jmap 的结果，可以看到 Java 默认的配置 MaxHeapFreeRatio=70，这个 70% Free 几乎很难达到。（另外注意 Xmx==Xms 的情况下这两个参数无效，因为他怎么扩缩都不会突破 Xms 和 Xmx 的限制）</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=go>  MinHeapFreeRatio         = 40
</span></span></span><span class=line><span class=cl><span class=go>  空闲堆空间的最小百分比，计算公式为：HeapFreeRatio =(CurrentFreeHeapSize/CurrentTotalHeapSize) * 100，值的区间为 0 到 100，默认值为 40。如果 HeapFreeRatio &lt; MinHeapFreeRatio，则需要进行堆扩容，扩容的时机应该在每次垃圾回收之后。
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>  MaxHeapFreeRatio         = 70
</span></span></span><span class=line><span class=cl><span class=go>  空闲堆空间的最大百分比，计算公式为：HeapFreeRatio =(CurrentFreeHeapSize/CurrentTotalHeapSize) * 100，值的区间为 0 到 100，默认值为 70。如果 HeapFreeRatio &gt; MaxHeapFreeRatio，则需要进行堆缩容，缩容的时机应该在每次垃圾回收之后。
</span></span></span></code></pre></div></li></ul><p>对于 ZGC，默认是交还给操作系统的。可通过 <code>-XX:+ZUncommit -XX:ZUncommitDelay=300</code> 这两个参数控制（不再使用的内存最多延迟 300s 归还给 OS，线下环境可以改小点）。</p><p>经过调整后的服务，内存提交在 500&ndash;800M 之间浮动，不再是一条直线。</p><p><a href=./memory-dance.png><img alt=memory-dance class="img-fluid lazyload blur-up" data-sizes=auto data-srcset="/blog/java-memory/memory-dance_hu34b26c276e04a46c5d9869fdc6d53212_111452_22f33c374807b2f78759f09a256b5866.webp 480w, /blog/java-memory/memory-dance_hu34b26c276e04a46c5d9869fdc6d53212_111452_a9bdf89c4c7e3ab761826464c83df597.webp 640w, /blog/java-memory/memory-dance_hu34b26c276e04a46c5d9869fdc6d53212_111452_651ffc7c444b46a1b0a5e9bde90c1186.webp 800w, /blog/java-memory/memory-dance_hu34b26c276e04a46c5d9869fdc6d53212_111452_f967c81185e844fe4112a8cad34e3fb9.webp 1024w" fetchpriority=low height=765 id=h-rh-i-6 src=/blog/java-memory/memory-dance_hu34b26c276e04a46c5d9869fdc6d53212_111452_043484403358ac34c76d3f849e0d2312.webp width=1360></a></p><h2 id=问题原因分析和调整>问题原因分析和调整</h2><p>回到开头问题，通过上面分析，2G 内存，RSS 其实占用 600M，为什么最终还是 ContainerOOM 了。</p><ol><li>kernel memory 为 0，排除 kernel 泄漏的原因。下面的参考资料里介绍了 kernel 泄露的两种场景。</li><li>Cache 很大，说明文件操作多。搜了一下代码，确实有很多 InputStream 调用没有显式关闭，而且有的 InputSteam Root 引用在 ThreadLocal 里，ThreadLocal 只 init 未 remove。 但是，ThreadLocal 的引用对象是线程池，池不回收，所以这部分可能会无法关闭，但是不会递增，但是 cache 也不能回收。
优化办法：ThreadLocal 中对象是线程安全的，无数据传递，直接干掉 ThreadLocal；显式关闭 InputStream。运行一周发现 cache 大约比优化前低 200&ndash;500M。
ThreadLocal 引起内存泄露是 Java 中很经典的一个场景，一定要特别注意。</li><li>一般场景下，Java 程序都是堆内存占用高，但是这个服务堆内存其实在 200-500M 之间浮动，我们给他分了 768M，从来没有到过这个值，所以调低 Xms。留出更多内存给 JNI 使用。</li><li>线下环境内存分配切换到 jemalloc，长期观察大部分效果可以，但是对部分应用基本没有效果。</li></ol><p>经过上述调整以后，线下环境 Pod 内存使用量由 1G 降到 600M 作用。线上环境内存使用量在 50%&ndash;80%之间根据流量大小浮动，原来是 85% 居高不小。</p><h2 id=参考资料>参考资料</h2><p>Java 进程内存分布：<a href=https://cloud.tencent.com/developer/article/1666640>https://cloud.tencent.com/developer/article/1666640</a></p><p>Java Metaspace 详解：<a href=https://www.javadoop.com/post/metaspace>https://www.javadoop.com/post/metaspace</a></p><p>how we’ve reduced memory usage without changing any code：<a href=https://blog.malt.engineering/java-in-k8s-how-weve-reduced-memory-usage-without-changing-any-code-cbef5d740ad>https://blog.malt.engineering/java-in-k8s-how-weve-reduced-memory-usage-without-changing-any-code-cbef5d740ad</a></p><p>Spring Boot 引起的堆外内存泄漏排查及经验总结：<a href=https://tech.meituan.com/2019/01/03/spring-boot-native-memory-leak.html>https://tech.meituan.com/2019/01/03/spring-boot-native-memory-leak.html</a></p><p>Pod 进程内存缓存分析：<a href=https://zhuanlan.zhihu.com/p/449630026>https://zhuanlan.zhihu.com/p/449630026</a></p><p>Linux 内存中的 Cache 真的能被回收么：<a href=https://cloud.tencent.com/developer/article/1115557>https://cloud.tencent.com/developer/article/1115557</a></p><p>Linux kernel memory 导致的 POD OOM killed: <a href=https://www.cnblogs.com/yannwang/p/13287963.html>https://www.cnblogs.com/yannwang/p/13287963.html</a></p><p>cgroup 内存泄露问题：<a href=https://www.cnblogs.com/leffss/p/15019898.html>https://www.cnblogs.com/leffss/p/15019898.html</a></p><div class=tag-list-single><a class="btn btn-light" href=/tags/k8s/ role=button>k8s</a>
<a class="btn btn-light" href=/tags/java/ role=button>Java</a></div></div></div></article></div></div></div><footer class="footer text-muted"><div class=container-fluid><div class=row><div class="col-lg-8 text-center text-lg-start"><ul class=list-inline><li class=list-inline-item><a class=text-muted href=/>Powered by Hugo, and based on Doks</a></li></ul></div><div class="col-lg-8 text-center text-lg-end"><ul class=list-inline><li class=list-inline-item></li></ul></div></div></div></footer><script async src=/js/app.1e84dcdaa5ccef6fe83f6d88ac1e444a1f237111cfa2a7a97427f758e9c27ef4.js integrity="sha256-HoTc2qXM72/oP22IrB5ESh8jcRHPoqepdCf3WOnCfvQ="></script><script async src=/js/flexsearch.78e4b7a9456a2770bd6faffb1f6488b30db12ca157c3df4e89e3ae2e258888ae.js integrity="sha256-eOS3qUVqJ3C9b6/7H2SIsw2xLKFXw99OieOuLiWIiK4="></script><script async src=/js/search-modal.c4c023efcdee308e4b5f1110afe1db6b1656f217e67aa7efe54e2f4bf2feccf8.js integrity="sha256-xMAj783uMI5LXxEQr+HbaxZW8hfmeqfv5U4vS/L+zPg="></script></body></html>