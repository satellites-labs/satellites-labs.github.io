[{"content":"","date":"2023-09-07","id":0,"permalink":"/docs/guides/","summary":"","tags":[],"title":"Guides"},{"content":"云原生技术探索。\n","date":"2023-09-07","id":1,"permalink":"/docs/cloud/introduction/","summary":"云原生技术探索。","tags":[],"title":"Introduction"},{"content":"卫星实验室，一个专注于研究 CRM（Customer Relationship Management，客户关系管理） 的开源组织。\n此项目为卫星实验室主页 xlabs.club 的源码，在这里我们将分享 CRM 领域独有的建设经验，介绍如何以技术驱动 CRM 长期发展和高速增长。\n加入我们，与我们共同共同探索 CRM 技术前沿，解决行业中的挑战。\n主页内容 平台工程：我们的平台工程建设之路，关于 DevOps, DataOps, FinOps 以及 AIOps 的工程实践。 云原生：云原生技术探索，如何以云原生技术支撑起不断变化的复杂业务。 技术博客：研发踩坑记录，翻一翻总有惊喜。 awesome-x-ops：一些关于 AIOps、DataOps、DevOps、GitOps、FinOps 的优秀软件、博客、配套工具。 xlabs-ops：一些运维脚本和模板，如 Argo Workflows 模板仓库，是对官方 Examples 的组合、扩展。 License 本文档采用 CC BY-NC 4.0 许可协议。\n","date":"2023-09-07","id":2,"permalink":"/docs/guides/introduction/","summary":"卫星实验室，一个专注于研究 CRM（Customer Relationship Management，客户关系管理） 的开源组织。\n此项目为卫星实验室主页 xlabs.club 的源码，在这里我们将分享 CRM 领域独有的建设经验，介绍如何以技术驱动 CRM 长期发展和高速增长。\n加入我们，与我们共同共同探索 CRM 技术前沿，解决行业中的挑战。\n主页内容 平台工程：我们的平台工程建设之路，关于 DevOps, DataOps, FinOps 以及 AIOps 的工程实践。 云原生：云原生技术探索，如何以云原生技术支撑起不断变化的复杂业务。 技术博客：研发踩坑记录，翻一翻总有惊喜。 awesome-x-ops：一些关于 AIOps、DataOps、DevOps、GitOps、FinOps 的优秀软件、博客、配套工具。 xlabs-ops：一些运维脚本和模板，如 Argo Workflows 模板仓库，是对官方 Examples 的组合、扩展。 License 本文档采用 CC BY-NC 4.0 许可协议。","tags":[],"title":"Introduction"},{"content":"常用 Kubernetes 命令，复制，粘贴，这就是生活。\n复制 secret 到另一个 namespace。 kubectl get secret mys --namespace=na -oyaml | grep -v \u0026#39;^\\s*namespace:\\s\u0026#39; | kubectl apply --namespace=nb -f -\r批量删除 pod。 kubectl get pods --all-namespaces | grep Evicted | awk \u0026#39;{print $2 \u0026#34; --namespace=\u0026#34; $1}\u0026#39; | xargs kubectl delete pod # Delete by label kubectl delete pod -n idaas-book -l app.kubernetes.io/name=idaas-book\r原地重启 Pod。 kubectl rollout restart deploy/xxx -n your-namespace\r命令行快速扩缩容。 # kubectl scale -h kubectl scale --replicas=1 deploy/xxx -n your-namespace\r密钥解密。 kubectl get secret my-creds -n mysql -o jsonpath=\u0026#34;{.data.ADMIN_PASSWORD}\u0026#34; | base64 --decode\r合并多个 kube config 文件。 export KUBECONFIG=~/.kube/config:~/.kube/anotherconfig kubectl config view --flatten \u0026gt; ~/.kube/config-all cp ~/.kube/config-all ~/.kube/config # 顺手把权限改了，避免 helm 或 kubectl 客户端 warning chmod 600 ~/.kube/config\r获取某个 namespace 下的全部资源，找出你看不见的资源，常用于 CR/CRD 清理。 ns=your-namespace for resource in `kubectl api-resources --verbs=list --namespaced -o name | xargs -n 1 kubectl get -o name -n $ns`; do kubectl get $resource -n $ns; # kubectl patch $resource -p \u0026#39;{\u0026#34;metadata\u0026#34;: {\u0026#34;finalizers\u0026#34;: []}}\u0026#39; --type=\u0026#39;merge\u0026#39; -n $ns; done\r根据特定字段排序 Pod 列表。 # 根据重启次数排序 kubectl get pods --sort-by=\u0026#39;.status.containerStatuses[0].restartCount\u0026#39; -A\r","date":"2023-09-07","id":3,"permalink":"/docs/tldr/kubernetes/","summary":"常用 Kubernetes 命令，复制，粘贴，这就是生活。\n复制 secret 到另一个 namespace。 kubectl get secret mys --namespace=na -oyaml | grep -v \u0026#39;^\\s*namespace:\\s\u0026#39; | kubectl apply --namespace=nb -f -\r批量删除 pod。 kubectl get pods --all-namespaces | grep Evicted | awk \u0026#39;{print $2 \u0026#34; --namespace=\u0026#34; $1}\u0026#39; | xargs kubectl delete pod # Delete by label kubectl delete pod -n idaas-book -l app.kubernetes.io/name=idaas-book\r原地重启 Pod。 kubectl rollout restart deploy/xxx -n your-namespace\r命令行快速扩缩容。 # kubectl scale -h kubectl scale --replicas=1 deploy/xxx -n your-namespace\r密钥解密。 kubectl get secret my-creds -n mysql -o jsonpath=\u0026#34;{.","tags":[],"title":"Kubernetes"},{"content":"我们的平台工程建设之路，介绍前期方案设计、中间踩坑历程。\n原则 分享我们平台工程建设的一些基本原则。\n以开发者为中心：赋能开发者，了解困难，解决问题，让开发者生活更轻松。 自动化：自动化手动和重复性任务，减少人为错误，提高效率。 标准化：标准化保持一致性，减少复杂性，减少团队认知负载，提供最佳实践和统一的编码结构。 模块化：松耦合且独立的模块，可独立开发、测试和部署。 弹性：可扩展水平扩缩容的能力，以及容错抗脆弱的能力。 安全：相比于微服务、云原生领域的安全，在平台工程里，更强调代码、基础设施、数据和其他资源的安全。 协作：平台工程师、开发人员、运维运营人员以及其他参与者之间的协作，提高生产力、促进创新并创造积极包容的工作环境。 持续改进：持续性反馈、评估、改进。 架构概述 为便于理解，我们仍然按照惯用架构模型，将架构分为 IaaS、CaaS、PaaS、Applications 这几个层级。\n专业的运维人员作为 platform engineer 着重于 IaaS、CaaS、PaaS 建设，开发人员作为 application engineer 更专注于 PaaS、Applications 建设，为开发和运维提供工具、协作平台、基础应用。\nC4Context title 平台工程总体架构 Boundary(users, \"Users\", \"用户接入\") { Person(superAdmin, \"超级管理员\") Person(admin, \"平台管理员\") Person(developer, \"开发人员\") Person(maintenance, \"运维人员\") } Boundary(console, \"Console\", \"开发者平台\") { Container(backstage, \"Backstage\",\"react\",\"开发者门户\") Container(apps, \"应用管理平台\",\"Application\",\"容器管理、应用管理、配置管理、自动化测试\") Container(ops, \"统一运维平台\",\"x-ops\",\"数据库、中间件、日志、监控告警平台\") Container(iam, \"IAM\", \"Keycloak\", \"统一用户、组织、角色权限管理\") } Boundary(paas, \"PaaS\", \"PaaS\") { ContainerDb(rds, \"RDS\", \"PostgreSQL/MySQL\", \"PostgreSQL、MySQL 等关系型数据库\") ContainerDb(clickhouse, \"ClickHouse\", \"ClickHouse\", \"BI、Logging、Metrics 等列式数据库\") ContainerDb(nosql, \"NoSQL\", \"NoSQL\", \"ES、Redis、Mongo 等缓存数据库、文档数据库\") ContainerDb(mq, \"消息队列\", \"Kafka\", \"Kafka、RocketMQ 等消息队列\") } Boundary(caas, \"CaaS\", \"CaaS\") { Container(k8s, \"Kubernetes\",\"k8s\",\"K8S 容器平台\") Container(workflow, \"编排引擎\",\"Argo\",\"流水线，流程、应用编排引擎\") Container(kms, \"KMS\",\"HashiCorp Vault\",\"秘钥管理系统\") Container(harbor, \"Harbor\",\"harbor\",\"容器镜像仓库\") Container(git, \"IaC\",\"GitLab\",\"IaC、GitOps 源码仓库\") } Boundary(iaas, \"IaaS\", \"IaaS\") { Container(vm, \"云主机\",\"vm\",\"云主机自带本地存储\") Container(cbh, \"堡垒机\",\"cbh\",\"安全运维审计堡垒机\") Container(s3, \"S3\",\"S3/Minio\",\"分布式对象存储\") Container(nfs, \"NFS\",\"nfs\",\"共享文件存储\") } UpdateLayoutConfig($c4ShapeInRow=\"4\", $c4BoundaryInRow=\"1\")\r基础设施标准化 基础设施标准化是平台工程建设的第一步，通过对基础设施服务进行标准化，减少开发人员和运维团队之间的摩擦，减少运维难度，大大降低出错的概率。\n云平台已经成熟，在云平台的基础上，我们更近一步，提出 All In K8S 策略，这里的 All In 主要包括两点：\nRun On K8S：数据库、缓存、消息队列、业务应用，一切都运行在 K8S 上，K8S 是事实上的基础设施。 面向 K8S 设计：遵循 K8S 设计原则，拥抱 K8S 生态，充分利用开源功能特性。 作为一个由传统运维转变而来的团队，分享几个可能有用的关注点：\n操作习惯变化，由命令式到声明式：过去在虚拟机内直接修改文件、执行某个命令、安装某个插件，变为修改 yaml 配置、修改基础镜像、修改镜像 init 进程。 资源隔离：从集群、node、namespace 不同维度进行物理或逻辑隔离，对数据库、业务应用按 Label 、污点进行逻辑分区。 权限和策略管理：有容器云平台用平台即可。如果没有，K8S 4 种鉴权模式 Node、ABAC、RBAC 和 Webhook 能否满足要求，考虑是否引入 OPA Gatekeeper 或 Kyverno 完善和简化工作。 网络安全：截止目前 K8S 安装时仍然推荐禁用虚拟机内部防火墙，云平台提供虚拟机安全组一般用于防止外部网络入侵， K8S 内部考虑使用 NetworkPolicy 防止内部人员搞坏。比如不同组织的多个数据库，结合上面的资源隔离和权限控制，从一开始就要规划好是按集群还是 namespace 划分。 严格版本化管理：操作系统、容器镜像、Helm Charts、命令行工具，精确到小版本。 显式声明参数：以 Helm 应用为例，理解 values.yaml 文件中的每个变量并避免使用默认值，每次更新版本比对变量差异并再次执行显式声明参数原则。 应用交付自动化：自动化、规范化应用交付全流程，并严格践行面向失败设计的原则。减少人的动作流程才能减少生产事故。 日志采集：应用日志可能打印到文件或 Pod console，统一采集展示。 故障演练：对不同应用进行故障演练，尤其是模拟备份和恢复，传统的停进程、恢复文件、重启服务的操作模式在容器内可能行不通。 开发者体验 如何提升开发者体验。如何实现开发者自助。内部开发者平台解决不了的问题有哪些，如何解决的。\n可观测性 可观测助力提升开发者体验和质量提升。AIOps 是不是未来。\n组织架构变革 平台工程如何推动和影响组织架构变革。\n","date":"2023-09-07","id":4,"permalink":"/docs/platform/introduction/","summary":"我们的平台工程建设之路，介绍前期方案设计、中间踩坑历程。\n原则 分享我们平台工程建设的一些基本原则。\n以开发者为中心：赋能开发者，了解困难，解决问题，让开发者生活更轻松。 自动化：自动化手动和重复性任务，减少人为错误，提高效率。 标准化：标准化保持一致性，减少复杂性，减少团队认知负载，提供最佳实践和统一的编码结构。 模块化：松耦合且独立的模块，可独立开发、测试和部署。 弹性：可扩展水平扩缩容的能力，以及容错抗脆弱的能力。 安全：相比于微服务、云原生领域的安全，在平台工程里，更强调代码、基础设施、数据和其他资源的安全。 协作：平台工程师、开发人员、运维运营人员以及其他参与者之间的协作，提高生产力、促进创新并创造积极包容的工作环境。 持续改进：持续性反馈、评估、改进。 架构概述 为便于理解，我们仍然按照惯用架构模型，将架构分为 IaaS、CaaS、PaaS、Applications 这几个层级。\n专业的运维人员作为 platform engineer 着重于 IaaS、CaaS、PaaS 建设，开发人员作为 application engineer 更专注于 PaaS、Applications 建设，为开发和运维提供工具、协作平台、基础应用。\nC4Context title 平台工程总体架构 Boundary(users, \"Users\", \"用户接入\") { Person(superAdmin, \"超级管理员\") Person(admin, \"平台管理员\") Person(developer, \"开发人员\") Person(maintenance, \"运维人员\") } Boundary(console, \"Console\", \"开发者平台\") { Container(backstage, \"Backstage\",\"react\",\"开发者门户\") Container(apps, \"应用管理平台\",\"Application\",\"容器管理、应用管理、配置管理、自动化测试\") Container(ops, \"统一运维平台\",\"x-ops\",\"数据库、中间件、日志、监控告警平台\") Container(iam, \"IAM\", \"Keycloak\", \"统一用户、组织、角色权限管理\") } Boundary(paas, \"PaaS\", \"PaaS\") { ContainerDb(rds, \"RDS\", \"PostgreSQL/MySQL\", \"PostgreSQL、MySQL 等关系型数据库\") ContainerDb(clickhouse, \"ClickHouse\", \"ClickHouse\", \"BI、Logging、Metrics 等列式数据库\"","tags":[],"title":"总体架构"},{"content":"统一身份认证（Identity and Access Management，身份认证和访问控制，简称 IAM）的技术选型和实践。\n核心需求 集中管理：从一个地方管理账户和身份。 单点登录：允许用户使用一组凭据访问所有集成的系统和应用，避免记忆多个用户名和密码。 动态访问控制：基于角色和策略动态授予或撤销访问权限。 审计与合规：记录和监控访问活动，以支持合规性审计。 无缝快速集成：作为平台工程的一部分更强调“自助”，各个应用能够无缝快速接入，甚至有些应用只需要简单的权限能够不需要开发自动接入。 强化认证机制：采用多因素认证（MFA）等方法，确保只有授权用户才能访问系统和数据。 技术选项 为满足以上需求，在初期技术选项时主要关注以下几个开源组件。\nkeycloak: 全面的 IAM 解决方案 ，实现用户、权限管理，单点登录、MFA 等。 Dex: 身份代理，连接多个身份源，仅作为 OpenID Connect。 Ory: 包含多个独立的组件，组成一个全家桶的解决方案。 oauth2-proxy: 反向代理工具，专为提供 OAuth 2.0 身份验证和授权服务而设计，附带基于用户、分组、角色的权限管理。 Pomerium: Pomerium 不仅仅是一个 OAuth 2.0 代理，它还提供了细粒度的访问控制，能够根据用户、组、和其他上下文属性来决定访问权限。 以下为 keycloak 和 Dex 的简单对比。为什么不把 Ory 加进来，因为没有实际用过，不便于发表意见，如果你是一个 Ory 用户欢迎补充。\n特性/工具 Keycloak Dex 类型 全面的 IAM 解决方案 身份代理 用户管理 支持内置用户管理 不直接管理用户，依赖外部身份提供者 协议支持 OpenID Connect、OAuth 2.0、SAML 2.0 OpenID Connect SSO 支持 依赖外部身份提供者实现 社交登录 支持多种社交登录选项 不直接支持，可通过连接外部身份提供者实现 角色管理 支持复杂的角色和权限管理 不直接支持 扩展性 高，适合各种规模和复杂性的需求 适合将多个身份源统一到一个认证流程的环境 使用场景 需要全面、集中式身份管理的组织 需要统一多个身份源认证，如在云原生环境中 用户界面 提供丰富的用户和管理员界面 主要是 API，没有详细的用户界面 适用性 适用于需要完整 IAM 解决方案的组织 适用于作为多个身份源代理，尤其在 Kubernetes 环境中 以下为 OAuth2 Proxy 和 Pomerium 的简单对比。\n特性/工具 OAuth2 Proxy Pomerium 主要用途 身份验证代理 边缘身份验证和访问控制 身份认证方法 支持多种 OAuth 2.0 提供商 支持 OAuth 2.0、OpenID Connect 和 SAML 安全性 基本的身份验证和授权 高级访问控制，包括路由规则和策略 单点登录（SSO） 支持 支持 部署复杂性 相对简单，易于部署 较为复杂，但提供更多的配置选项和灵活性 用户界面 主要通过配置文件和命令行界面操作 提供 Web 界面用于配置和管理 适用场景 适合于小型应用和简单的身份验证需求 适合于大型企业和复杂的访问控制需求 集成能力 适合单一应用或小型系统 适合复杂的系统架构和多应用环境 扩展性和灵活性 基本扩展性，适用于标准用例 高度可扩展和灵活，适用于复杂多变的环境 社区和支持 拥有活跃的社区 社区支持强大，提供企业级支持和服务 方案落地 为满足核心诉求，根据不同场景我们做了以下选择。\n以 keycloak + ldap 管理用户、角色、权限，用户原始数据存于 ldap 中。 以 oatuh2-proxy + keycloak oidc 作为认证代理中间件，为那些不具备认证授权机制的应用提供统一的认证授权访问控制。 一个简单的应用统一身份认证总架构示例如下。\nC4Context Person(admin, \"超级管理员\") Person_Ext(extUser, \"自助注册\") Person_Ext(socialUser, \"社交登录\") Person(developer, \"内部人员\") Boundary(iam, \"IAM\", \"iam\") { Container(keycloak, \"keycloak\", \"Quarkus\", \"Admin 控制台\n用户、组织、角色、权限管理\") Container(ldap, \"LDAP/AD\",\"ldap\", \"LDAP 或 Windows AD 作为用户源\") ContainerDb(spiUser, \"自定义用户源\", \"Java SPI\", \"通过实现 SPI 自定义用户源\") System_Ext(providers, \"身份联合\", \"Github、Gitlab、Wechart、SAML v2.0、OIDC 等\n用户身份联合\") } Boundary(sso, \"Plugins Applications\", \"sso\") { Container(backstage, \"Backstage\",\"react\",\"开发者门户\") Container(grafana, \"Grafana\",\"grafana\",\"监控告警平台\") Container(k8s, \"Kubernetes\",\"kubernetes\",\"Kubernetes Service Account\") } Boundary(apps, \"OAuth Proxy Applications\", \"sso\") { Container(proxy, \"oauth2-proxy\",\"golang\",\"认证代理\n认证和 RBAC 权限管理\n通过后透传用户、角色等信息\") Container(login, \"自研应用\",\"any\", \"需要登录才可访\n自身无用户权限管理功能\") } Rel(admin, keycloak, \"用户管理\", \"http\") Rel(developer, keycloak, \"用户自助\", \"http\") Rel(extUser, keycloak, \"用户自助注册\", \"http\") Rel(socialUser, keycloak, \"社交账户登录\", \"http\") Rel(developer, backstage, \"开发者门户\", \"http\") Rel(developer, proxy, \"SSO\", \"http\") Rel(developer, grafana, \"SSO\", \"http\") Rel(developer, k8s, \"统一身份\", \"http\") Rel(proxy, login, \"RBAC\",\"x-user-id/group/role\") Rel(keycloak, ldap, \"user storage\") Rel(keycloak, spiUser, \"user storage\") Rel(keycloak, providers, \"identity provider\") Rel(backstage, keycloak, \"plugin auth backend\") Rel(proxy, keycloak, \"oidc provider\") Rel(grafana, keycloak, \"oidc provider\") Rel(k8s, keycloak, \"oidc provider\") UpdateRelStyle(proxy, keycloak,$lineColor=\"blue\" $offsetX=\"260\") UpdateLayoutConfig($c4ShapeInRow=\"4\", $c4BoundaryInRow=\"4\")\r","date":"2023-12-19","id":5,"permalink":"/docs/platform/iam/","summary":"统一身份认证（Identity and Access Management，身份认证和访问控制，简称 IAM）的技术选型和实践。\n核心需求 集中管理：从一个地方管理账户和身份。 单点登录：允许用户使用一组凭据访问所有集成的系统和应用，避免记忆多个用户名和密码。 动态访问控制：基于角色和策略动态授予或撤销访问权限。 审计与合规：记录和监控访问活动，以支持合规性审计。 无缝快速集成：作为平台工程的一部分更强调“自助”，各个应用能够无缝快速接入，甚至有些应用只需要简单的权限能够不需要开发自动接入。 强化认证机制：采用多因素认证（MFA）等方法，确保只有授权用户才能访问系统和数据。 技术选项 为满足以上需求，在初期技术选项时主要关注以下几个开源组件。\nkeycloak: 全面的 IAM 解决方案 ，实现用户、权限管理，单点登录、MFA 等。 Dex: 身份代理，连接多个身份源，仅作为 OpenID Connect。 Ory: 包含多个独立的组件，组成一个全家桶的解决方案。 oauth2-proxy: 反向代理工具，专为提供 OAuth 2.0 身份验证和授权服务而设计，附带基于用户、分组、角色的权限管理。 Pomerium: Pomerium 不仅仅是一个 OAuth 2.0 代理，它还提供了细粒度的访问控制，能够根据用户、组、和其他上下文属性来决定访问权限。 以下为 keycloak 和 Dex 的简单对比。为什么不把 Ory 加进来，因为没有实际用过，不便于发表意见，如果你是一个 Ory 用户欢迎补充。\n特性/工具 Keycloak Dex 类型 全面的 IAM 解决方案 身份代理 用户管理 支持内置用户管理 不直接管理用户，依赖外部身份提供者 协议支持 OpenID Connect、OAuth 2.0、SAML 2.0 OpenID Connect SSO 支持 依赖外部身份提供者实现 社交登录 支持多种社交登录选项 不直接支持，可通过连接外部身份提供者实现 角色管理 支持复杂的角色和权限管理 不直接支持 扩展性 高，适合各种规模和复杂性的需求 适合将多个身份源统一到一个认证流程的环境 使用场景 需要全面、集中式身份管理的组织 需要统一多个身份源认证，如在云原生环境中 用户界面 提供丰富的用户和管理员界面 主要是 API，没有详细的用户界面 适用性 适用于需要完整 IAM 解决方案的组织 适用于作为多个身份源代理，尤其在 Kubernetes 环境中 以下为 OAuth2 Proxy 和 Pomerium 的简单对比。","tags":[],"title":"统一身份认证"},{"content":"我们的平台工程建设之路。\n","date":"2023-09-07","id":6,"permalink":"/docs/platform/","summary":"我们的平台工程建设之路。","tags":[],"title":"平台工程"},{"content":"云原生技术探索。\n","date":"2023-09-07","id":7,"permalink":"/docs/cloud/","summary":"云原生技术探索。","tags":[],"title":"云原生"},{"content":"在 K8S 中使用 Helm 部署了一些有状态应用，并通过 Helm 自动生成了 PV 和 PVC，某天想扩容，竟然报错了。\n以下以 bitnami zookeeper 为例，其他 StatefulSet 同理。\n为了实现磁盘扩容，改大 persistence.size，比如由 8Gi 改为 10Gi，然后执行 helm upgrade，出现错误。\nError: UPGRADE FAILED: cannot patch \u0026#34;zookeeper\u0026#34; with kind StatefulSet: StatefulSet.apps \u0026#34;zookeeper\u0026#34; is invalid: spec: Forbidden: updates to statefulset spec for fields other than \u0026#39;replicas\u0026#39;, \u0026#39;template\u0026#39;, \u0026#39;updateStrategy\u0026#39;, \u0026#39;persistentVolumeClaimRetentionPolicy\u0026#39; and \u0026#39;minReadySeconds\u0026#39; are forbidden 实际上我们想更新的是 StatefulSet 的 spec.volumeClaimTemplates 中的 storage 大小，根据提示信息，StatefulSet 竟然不允许。\nspec: volumeClaimTemplates: - apiVersion: v1 spec: resources: requests: storage: 8Gi\r查看 K8S 官方说明，果然当前版本 (1.29) 还不支持，参考链接如下。\nKEP-0661: StatefulSet volume resize kubernetes/enhancements#3412 https://github.com/kubernetes/enhancements/pull/3412 Support Volume Expansion Through StatefulSets kubernetes/enhancements#661 https://github.com/kubernetes/enhancements/issues/661 注意此时 helm release 的状态是 failed，先把 persistence.size 改为原大小，然后再执行一遍 helm upgrade，先把 release 恢复正常。\n那么如何实现 StatefulSet 磁盘平衡扩容，issues/661 提供了解决方案。\n确保你用的 StorageClass 支持扩容，比如 NFS、rancher/local-path-provisioner 都是不支持扩容的。 删除 StatefulSet，但是保留 Pod，让服务继续运行。删除时增加 --cascade=orphan 参数，执行 kubectl delete statefulset --cascade=orphan zookeeper。 修改 Helm 中声明的 size，继续执行 helm upgrade 更新应用，此时通过 kubectl 可以看到 PV/PVC 大小已经变更，StatefulSet 已经重建，但是 Pod 无任何变化。 重建 Pod，使更新生效：kubectl rollout restart statefulset zookeeper 。 rancher/local-path-provisioner rancher/local-path-provisioner 是 Rancher 提供的一个本地存储卷插件，它主要用于在 Kubernetes 集群中动态创建和管理本地存储卷，让本地存储使用起来更简单。\n有些应用使用了此 local-path 作为存储，但是在更新完 size 后，发现 PV 大小已经变更，PVC 大小仍然是旧值，比如以下 PVC 声明的是 10Gi，但是 status 仍然是 8Gi。\napiVersion: v1 kind: List items: - apiVersion: v1 kind: PersistentVolumeClaim spec: resources: requests: storage: 10Gi storageClassName: local-path status: capacity: storage: 8Gi phase: Bound\r通过 kubectl describe pvc 发现 Events 中有以下 warning。\n# Warning ExternalExpanding 8h volume_expand Ignoring the PVC: didn\u0026#39;t find a plugin capable of expanding the volume; waiting for an external controller to process this PVC. 然而我们的 local-path-provisioner 已经声明了 allowVolumeExpansion: true，通过以下命令查看值也是对的，为啥不行呢，因为当前版本（我使用的是 v0.0.26） 就是不支持。\n$ kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE local-path cluster.local/local-path-provisioner Retain WaitForFirstConsumer true 2d 使用 local-path-provisioner 注意事项：\nlocal-path-provisioner 使用 hostPath 映射，不支持磁盘扩容，扩容后会出现以上 Warning。 不管是 K8S hostPath、Local volumes 还是 local-path-provisioner，都不支持限制磁盘大小，映射到主机的 hostPath 磁盘有多大，Pod 就能用多大。PV/PVC 声明的大小都只是声明并不起作用，所以也不用给 PV/PVC 尝试做扩容。 目前常用的 NFS 存储和 local-path-provisioner 一样，也不支持扩容和限制磁盘大小。 ","date":"2024-03-31","id":8,"permalink":"/blog/statefulset-resize-pvc/","summary":"在 K8S 中使用 Helm 部署了一些有状态应用，并通过 Helm 自动生成了 PV 和 PVC，某天想扩容，竟然报错了。\n以下以 bitnami zookeeper 为例，其他 StatefulSet 同理。\n为了实现磁盘扩容，改大 persistence.size，比如由 8Gi 改为 10Gi，然后执行 helm upgrade，出现错误。\nError: UPGRADE FAILED: cannot patch \u0026#34;zookeeper\u0026#34; with kind StatefulSet: StatefulSet.apps \u0026#34;zookeeper\u0026#34; is invalid: spec: Forbidden: updates to statefulset spec for fields other than \u0026#39;replicas\u0026#39;, \u0026#39;template\u0026#39;, \u0026#39;updateStrategy\u0026#39;, \u0026#39;persistentVolumeClaimRetentionPolicy\u0026#39; and \u0026#39;minReadySeconds\u0026#39; are forbidden 实际上我们想更新的是 StatefulSet 的 spec.volumeClaimTemplates 中的 storage 大小，根据提示信息，StatefulSet 竟然不允许。\nspec: volumeClaimTemplates: - apiVersion: v1 spec: resources: requests: storage: 8Gi\r查看 K8S 官方说明，果然当前版本 (1.","tags":[],"title":"K8S StatefulSet 应用 PV/PVC 平滑扩容"},{"content":"作为一个开发， 经常在 MacOS 遇到 Too many open files in system 的报错，尤其是碰到万恶的 node_modules 时，如何固化配置彻底解决，直接上代码。\n输入 launchctl limit 即可看到当前的限制，我这里 maxfiles 是改过以后的。\n$ launchctl limit cpu unlimited unlimited filesize unlimited unlimited data unlimited unlimited stack 8388608 67104768 core 0 unlimited rss unlimited unlimited memlock unlimited unlimited maxproc 1392 2088 maxfiles 10240 102400 开始创建文件 sudo vi /Library/LaunchDaemons/limit.maxfiles.plist ，内容如下，可根据自己爱好改后面的两个数字值。\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;!DOCTYPE plist PUBLIC \u0026#34;-//Apple//DTD PLIST 1.0//EN\u0026#34; \u0026#34;http://www.apple.com/DTDs/PropertyList-1.0.dtd\u0026#34;\u0026gt; \u0026lt;plist version=\u0026#34;1.0\u0026#34;\u0026gt; \u0026lt;dict\u0026gt; \u0026lt;key\u0026gt;Label\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;limit.maxfiles\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;ProgramArguments\u0026lt;/key\u0026gt; \u0026lt;array\u0026gt; \u0026lt;string\u0026gt;launchctl\u0026lt;/string\u0026gt; \u0026lt;string\u0026gt;limit\u0026lt;/string\u0026gt; \u0026lt;string\u0026gt;maxfiles\u0026lt;/string\u0026gt; \u0026lt;string\u0026gt;10240\u0026lt;/string\u0026gt; \u0026lt;string\u0026gt;102400\u0026lt;/string\u0026gt; \u0026lt;/array\u0026gt; \u0026lt;key\u0026gt;RunAtLoad\u0026lt;/key\u0026gt; \u0026lt;true/\u0026gt; \u0026lt;key\u0026gt;ServiceIPC\u0026lt;/key\u0026gt; \u0026lt;false/\u0026gt; \u0026lt;/dict\u0026gt; \u0026lt;/plist\u0026gt;\r验证文件格式和内容，并应用生效。\nplutil /Library/LaunchDaemons/limit.maxfiles.plist sudo launchctl load -w /Library/LaunchDaemons/limit.maxfiles.plist 再次输入 launchctl limit 查看是否生效。\n","date":"2024-03-19","id":9,"permalink":"/blog/macos-too-many-open-files/","summary":"作为一个开发， 经常在 MacOS 遇到 Too many open files in system 的报错，尤其是碰到万恶的 node_modules 时，如何固化配置彻底解决，直接上代码。\n输入 launchctl limit 即可看到当前的限制，我这里 maxfiles 是改过以后的。\n$ launchctl limit cpu unlimited unlimited filesize unlimited unlimited data unlimited unlimited stack 8388608 67104768 core 0 unlimited rss unlimited unlimited memlock unlimited unlimited maxproc 1392 2088 maxfiles 10240 102400 开始创建文件 sudo vi /Library/LaunchDaemons/limit.maxfiles.plist ，内容如下，可根据自己爱好改后面的两个数字值。\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;!DOCTYPE plist PUBLIC \u0026#34;-//Apple//DTD PLIST 1.0//EN\u0026#34; \u0026#34;http://www.apple.com/DTDs/PropertyList-1.0.dtd\u0026#34;\u0026gt; \u0026lt;plist version=\u0026#34;1.0\u0026#34;\u0026gt; \u0026lt;dict\u0026gt; \u0026lt;key\u0026gt;Label\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;limit.maxfiles\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;ProgramArguments\u0026lt;/key\u0026gt; \u0026lt;array\u0026gt; \u0026lt;string\u0026gt;launchctl\u0026lt;/string\u0026gt; \u0026lt;string\u0026gt;limit\u0026lt;/string\u0026gt; \u0026lt;string\u0026gt;maxfiles\u0026lt;/string\u0026gt; \u0026lt;string\u0026gt;10240\u0026lt;/string\u0026gt; \u0026lt;string\u0026gt;102400\u0026lt;/string\u0026gt; \u0026lt;/array\u0026gt; \u0026lt;key\u0026gt;RunAtLoad\u0026lt;/key\u0026gt; \u0026lt;true/\u0026gt; \u0026lt;key\u0026gt;ServiceIPC\u0026lt;/key\u0026gt; \u0026lt;false/\u0026gt; \u0026lt;/dict\u0026gt; \u0026lt;/plist\u0026gt;\r验证文件格式和内容，并应用生效。","tags":[],"title":"MacOS too many open files in system 问题"},{"content":"文件压缩解压缩快速参考。\n常用文件格式 .tar：tar 其实打包（或翻译为归档）文件，本身并没有压缩。在 Linux 里 man tar 可以看到它的描述也是“manipulate tape archives”（tar 最初被用来在磁带上创建档案，现在，用户可以在任何设备上创建档案，只是它的描述还没有改）。\n.gz：gzip 是 GNU 组织开发的一个压缩程序，.gz 结尾的文件就是 gzip 压缩的结果。\n.bz2：bzip2 是一个压缩能力更强的压缩程序，.bz2 结尾的文件就是 bzip2 压缩的结果。\n.Z：compress 也是一个压缩程序。.Z 结尾的文件就是 compress 压缩的结果。\n.zip：使用 zip 软件压缩的文件。\n.tar.gz、.tar.bz2、.tar.xz 等可以理解为打包+压缩的效果，用软件解压可以发现比。gz 多了一层包。gzip 和 bzip2，不能同时压缩多个文件，tar 相当于开个挂加上同时压缩的特效，tar 先归档为一个大文件，而归档为大文件的速度是很快的，测试了一下几乎可以忽略不计。\n除了这些格式外，常见的 deb、exe、msi、rpm、dmg、iso 等安装软件，其实都是经过压缩的，一般情况下没有必要再压缩。而 rar 基本认为是 Windows 平台专属的压缩算法了，各个 Linux 发行版都不自带 rar 压缩解压缩软件，所以可以看到很多软件发行的格式都是 .tar.gz 或 .zip。\n解压缩 根据文件名后缀自行选择解压缩命令。\ntar -xf test.tar gzip -d test.gz gunzip test.gz # -C 直接解压到指定目录 tar -xzf test.tar.gz -C /home bzip2 -d test.bz2 bunzip2 test.bz2 tar -xjf test.tar.bz2 tar -xvJf test.tar.xz\r压缩 请根据需要选择压缩算法。\n# 将当前目录下所有 jpg 格式的文件打包为 pictures.tar tar -cf pictures.tar *.jpg # 将 Picture 目录下所有文件打包并用 gzip 压缩为 pictures.tar.gz tar -czf pictures.tar.gz Picture/ # 将 Picture 目录下所有文件打包并用 bzip2 压缩为 pictures.tar.bz2 tar -cjf pictures.tar.bz2 Picture/\r","date":"2024-03-10","id":10,"permalink":"/blog/tar-gz/","summary":"文件压缩解压缩快速参考。\n常用文件格式 .tar：tar 其实打包（或翻译为归档）文件，本身并没有压缩。在 Linux 里 man tar 可以看到它的描述也是“manipulate tape archives”（tar 最初被用来在磁带上创建档案，现在，用户可以在任何设备上创建档案，只是它的描述还没有改）。\n.gz：gzip 是 GNU 组织开发的一个压缩程序，.gz 结尾的文件就是 gzip 压缩的结果。\n.bz2：bzip2 是一个压缩能力更强的压缩程序，.bz2 结尾的文件就是 bzip2 压缩的结果。\n.Z：compress 也是一个压缩程序。.Z 结尾的文件就是 compress 压缩的结果。\n.zip：使用 zip 软件压缩的文件。\n.tar.gz、.tar.bz2、.tar.xz 等可以理解为打包+压缩的效果，用软件解压可以发现比。gz 多了一层包。gzip 和 bzip2，不能同时压缩多个文件，tar 相当于开个挂加上同时压缩的特效，tar 先归档为一个大文件，而归档为大文件的速度是很快的，测试了一下几乎可以忽略不计。\n除了这些格式外，常见的 deb、exe、msi、rpm、dmg、iso 等安装软件，其实都是经过压缩的，一般情况下没有必要再压缩。而 rar 基本认为是 Windows 平台专属的压缩算法了，各个 Linux 发行版都不自带 rar 压缩解压缩软件，所以可以看到很多软件发行的格式都是 .tar.gz 或 .zip。\n解压缩 根据文件名后缀自行选择解压缩命令。\ntar -xf test.tar gzip -d test.gz gunzip test.gz # -C 直接解压到指定目录 tar -xzf test.tar.gz -C /home bzip2 -d test.","tags":[],"title":"文件压缩解压缩快速参考"},{"content":"项目中需要根据 SQL 文件导入数据，文件大约 20G，正常导入约需要 2 小时，如何加快导入速度。\n如果一个 SQL 文件只有一个表的数据，可以直接使用 mysql load data infile 语法，速度比较快。\n我们是一个 SQL 文件包含了很多表，导入过程经过如下设置，20G 大约需要 40 分钟，比之前快了很多。\n# 进入 mysql mysql -u root -p # 创建数据库（如果已经有数据库忽略此步骤） CREATE DATABASE 数据库名； # 设置参数 set sql_log_bin=OFF;//关闭日志 set autocommit=0;//关闭 autocommit 自动提交模式 0 是关闭 1 是开启（默认） set global max_allowed_packet = 20 *1024* 1024 * 1024; # 使用数据库 use 数据库名； # 开启事务 START TRANSACTION; # 导入 SQL 文件并 COMMIT（因为导入比较耗时，导入和 COMMIT 一行命令） source 文件的路径；COMMIT;\r","date":"2024-03-10","id":11,"permalink":"/blog/mysql-large-import/","summary":"项目中需要根据 SQL 文件导入数据，文件大约 20G，正常导入约需要 2 小时，如何加快导入速度。\n如果一个 SQL 文件只有一个表的数据，可以直接使用 mysql load data infile 语法，速度比较快。\n我们是一个 SQL 文件包含了很多表，导入过程经过如下设置，20G 大约需要 40 分钟，比之前快了很多。\n# 进入 mysql mysql -u root -p # 创建数据库（如果已经有数据库忽略此步骤） CREATE DATABASE 数据库名； # 设置参数 set sql_log_bin=OFF;//关闭日志 set autocommit=0;//关闭 autocommit 自动提交模式 0 是关闭 1 是开启（默认） set global max_allowed_packet = 20 *1024* 1024 * 1024; # 使用数据库 use 数据库名； # 开启事务 START TRANSACTION; # 导入 SQL 文件并 COMMIT（因为导入比较耗时，导入和 COMMIT 一行命令） source 文件的路径；COMMIT;\r","tags":[],"title":"MySQL 大文件导入优化"},{"content":"介绍基于 start.spring.io 快速定制自己的 Spring Boot 脚手架，主要应用场景：\n规范公司自己的 parent pom，增加特定的依赖项。 根据公司规范生成统一的包结构，统一命名。 根据需要增加特定代码或文件，比如根据公司要求统一 logback.xml、 application.properties 文件。 提供公司自研的二方 jar 包。 快速开始 基本步骤：\n对于 spring.initializr 我们没有定制的需求，直接引用官方的。 拷贝一份 start.spring.io，直接基于这个项目开发、部署、运行。以下都是关于如何修改 start.spring.io。 start.spring.io 主要关注两个模块：\nstart-client：前端页面，可以定制些自己的 logo、title 等。 start-site：是一个标准的 spring boot 项目，实际 run 起来的服务，引用了 start-client，直接 run 这个项目的 main 方法就能看到效果。 主要配置文件：start-site/src/main/resources/application.yml，通过修改这个配置文件可以达到的效果如下。\n修改 start 启动时默认 group，把 com.example 改为公司自己的 group。\ninitializr: group-id: value: com.yourgroup\r修改父 pom，使用公司自己的 pom。\ninitializr: env: maven: # use your parent pom parent: groupId: com.yourself artifactId: your-parent version: 1.0.0 # relativePath: ../pom.xml includeSpringBootBom: false\r限定 Java 和 Spring Boot 版：修改 languages 和 bootVersions 即可。\n增加公司自己的 starter，参考文件中例子增加即可。\n核心扩展接口 ProjectContributor: 用于实现文件结构变化，比如加个文件夹，增加配置文件、代码片段等。 BuildCustomizer：动态修改 pom.xml，用于修改 maven/gradle 的 dependencies/repository/plugins 等。 更多自带定制接口参考：MainSourceCodeCustomizer, MainCompilationUnitCustomizer, MainApplicationTypeCustomizer, TestSourceCodeCustomizer, TestApplicationTypeCustomizer. 场景：生成默认的 logback-spring.xml。\nimport io.spring.initializr.generator.project.contributor.SingleResourceProjectContributor; /** * 定制 logback xml 文件，从当前项目的`classpath:configuration`拷贝到指定的 src/main/resources 目录下 */ public class LogbackContributor extends SingleResourceProjectContributor { public LogbackContributor() { this(\u0026#34;classpath:configuration/logback-spring.xml\u0026#34;); } public LogbackContributor(String resourcePattern) { super(\u0026#34;src/main/resources/logback-spring.xml\u0026#34;, resourcePattern); } }\r场景：按照统一规范生成目录结构。\n/** * 按照规范生成默认的 java 目录结构 */ public class DefaultPackageContributor implements ProjectContributor { private final ProjectDescription description; public DefaultPackageContributor(ProjectDescription description) { this.description = description; } @Override public void contribute(Path projectRoot) throws IOException { Language language = description.getLanguage(); // \u0026#34;src/main/java/com.test.demo\u0026#34; Path packageRoot = projectRoot.resolve(\u0026#34;src/main/\u0026#34;) .resolve(language.id()) .resolve(description.getPackageName().replaceAll(\u0026#34;\\\\.\u0026#34;,\u0026#34;/\u0026#34;)); Files.createDirectories(packageRoot.resolve(\u0026#34;config\u0026#34;)); Files.createDirectories(packageRoot.resolve(\u0026#34;dao\u0026#34;)); Files.createDirectories(packageRoot.resolve(\u0026#34;service\u0026#34;)); Files.createDirectories(packageRoot.resolve(\u0026#34;web\u0026#34;)); } }\r场景：在 xxx 条件下做 xxx。比如我们使用 spock 作为测试框架，spock 使用 groovy 语言。如果引入 spock 的时候，默认在 maven plugin 里增加 groovy 插件 gmavenplus-plugin。\nclass SpockMavenBuildCustomizer implements BuildCustomizer\u0026lt;MavenBuild\u0026gt; { @Override public void customize(MavenBuild build) { // add groovy plugin build.plugins() .add(\u0026#34;org.codehaus.gmavenplus\u0026#34;, \u0026#34;gmavenplus-plugin\u0026#34;); } }\r@ProjectGenerationConfiguration public class MyProjectGenerationConfiguration { @Bean @ConditionalOnRequestedDependency(\u0026#34;spock\u0026#34;) public SpockMavenBuildCustomizer spockMavenBuildCustomizer() { return new SpockMavenBuildCustomizer(); } }\r","date":"2024-03-09","id":12,"permalink":"/blog/spring-boot-start-site/","summary":"介绍基于 start.spring.io 快速定制自己的 Spring Boot 脚手架，主要应用场景：\n规范公司自己的 parent pom，增加特定的依赖项。 根据公司规范生成统一的包结构，统一命名。 根据需要增加特定代码或文件，比如根据公司要求统一 logback.xml、 application.properties 文件。 提供公司自研的二方 jar 包。 快速开始 基本步骤：\n对于 spring.initializr 我们没有定制的需求，直接引用官方的。 拷贝一份 start.spring.io，直接基于这个项目开发、部署、运行。以下都是关于如何修改 start.spring.io。 start.spring.io 主要关注两个模块：\nstart-client：前端页面，可以定制些自己的 logo、title 等。 start-site：是一个标准的 spring boot 项目，实际 run 起来的服务，引用了 start-client，直接 run 这个项目的 main 方法就能看到效果。 主要配置文件：start-site/src/main/resources/application.yml，通过修改这个配置文件可以达到的效果如下。\n修改 start 启动时默认 group，把 com.example 改为公司自己的 group。\ninitializr: group-id: value: com.yourgroup\r修改父 pom，使用公司自己的 pom。\ninitializr: env: maven: # use your parent pom parent: groupId: com.yourself artifactId: your-parent version: 1.","tags":["Spring Boot","Java"],"title":"Spring Boot Start 脚手架定制开发和快速入门"},{"content":"基于 Alibaba Sentinel 实现的分布式限流中间件服务。主要对服务提供者提供限流、系统保护，对服务调用者提供熔断降级、限流排队等待效果。\n实现目标：\n作为服务提供者，保护自己不被打死，服务可以慢不可以挂。 作为客户端及时限速和熔断，防止对服务提供方包含 Http、数据库、MQ 等造成太大压力，防止把糟糕的情况变得更糟。 以用户、租户、对象等更细粒度进行流量精细控制。 服务预热，应用新发布上线，缓存尚未完全建立，防止流量一下子把服务打死。 能够根据 Prometheus、ClickHouse、Elasticsearch 提供的监控指标，动态生成规则，自适应调整规则。 概述 Sentinel 的基础知识请参考官方文档描述，这里单独介绍一些与我们定制相关的内容。\n限流简单来说就三个点：资源、规则、效果。\n资源：就是一个字符串，这个字符串可以自己定义、可以用注解自动生成、可以通过拦截器按规则生成。 规则：Sentinel 定义的一系列限流保护规则，比如流量控制规则、自适应保护规则。 效果：实际上“效果”也是“规则”定义的一部分。任何一条请求，命中某些资源规则后产生的效果，比如直接抛出异常、匀速等待。\nSentinel 全局注意事项和使用限制 使用开源默认 Sentinel 组件，有一些坑，或者说需要关注的注意事项：\n单个进程内资源数量阈值是 6000，多出的资源规则将不会生效（因为是懒加载，资源先到先得），也不提示错误而是直接忽略，资源数量太多建议使用热点参数控制。 对于限流的链路模式，context 阈值是 2000，所以默认的 WEB_CONTEXT_UNIFY 为 true，如果需要链路限流需要把这个改为 false。 自定义时，资源名中不要带 | 线， 这个日志中要用，日志以此作为分割符。 Sentinel 支持按来源限流，注意 origin 数量不能太多，否则会导致内存暴涨，并且目前不支持模式匹配。 一个资源可以有多个规则，一条请求能否通过，取决于规则里阈值最小的限制条件。 限流的目的是保护系统，计数计量并不准确，所以不要拿限流做计量或配额控制。 增加限流一定程度上通过时间换空间，降低了 CPU、内存负载，对 K8S HPA 策略会有一定影响。后续我们也会考虑根据 Sentinel 限流指标进行扩缩容。 限流中如果有增加等待效果会使接口变慢，各调用链需要关注调用超时和事务配置。 目前 sentinel-web-servlet 和 sentinel-spring-webmvc-adapter 均不支持热点参数限流。为了支持热点参数需要自行扩展。 sentinel-web-servlet 和 sentinel-spring-webmvc-adapter 会将每个到来的不同的 URL 都作为不同的资源处理，因此对于 REST 风格的 API，需要自行实现 UrlCleaner 接口清洗一下资源（比如将满足 /foo/:id 的 URL 都归到 /foo/* 资源下）。否则会导致资源数量过多，超出资源数量阈值（目前是 6000）时多出的资源的规则将不会生效。 接入指导 总体架构图。\n我们所有组件，规则加载都是由 Datasource 组件统一加载，配置是懒加载的，在第一次访问的时候加载，如果需要定义规则请在配置中心定义，这是由 Sentinel 在第一次初始化的时候（参考源码：com.alibaba.csp.sentinel.Env.java）通过 SPI 加载的。\n注意：如果你有自编码使用 Sentinel SDK 自带的 XxxRuleManager.loadRules 加载规则，会被远端配置中心覆盖掉，远端配置变更自动刷新后会以远端配置为准，把 XxxRuleManager.loadRules 加载的规则覆盖掉。\n规则参数详解 Sentinel 规则的资源名字匹配支持正则表达式，但是不知道为什么文档里从未提及，可能是考虑到性能。如果要为某个规则启用正则，需主动设置 xxRule.setRegex(true)，另外注意用的是 Java 正则匹配，不要和 Spring Path 的正则匹配混了。比如 Java 里 .* 代表任意匹配，Spring * 表达任意匹配。\n系统自适应保护规则 参数示例：\n{ \u0026#34;avgRt\u0026#34;: 500, \u0026#34;highestSystemLoad\u0026#34;: 100, \u0026#34;highestCpuUsage\u0026#34;: 90.0, \u0026#34;maxThread\u0026#34;: 100, \u0026#34;qps\u0026#34;: 200.0 }\rhighestSystemLoad：当系统 load1 超过阈值，且系统当前的并发线程数超过系统容量时才会触发系统保护。系统容量由系统的 maxQps * minRt 计算得出。设定参考值一般是 CPU cores * 2.5。 highestCpuUsage：当系统 CPU 使用率超过阈值即触发系统保护（取值范围 0.0-1.0）。 avgRt ：当单台机器上所有入口流量的平均 RT 达到阈值即触发系统保护，单位是毫秒。 maxThread：当单台机器上所有入口流量的并发线程数达到阈值即触发系统保护。 qps：当单台机器上所有入口流量的 QPS 达到阈值即触发系统保护。 以上参数默认是 -1，代表无限制。\n注意：\n在 K8S 环境下，Sentinel 读取当前指标值时，highestSystemLoad 获取的是宿主机的 load1，不是 Pod 的。参考：https://github.com/alibaba/Sentinel/issues/2260 Sentinel 读取当前指标值时，获取的 CPU 指标取的是 Pod Cpu 和宿主机 CPU 的最大值，也就是说如果 宿主机 CPU 占用太高，Pod CPU 很低，会误伤，会触发限流。 highestSystemLoad 相当于要不要自适应的开关，达到条件后会计算下是否还能承受流量，不行才拒绝。这就是所谓的“自适应”。除 highestSystemLoad 外，其他几个参数是达到阈值就拒绝。 规则中的几个参数，可以在一条规则里全部设置，也可以分多个规则配置不同参数，也可以只设置某个，Sentinel 会自行合并参数计算。 流量控制 参数示例：\n{ \u0026#34;resource\u0026#34;: \u0026#34;spring-cloud-samples:GET:/api-provider/pets/{id}\u0026#34;, \u0026#34;count\u0026#34;: 100.0, \u0026#34;grade\u0026#34;: 1, \u0026#34;controlBehavior\u0026#34;: 0, \u0026#34;warmUpPeriodSec\u0026#34;: 10, \u0026#34;maxQueueingTimeMs\u0026#34;: 5000, \u0026#34;limitApp\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;strategy\u0026#34;: 0 }\rresource：资源名，即限流规则的作用对象 count: 限流阈值 grade: 限流阈值类型，QPS（RuleConstant.FLOW_GRADE_QPS = 1） 或线程数（RuleConstant.FLOW_GRADE_THREAD = 0）。 controlBehavior：限流效果，有直接拒绝（RuleConstant.CONTROL_BEHAVIOR_DEFAULT = 0）、冷启动（RuleConstant.CONTROL_BEHAVIOR_WARM_UP=1）、匀速器（RuleConstant.CONTROL_BEHAVIOR_RATE_LIMITER=2）、冷启动-匀速器（RuleConstant.CONTROL_BEHAVIOR_WARM_UP_RATE_LIMITER=3）。 warmUpPeriodSec：冷启动时间，单位秒，默认 10s。 maxQueueingTimeMs：最大排队等待时长，默认 500ms。（仅在匀速排队模式 + QPS 下生效） limitApp: 按来源限流，默认 default 表示忽略来源。 strategy: 根据调用关系选择策略（默认用 RuleConstant.STRATEGY_DIRECT=0，直接来源） 注意：\n匀速器模式的时候一定要计算好 maxQueueingTimeMs，这个值默认比较小，避免排队超时（也就抛出异常）达不到匀速的效果。 匀速排队模式不支持 QPS \u0026gt; 1000 的场景，因为 Sentinel 内部通过 Thread::sleep 来实现虚拟等待队列，QPS 等于 1000 时，每个请求正好 sleep 1 ms，而当 QPS \u0026gt; 1000 时，没法精准的控制 sleep 小于 1 ms 的时长。 热点参数限流 热点参数限流会统计传入参数中的热点参数，并根据配置的限流阈值与模式，对包含热点参数的资源调用进行限流。热点参数限流可以看做是一种特殊的流量控制，仅对包含热点参数的资源调用生效。 Sentinel 利用 LRU 策略统计最近最常访问的热点参数，结合令牌桶算法来进行参数级别的流控。\n参数示例：\n{ \u0026#34;resource\u0026#34;: \u0026#34;spring-cloud-samples:GET:/api-provider/pets/{id}\u0026#34;, \u0026#34;count\u0026#34;: 100.0, \u0026#34;grade\u0026#34;: 1, \u0026#34;paramIdx\u0026#34;: 0, \u0026#34;controlBehavior\u0026#34;: 0, \u0026#34;warmUpPeriodSec\u0026#34;: 1, \u0026#34;maxQueueingTimeMs\u0026#34;: 5000, \u0026#34;durationInSec\u0026#34;: 1, \u0026#34;burstCount\u0026#34;: 0, \u0026#34;limitApp\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;paramFlowItemList\u0026#34;: [{ \u0026#34;object\u0026#34;: \u0026#34;ea-vip\u0026#34;, \u0026#34;classType\u0026#34;: \u0026#34;String\u0026#34;, \u0026#34;count\u0026#34;: 1000 }], \u0026#34;strategy\u0026#34;: 0 }\r默认参数参考流量控制规则的解释。 paramIdx：热点参数的索引，必填，对应 SphU.entry(xxx, args) 中的 args 参数索引位置，从 0 开始。 durationInSec：统计窗口时间长度（单位为秒），默认 1s。 paramFlowItemList：参数例外项，可以针对指定的参数值单独设置限流阈值，不受前面 count 阈值的限制。仅支持基本类型和字符串类型。 burstCount: 为应对突发流量\u0026quot;额外允许\u0026quot;的流量，在原 count 的基础上再额外加上这个值，相当于保底。默认为 0，仅在 快速失败|Warm UP + QPS 下生效。（Java 文档中未提及，代码中支持） 注意：\n可以通过 paramFlowItemList 设置例外项，比如为 VIP 单独设置限流阈值。 每个参数索引 (paramIdx) 对应的不同值最多统计 4000（ParameterMetric.BASE_PARAM_MAX_CAPACITY）个。 在统计窗口时间长度（durationInSec）内最多允许统计 20 万个。可以理解为 LRU 的 Top N。 来源访问控制 参数示例：\n{ \u0026#34;resource\u0026#34;: \u0026#34;spring-cloud-samples:GET:/api-provider/pets/{id}\u0026#34;, \u0026#34;limitApp\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;strategy\u0026#34;: 0 }\rresource：资源名，即限流规则的作用对象 limitApp：对应的黑名单/白名单，不同 origin 用 , 分隔，如 appA,appB strategy：限制模式，AUTHORITY_WHITE=0 为白名单模式，AUTHORITY_BLACK=1 为黑名单模式，默认为白名单模式。 熔断降级 现代微服务架构都是分布式的，由非常多的服务组成。不同服务之间相互调用，组成复杂的调用链路。以上的问题在链路调用中会产生放大的效果。复杂链路上的某一环不稳定，就可能会层层级联，最终导致整个链路都不可用。因此我们需要对不稳定的弱依赖服务调用进行熔断降级，暂时切断不稳定调用，避免局部不稳定因素导致整体的雪崩。熔断降级作为保护自身的手段，通常在客户端（调用端）进行配置。\nSentinel 提供以下几种熔断策略：\n慢调用比例 (SLOW_REQUEST_RATIO)：选择以慢调用比例作为阈值，需要设置允许的慢调用 RT（即最大的响应时间），请求的响应时间大于该值则统计为慢调用。当单位统计时长（statIntervalMs）内请求数目大于设置的最小请求数目，并且慢调用的比例大于阈值，则接下来的熔断时长内请求会自动被熔断。经过熔断时长后熔断器会进入探测恢复状态（HALF-OPEN 状态），若接下来的一个请求响应时间小于设置的慢调用 RT 则结束熔断，若大于设置的慢调用 RT 则会再次被熔断。 异常比例 (ERROR_RATIO)：当单位统计时长（statIntervalMs）内请求数目大于设置的最小请求数目，并且异常的比例大于阈值，则接下来的熔断时长内请求会自动被熔断。经过熔断时长后熔断器会进入探测恢复状态（HALF-OPEN 状态），若接下来的一个请求成功完成（没有错误）则结束熔断，否则会再次被熔断。异常比率的阈值范围是 [0.0, 1.0]，代表 0% - 100%。 异常数 (ERROR_COUNT)：当单位统计时长内的异常数目超过阈值之后会自动进行熔断。经过熔断时长后熔断器会进入探测恢复状态（HALF-OPEN 状态），若接下来的一个请求成功完成（没有错误）则结束熔断，否则会再次被熔断。 { \u0026#34;resource\u0026#34;: \u0026#34;spring-cloud-samples:GET:/api-provider/pets/{id}\u0026#34;, \u0026#34;grade\u0026#34;: 0, \u0026#34;count\u0026#34;: 100.0, \u0026#34;timeWindow\u0026#34;: 10, \u0026#34;minRequestAmount\u0026#34;: 5, \u0026#34;statIntervalMs\u0026#34;: 1000, \u0026#34;slowRatioThreshold\u0026#34;: 1 }\rgrade：熔断策略，支持慢调用比例/异常比例/异常数策略。默认慢调用比例。 count：慢调用比例模式下为慢调用临界 RT（超出该值计为慢调用）；异常比例/异常数模式下为对应的阈值。 timeWindow：熔断时长，单位为秒。 minRequestAmount：熔断触发的最小请求数，请求数小于该值时即使异常比率超出阈值也不会熔断。默认 5。 statIntervalMs：统计时长（单位为 ms），如 60*1000 代表分钟级。默认 1000ms。 slowRatioThreshold：慢调用比例阈值，仅慢调用比例模式有效。 注意事项：\n熔断降级规则在服务端时，Spring 的全局异常处理器一般会消化掉异常转换成一个合法的 Response，会导致熔断规则中的异常数规则失效，我们在 Server 端并不准备支持，参考 Issue。 日志和监控模块 我们自定义了日志模块，在 block 和 metrics 日志中增加 trace id、user id 等更多参数，通过 fluent-bit 收集到 ClickHouse 中。\n可在 Granfana 中以 ClickHouse 作为数据源配置自己需要的视图，并结合告警组件配置告警，比如应用 1 分钟 block 次数超过 10 次触发告警。\nFAQ Q：Sentinel 资源生成时如何忽略某些资源。\nA：自定义 UrlCleaner，对想忽略的资源返回空字符。\nQ：对于限流的冷启动效果，冷启动结束进入稳定状态后，还会不会重新回到冷启动阶段。\nA：会，一段时间流量较小或无流量后会回到冷启动阶段。Sentinel 固定速率产生令牌再消费，服务第一次启动时，或者接口很久没有被访问，都会导致当前时间与上次生产令牌的时间相差甚远，所以第一次生产令牌将会生产 maxPermits 个令牌，直接将令牌桶装满。由于令牌桶已满，接下来 N 秒就是冷启动阶段。具体查看参考资料里的冷启动算法详解。\n参考资料 令牌桶算法在 Sentinel 中的应用：https://blog.51cto.com/morris131/6506314 Sentinel 中的冷启动限流算法：https://cloud.tencent.com/developer/article/1674916 ","date":"2024-03-07","id":13,"permalink":"/blog/sentinel/","summary":"基于 Alibaba Sentinel 实现的分布式限流中间件服务。主要对服务提供者提供限流、系统保护，对服务调用者提供熔断降级、限流排队等待效果。\n实现目标：\n作为服务提供者，保护自己不被打死，服务可以慢不可以挂。 作为客户端及时限速和熔断，防止对服务提供方包含 Http、数据库、MQ 等造成太大压力，防止把糟糕的情况变得更糟。 以用户、租户、对象等更细粒度进行流量精细控制。 服务预热，应用新发布上线，缓存尚未完全建立，防止流量一下子把服务打死。 能够根据 Prometheus、ClickHouse、Elasticsearch 提供的监控指标，动态生成规则，自适应调整规则。 概述 Sentinel 的基础知识请参考官方文档描述，这里单独介绍一些与我们定制相关的内容。\n限流简单来说就三个点：资源、规则、效果。\n资源：就是一个字符串，这个字符串可以自己定义、可以用注解自动生成、可以通过拦截器按规则生成。 规则：Sentinel 定义的一系列限流保护规则，比如流量控制规则、自适应保护规则。 效果：实际上“效果”也是“规则”定义的一部分。任何一条请求，命中某些资源规则后产生的效果，比如直接抛出异常、匀速等待。\nSentinel 全局注意事项和使用限制 使用开源默认 Sentinel 组件，有一些坑，或者说需要关注的注意事项：\n单个进程内资源数量阈值是 6000，多出的资源规则将不会生效（因为是懒加载，资源先到先得），也不提示错误而是直接忽略，资源数量太多建议使用热点参数控制。 对于限流的链路模式，context 阈值是 2000，所以默认的 WEB_CONTEXT_UNIFY 为 true，如果需要链路限流需要把这个改为 false。 自定义时，资源名中不要带 | 线， 这个日志中要用，日志以此作为分割符。 Sentinel 支持按来源限流，注意 origin 数量不能太多，否则会导致内存暴涨，并且目前不支持模式匹配。 一个资源可以有多个规则，一条请求能否通过，取决于规则里阈值最小的限制条件。 限流的目的是保护系统，计数计量并不准确，所以不要拿限流做计量或配额控制。 增加限流一定程度上通过时间换空间，降低了 CPU、内存负载，对 K8S HPA 策略会有一定影响。后续我们也会考虑根据 Sentinel 限流指标进行扩缩容。 限流中如果有增加等待效果会使接口变慢，各调用链需要关注调用超时和事务配置。 目前 sentinel-web-servlet 和 sentinel-spring-webmvc-adapter 均不支持热点参数限流。为了支持热点参数需要自行扩展。 sentinel-web-servlet 和 sentinel-spring-webmvc-adapter 会将每个到来的不同的 URL 都作为不同的资源处理，因此对于 REST 风格的 API，需要自行实现 UrlCleaner 接口清洗一下资源（比如将满足 /foo/:id 的 URL 都归到 /foo/* 资源下）。否则会导致资源数量过多，超出资源数量阈值（目前是 6000）时多出的资源的规则将不会生效。 接入指导 总体架构图。","tags":["Java"],"title":"使用 Sentinel 实现分布式应用限流"},{"content":"背景：公司 Windows 办公机受域控安全策略限制，部分文件无权直接修改，另外开发常用的设置系统环境变量也变灰无法设置。此问题解决方式如下。\n提升文件权限 点击 Windows + X 快捷键 – 选择「命令提示符（管理员）。\n在 CDM 窗口中执行如下命令。\ntakeown /f C:\\要修复的文件路径\r在拿到文件所有权后，还需要使用如下命令获取文件的完全控制权限。\nicacls C:\\要修复的文件路径 /Grant Administrators:F\r命令行设置环境变量 Windows 下命令行设置环境变量，方式为 setx 变量名 变量值，变量值带空格等特殊符号的，用引号引起来。\n# 通过命令行设置 Java Home setx JAVA_HOME \u0026#34;C:\\Program Files\\Java\\jdk-11.0.2\u0026#34; # 设置 GO Path setx GOPATH \u0026#34;D:\\workspace\\go\u0026#34;\r","date":"2024-02-26","id":14,"permalink":"/blog/windows-takeown/","summary":"背景：公司 Windows 办公机受域控安全策略限制，部分文件无权直接修改，另外开发常用的设置系统环境变量也变灰无法设置。此问题解决方式如下。\n提升文件权限 点击 Windows + X 快捷键 – 选择「命令提示符（管理员）。\n在 CDM 窗口中执行如下命令。\ntakeown /f C:\\要修复的文件路径\r在拿到文件所有权后，还需要使用如下命令获取文件的完全控制权限。\nicacls C:\\要修复的文件路径 /Grant Administrators:F\r命令行设置环境变量 Windows 下命令行设置环境变量，方式为 setx 变量名 变量值，变量值带空格等特殊符号的，用引号引起来。\n# 通过命令行设置 Java Home setx JAVA_HOME \u0026#34;C:\\Program Files\\Java\\jdk-11.0.2\u0026#34; # 设置 GO Path setx GOPATH \u0026#34;D:\\workspace\\go\u0026#34;\r","tags":["Tools"],"title":"Windows 提权和设置环境变量"},{"content":"Git 为不同目录配置不同的 config，比如在同一个电脑上区分个人开发账号和公司开发账号，开源项目放一个文件夹，公司项目放一个文件夹，这样在提交代码的时候就不会混乱。\n为账户 B 准备一个单独的配置文件，比如： ~/.gitconfig-b，内容根据需要定义。\n[user] name = userb-name email = userb-email@test.com\r修改 ~/.gitconfig 文件，增加以下配置，引用上面创建的配置文件，注意其中的路径用绝对路径，并且路径以 / 结尾。\n[includeIf \u0026#34;gitdir:/project/path-b/\u0026#34;] path = /Users/xxxx/.gitconfig-b\r保存后，在 /project/path-b/ 下新的仓库都会以 .gitconfig-b 中的用户名和邮箱提交了。\n注意如果使用 ssh key 方式，在生成 key 的时候 ssh-keygen 名字指定文件名，多个 key 不要覆盖了。\n","date":"2024-02-26","id":15,"permalink":"/blog/git-multi-user/","summary":"Git 为不同目录配置不同的 config，比如在同一个电脑上区分个人开发账号和公司开发账号，开源项目放一个文件夹，公司项目放一个文件夹，这样在提交代码的时候就不会混乱。\n为账户 B 准备一个单独的配置文件，比如： ~/.gitconfig-b，内容根据需要定义。\n[user] name = userb-name email = userb-email@test.com\r修改 ~/.gitconfig 文件，增加以下配置，引用上面创建的配置文件，注意其中的路径用绝对路径，并且路径以 / 结尾。\n[includeIf \u0026#34;gitdir:/project/path-b/\u0026#34;] path = /Users/xxxx/.gitconfig-b\r保存后，在 /project/path-b/ 下新的仓库都会以 .gitconfig-b 中的用户名和邮箱提交了。\n注意如果使用 ssh key 方式，在生成 key 的时候 ssh-keygen 名字指定文件名，多个 key 不要覆盖了。","tags":["Tools"],"title":"Git 多用户配置"},{"content":"我有一个 Spring Boot 应用服务，提供了一些简单的查询接口，本身运行很正常，通过 curl 或其他 http 客户端 localhost 请求都没有问题。\n某天通过 Traefik 代理了此服务，经过代理后再访问，某个接口一直都是 500 internal server error，其他接口都没有问题。通过 tcpdump 抓包发现，应用服务并没有返回任何 500 错误，而且响应时间和 Body 体大小都很正常。\n根据网上经验排查了 Traefik SSL 证书问题、路径问题、消息体太大问题、请求 Header 不合规问题，都一一否定。最后无意间看了一眼 Response Header，发现 Spring Boot 应用返回了两个 Transfer-Encoding: chunked Header。\n再根据此 Header 搜索，发现果然有人遇到过类似问题，请参考这几个链接。\nhttps://github.com/traefik/traefik/issues/7741 https://github.com/spring-projects/spring-framework/issues/21523 https://github.com/spring-projects/spring-boot/issues/37646 https://stackoverflow.com/questions/77042701/nginx-upstream-sent-duplicate-header-line-transfer-encoding-chunked-previo 从上面链接描述中可知，不仅 Traefik 会出现此问题，nginx 包含以 nginx 为基础的 ingress 也会出现同样问题，不过 nginx 返回错误信息类似 Nginx: upstream sent duplicate header line: \u0026quot;Transfer-Encoding: chunked\u0026quot;, previous value: \u0026quot;Transfer-Encoding: chunked” ，返回错误码一般是 502 Bad Gateway。\n我所使用的 Traefik(2.10.6) 和 Spring Boot(2.7.17) 都是当前日期最新版本，目前仍然有问题。\n出现此问题的代码类似如下。\nimport org.springframework.beans.factory.annotation.Autowired; import org.springframework.http.ResponseEntity; import org.springframework.stereotype.Controller; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.client.RestTemplate; @Controller @RequestMapping(\u0026#34;/status\u0026#34;) public class StatusController { @Autowired private RestTemplate restTemplate; @GetMapping(value = \u0026#34;/test\u0026#34;) public ResponseEntity\u0026lt;String\u0026gt; getStatus() { return restTemplate.getForEntity(\u0026#34;http://another-service/actuator/health\u0026#34;, String.class); } }\r修改为如下方式即解决问题。\nimport org.springframework.beans.factory.annotation.Autowired; import org.springframework.http.ResponseEntity; import org.springframework.stereotype.Controller; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.client.RestTemplate; @Controller @RequestMapping(\u0026#34;/status\u0026#34;) public class StatusController { @Autowired private RestTemplate restTemplate; @GetMapping(value = \u0026#34;/test\u0026#34;) public ResponseEntity\u0026lt;String\u0026gt; getStatus() { // 不要直接用 RestTemplate 返回值，使用 ResponseEntity 重新包装一次 return ResponseEntity.ok(restTemplate.getForEntity(\u0026#34;http://another-service/actuator/health\u0026#34;, String.class)); } }\r另外根据 GitHub Issue 反馈，不仅 RestTemplate，使用 OpenFeign 也会触发以上问题。\n同理，如果大家遇到服务经过 Nginx、Traefik 代理后出现的疑难问题，可关注下 Response Header 是否有异常。\n","date":"2023-11-26","id":16,"permalink":"/blog/duplicate-transfer-encoding-chunked/","summary":"我有一个 Spring Boot 应用服务，提供了一些简单的查询接口，本身运行很正常，通过 curl 或其他 http 客户端 localhost 请求都没有问题。\n某天通过 Traefik 代理了此服务，经过代理后再访问，某个接口一直都是 500 internal server error，其他接口都没有问题。通过 tcpdump 抓包发现，应用服务并没有返回任何 500 错误，而且响应时间和 Body 体大小都很正常。\n根据网上经验排查了 Traefik SSL 证书问题、路径问题、消息体太大问题、请求 Header 不合规问题，都一一否定。最后无意间看了一眼 Response Header，发现 Spring Boot 应用返回了两个 Transfer-Encoding: chunked Header。\n再根据此 Header 搜索，发现果然有人遇到过类似问题，请参考这几个链接。\nhttps://github.com/traefik/traefik/issues/7741 https://github.com/spring-projects/spring-framework/issues/21523 https://github.com/spring-projects/spring-boot/issues/37646 https://stackoverflow.com/questions/77042701/nginx-upstream-sent-duplicate-header-line-transfer-encoding-chunked-previo 从上面链接描述中可知，不仅 Traefik 会出现此问题，nginx 包含以 nginx 为基础的 ingress 也会出现同样问题，不过 nginx 返回错误信息类似 Nginx: upstream sent duplicate header line: \u0026quot;Transfer-Encoding: chunked\u0026quot;, previous value: \u0026quot;Transfer-Encoding: chunked” ，返回错误码一般是 502 Bad Gateway。","tags":["Java"],"title":"重复 Transfer-Encoding Response Header 引起的 Traefik 代理服务 500 问题"},{"content":"","date":"2023-09-07","id":17,"permalink":"/blog/","summary":"","tags":[],"title":"Blog"},{"content":"问题描述：\n一个 Java 应用跑在 K8S 容器内，Pod 内只有 Java 这一个进程。应用跑了一段时间后，CPU、内存占用都不高，但是却出现以下 OutOfMemoryError 错误。\nException in thread \u0026#34;slow-fetch-15\u0026#34; java.lang.OutOfMemoryError: unable to create new native thread 428 at java.lang.Thread.start0(Native Method) 429 at java.lang.Thread.start(Thread.java:719) 430 at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:957) 431 at java.util.concurrent.ThreadPoolExecutor.processWorkerExit(ThreadPoolExecutor.java:1025) 432 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167) 433 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) 进入 Pod 内，尝试执行任何操作，又会出现 unable to start container process 错误。\n一开始怀疑是内存不足，调大了内存，同时也缩小了 Java 的 xss，都不起作用。\n真实原因： K8S 容器限制了 PID 数，无法创建新的线程，在 Pod 内 cat /sys/fs/cgroup/pids/pids.max 发现是 1024。\n关于 K8S pid limit， 可参考此资料：https://kubernetes.io/zh-cn/docs/concepts/policy/pid-limiting/.\n但是，PID 为什么会超呢，Pod 内只有一个 Java 进程，PID 数不应该是 1 个吗，这个 PID 限制为什么影响了线程。\n简单来讲，在 Linux 中线程其实是通过轻量级进程实现的，也就是 LWP(light weight process)，因此在 Linux 中每个线程都是一个进程，都拥有一个 PID，换句话说，操作系统原理中的线程，对应的其实是 Linux 中的进程（即 LWP），因此 Linux 内核中的 PID 对应的其实是原理中的 TID。\n在 Pod 内通过 top -p pid -H 查看，可以看到第一列每个线程都分配了一个 PID。\nPID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 101 root 20 0 8622220 5.1g 15640 S 0.3 8.1 0:16.29 VM Thread 112 root 20 0 8622220 5.1g 15640 S 0.3 8.1 0:46.13 C2 CompilerThre 113 root 20 0 8622220 5.1g 15640 S 0.3 8.1 0:39.62 C1 CompilerThre 846 root 20 0 8622220 5.1g 15640 S 0.3 8.1 0:00.64 NettyClientSele 850 root 20 0 8622220 5.1g 15640 S 0.3 8.1 0:00.54 NettyClientWork 1 root 20 0 8622220 5.1g 15640 S 0.0 8.1 0:00.27 java 89 root 20 0 8622220 5.1g 15640 S 0.0 8.1 0:00.99 java 90 root 20 0 8622220 5.1g 15640 S 0.0 8.1 0:03.29 java 91 root 20 0 8622220 5.1g 15640 S 0.0 8.1 0:03.27 java 92 root 20 0 8622220 5.1g 15640 S 0.0 8.1 0:03.26 java 93 root 20 0 8622220 5.1g 15640 S 0.0 8.1 0:03.30 java 94 root 20 0 8622220 5.1g 15640 S 0.0 8.1 0:01.43 java 95 root 20 0 8622220 5.1g 15640 S 0.0 8.1 0:00.11 java 96 root 20 0 8622220 5.1g 15640 S 0.0 8.1 0:00.12 java 97 root 20 0 8622220 5.1g 15640 S 0.0 8.1 0:00.16 java 98 root 20 0 8622220 5.1g 15640 S 0.0 8.1 0:00.31 java 99 root 20 0 8622220 5.1g 15640 S 0.0 8.1 0:00.32 java 为什么要限制 POD PID 数。类似 CPU 和内存，进程 ID（PID）也是节点上的一种基础资源，很容易就会在尚未超出其它资源约束的时候就已经触及任务个数上限，进而导致宿主机不稳定。某日某个不起眼的服务因为无节制创建了 N 多线程，把整个宿主机打挂了，谁痛谁知道啊。\n","date":"2023-09-07","id":18,"permalink":"/blog/k8s-pid-limiting-oom/","summary":"问题描述：\n一个 Java 应用跑在 K8S 容器内，Pod 内只有 Java 这一个进程。应用跑了一段时间后，CPU、内存占用都不高，但是却出现以下 OutOfMemoryError 错误。\nException in thread \u0026#34;slow-fetch-15\u0026#34; java.lang.OutOfMemoryError: unable to create new native thread 428 at java.lang.Thread.start0(Native Method) 429 at java.lang.Thread.start(Thread.java:719) 430 at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:957) 431 at java.util.concurrent.ThreadPoolExecutor.processWorkerExit(ThreadPoolExecutor.java:1025) 432 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167) 433 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) 进入 Pod 内，尝试执行任何操作，又会出现 unable to start container process 错误。\n一开始怀疑是内存不足，调大了内存，同时也缩小了 Java 的 xss，都不起作用。\n真实原因： K8S 容器限制了 PID 数，无法创建新的线程，在 Pod 内 cat /sys/fs/cgroup/pids/pids.max 发现是 1024。\n关于 K8S pid limit， 可参考此资料：https://kubernetes.io/zh-cn/docs/concepts/policy/pid-limiting/.","tags":["k8s","Java"],"title":"K8S 容器 PID 限制引起的 Java OutOfMemoryError"},{"content":"故事背景：\n一个 K8S Pod，里面只有一个 Java 进程，K8S request 和 limit memory 都是 2G，Java 进程核心参数包括：-XX:+UseZGC -Xmx1024m -Xms768m。\n服务启动一段时间后，查看 Grafana 监控数据，Pod 内存使用量约 1.5G，JVM 内存使用量约 500M，通过 jvm dump 分析没有任何大对象，运行三五天后出现 K8S Container OOM。\n首先区分下 Container OOM 和 Jvm OOM，Container OOM 是 Pod 内进程申请内存大约 K8S Limit 所致。\n问题来了：\nPod 2G 内存，JVM 设置了 Xmx 1G，已经预留了 1G 内存，为什么还会 Container OOM，这预留的 1G 内存被谁吃了。 正常情况下（无 Container OOM），Grafana 看到的监控数据，Pod 内存使用量 1.5G， JVM 内存使用量 500M，差别为什么这么大。 Pod 内存使用量为什么超过 Xmx 限制。 Grafana 监控图。\n统计指标 Pod 内存使用量统计的指标是 container_memory_working_set_bytes：\ncontainer_memory_usage_bytes = container_memory_rss + container_memory_cache + kernel memory container_memory_working_set_bytes = container_memory_usage_bytes - total_inactive_file（未激活的匿名缓存页） container_memory_working_set_bytes 是容器真实使用的内存量，也是资源限制 limit 时的 OOM 判断依据。\n另外注意 cgroup 版本差异： container_memory_cache reflects cache (cgroup v1) or file (cgroup v2) entry in memory.stat.\nJVM 内存使用量统计的指标是 jvm_memory_bytes_used： heap、non-heap 以及其他 真实用量总和。下面解释其他。\n首先说结论：在 POD 内，通过 top、free 看到的指标都是不准确的，不用看了，如果要看真实的数据以 cgroup 为准。\ncontainer_memory_working_set_bytes 指标来自 cadvisor，cadvisor 数据来源 cgroup，可以查看以下文件获取真实的内存情况。\n# cgroup v2 文件地址 ll /sys/fs/cgroup/memory.* -r--r--r-- 1 root root 0 Jan 6 16:25 /sys/fs/cgroup/memory.current -r--r--r-- 1 root root 0 Jan 6 16:25 /sys/fs/cgroup/memory.events -r--r--r-- 1 root root 0 Jan 6 16:25 /sys/fs/cgroup/memory.events.local -rw-r--r-- 1 root root 0 Jan 7 11:50 /sys/fs/cgroup/memory.high -rw-r--r-- 1 root root 0 Jan 7 11:50 /sys/fs/cgroup/memory.low -rw-r--r-- 1 root root 0 Jan 7 11:50 /sys/fs/cgroup/memory.max -rw-r--r-- 1 root root 0 Jan 6 16:25 /sys/fs/cgroup/memory.min -r--r--r-- 1 root root 0 Jan 6 16:25 /sys/fs/cgroup/memory.numa_stat -rw-r--r-- 1 root root 0 Jan 7 11:50 /sys/fs/cgroup/memory.oom.group -rw-r--r-- 1 root root 0 Jan 6 16:25 /sys/fs/cgroup/memory.pressure -r--r--r-- 1 root root 0 Jan 6 16:25 /sys/fs/cgroup/memory.stat -r--r--r-- 1 root root 0 Jan 6 16:25 /sys/fs/cgroup/memory.swap.current -r--r--r-- 1 root root 0 Jan 6 16:25 /sys/fs/cgroup/memory.swap.events -rw-r--r-- 1 root root 0 Jan 6 16:25 /sys/fs/cgroup/memory.swap.high -rw-r--r-- 1 root root 0 Jan 6 16:25 /sys/fs/cgroup/memory.swap.max JVM 关于使用量和提交量的解释。\nUsed Size：The used space is the amount of memory that is currently occupied by Java objects. 当前实际真的用着的内存，每个 bit 都对应了有值的。\nCommitted Size：The committed size is the amount of memory guaranteed to be available for use by the Java virtual machine.\n操作系统向 JVM 保证可用的内存大小，或者说 JVM 向操作系统已经要的内存。站在操作系统的角度，就是已经分出去（占用）的内存，保证给 JVM 用了，其他进程不能用了。 由于操作系统的内存管理是惰性的，对于已申请的内存虽然会分配地址空间，但并不会直接占用物理内存，真正使用的时候才会映射到实际的物理内存，所以 committed \u0026gt; res 也是很可能的。\nJava 进程内存分析 Pod 的内存使用量 1.5G，都包含哪些。\nkernel memory 为 0，Cache 约 1100M，rss 约 650M，inactive_file 约 200M。可以看到 Cache 比较大，因为这个服务比较特殊有很多文件操作。\n# cgroup v2 变量变了 cat /sys/fs/cgroup/memory.stat anon 846118912 file 2321530880 kernel_stack 10895360 pagetables 15523840 percpu 0 sock 1212416 shmem 1933574144 file_mapped 1870290944 file_dirty 12288 file_writeback 0 swapcached 0 anon_thp 0 file_thp 0 shmem_thp 0 inactive_anon 2602876928 active_anon 176771072 inactive_file 188608512 active_file 199348224 unevictable 0 slab_reclaimable 11839688 slab_unreclaimable 7409400 slab 19249088 workingset_refault_anon 0 workingset_refault_file 318 workingset_activate_anon 0 workingset_activate_file 95 workingset_restore_anon 0 workingset_restore_file 0 workingset_nodereclaim 0 pgfault 2563565 pgmajfault 15 pgrefill 14672 pgscan 25468 pgsteal 25468 pgactivate 106436 pgdeactivate 14672 pglazyfree 0 pglazyfreed 0 thp_fault_alloc 0 thp_collapse_alloc 0 通过 Java 自带的 Native Memory Tracking 看下内存提交量。\n# Java 启动时先打开 NativeMemoryTracking，默认是关闭的。注意不要在生产环境长期开启，有性能损失 java -XX:NativeMemoryTracking=detail -jar # 查看详情 jcmd $(pgrep java) VM.native_memory detail scale=MB\r通过 Native Memory Tracking 追踪到的详情大致如下，关注其中每一项 committed 值。\nNative Memory Tracking: (Omitting categories weighting less than 1MB) Total: reserved=68975MB, committed=1040MB - Java Heap (reserved=58944MB, committed=646MB) (mmap: reserved=58944MB, committed=646MB) - Class (reserved=1027MB, committed=15MB) (classes #19551) #加载类的个数 ( instance classes #18354, array classes #1197) (malloc=3MB #63653) (mmap: reserved=1024MB, committed=12MB) ( Metadata: ) ( reserved=96MB, committed=94MB) ( used=93MB) ( waste=0MB =0.40%) ( Class space:) ( reserved=1024MB, committed=12MB) ( used=11MB) ( waste=1MB =4.63%) - Thread (reserved=337MB, committed=37MB) (thread #335) #线程的个数 (stack: reserved=336MB, committed=36MB) (malloc=1MB #2018) - Code (reserved=248MB, committed=86MB) (malloc=6MB #24750) (mmap: reserved=242MB, committed=80MB) - GC (reserved=8243MB, committed=83MB) (malloc=19MB #45814) (mmap: reserved=8224MB, committed=64MB) - Compiler (reserved=3MB, committed=3MB) (malloc=3MB #2212) - Internal (reserved=7MB, committed=7MB) (malloc=7MB #31683) - Other (reserved=18MB, committed=18MB) (malloc=18MB #663) - Symbol (reserved=19MB, committed=19MB) (malloc=17MB #502325) (arena=2MB #1) - Native Memory Tracking (reserved=12MB, committed=12MB) (malloc=1MB #8073) (tracking overhead=11MB) - Shared class space (reserved=12MB, committed=12MB) (mmap: reserved=12MB, committed=12MB) - Module (reserved=1MB, committed=1MB) (malloc=1MB #4996) - Synchronization (reserved=1MB, committed=1MB) (malloc=1MB #2482) - Metaspace (reserved=97MB, committed=94MB) (malloc=1MB #662) (mmap: reserved=96MB, committed=94MB) - Object Monitors (reserved=8MB, committed=8MB) (malloc=8MB #39137) Heap Heap 是 Java 进程中使用量最大的一部分内存，是最常遇到内存问题的部分，Java 也提供了很多相关工具来排查堆内存泄露问题，这里不详细展开。Heap 与 RSS 相关的几个重要 JVM 参数如下： Xms：Java Heap 初始内存大小。（目前我们用的百分比控制，MaxRAMPercentage) Xmx：Java Heap 的最大大小。(InitialRAMPercentage) XX:+UseAdaptiveSizePolicy：是否开启自适应大小策略。开启后，JVM 将动态判断是否调整 Heap size，来降低系统负载。\nMetaspace Metaspace 主要包含方法的字节码，Class 对象，常量池。一般来说，记载的类越多，Metaspace 使用的内存越多。与 Metaspace 相关的 JVM 参数有： XX:MaxMetaspaceSize: 最大的 Metaspace 大小限制【默认无限制】 XX:MetaspaceSize=64M: 初始的 Metaspace 大小。如果 Metaspace 空间不足，将会触发 Full GC。 类空间占用评估，给两个数字可供参考：10K 个类约 90M，15K 个类约 100M。 什么时候回收：分配给一个类的空间，是归属于这个类的类加载器的，只有当这个类加载器卸载的时候，这个空间才会被释放。释放 Metaspace 的空间，并不意味着将这部分空间还给系统内存，这部分空间通常会被 JVM 保留下来。 扩展：参考资料中的Java Metaspace 详解，这里完美解释 Metaspace、Compressed Class Space 等。\nThread NMT 中显示的 Thread 部分内存与线程数与 -Xss 参数成正比，一般来说 committed 内存等于 Xss *线程数 。\nCode JIT 动态编译产生的 Code 占用的内存。这部分内存主要由-XX:ReservedCodeCacheSize 参数进行控制。\nInternal Internal 包含命令行解析器使用的内存、JVMTI、PerfData 以及 Unsafe 分配的内存等等。 需要注意的是，Unsafe_AllocateMemory 分配的内存在 JDK11 之前，在 NMT 中都属于 Internal，但是在 JDK11 之后被 NMT 归属到 Other 中。\nSymbol Symbol 为 JVM 中的符号表所使用的内存，HotSpot 中符号表主要有两种：SymbolTable 与 StringTable。 大家都知道 Java 的类在编译之后会生成 Constant pool 常量池，常量池中会有很多的字符串常量，HotSpot 出于节省内存的考虑，往往会将这些字符串常量作为一个 Symbol 对象存入一个 HashTable 的表结构中即 SymbolTable，如果该字符串可以在 SymbolTable 中 lookup（SymbolTable::lookup）到，那么就会重用该字符串，如果找不到才会创建新的 Symbol（SymbolTable::new_symbol）。 当然除了 SymbolTable，还有它的双胞胎兄弟 StringTable（StringTable 结构与 SymbolTable 基本是一致的，都是 HashTable 的结构），即我们常说的字符串常量池。平时做业务开发和 StringTable 打交道会更多一些，HotSpot 也是基于节省内存的考虑为我们提供了 StringTable，我们可以通过 String.intern 的方式将字符串放入 StringTable 中来重用字符串。\nNative Memory Tracking Native Memory Tracking 使用的内存就是 JVM 进程开启 NMT 功能后，NMT 功能自身所申请的内存。\n观察上面几个区域的分配，没有明显的异常。\nNMT 追踪到的 是 Committed，不一定是 Used，NMT 和 cadvisor 没有找到必然的对应的关系。可以参考 RSS，cadvisor 追踪到 RSS 是 650M，JVM Used 是 500M，还有大约 150M 浮动到哪里去了。\n因为 NMT 只能 Track JVM 自身的内存分配情况，比如：Heap 内存分配，direct byte buffer 等。无法追踪的情况主要包括：\n使用 JNI 调用的一些第三方 native code 申请的内存，比如使用 System.Loadlibrary 加载的一些库。 标准的 Java Class Library，典型的，如文件流等相关操作（如：Files.list、ZipInputStream 和 DirectoryStream 等）。主要涉及到的调用是 Unsafe.allocateMemory 和 java.util.zip.Inflater.init(Native Method)。 怎么追踪 NMT 追踪不到的其他内存，目前是安装了 jemalloc 内存分析工具，他能追踪底层内存的分配情况输出报告。\n通过 jemalloc 内存分析工具佐证了上面的结论，Unsafe.allocateMemory 和 java.util.zip.Inflater.init 占了 30%，基本吻合。\n启动 arthas 查看下类调用栈，在 arthas 里执行以下命令：\n# 先设置 unsafe true options unsafe true # 这个没有 stack sun.misc.Unsafe allocateMemory # 这个有 stack jdk.internal.misc.Unsafe allocateMemory stack java.util.zip.Inflater inflate # stack 经常追踪不到，改用 profiler 输出内存分配火焰图 profiler start --event alloc --duration 600 profiler start --event Unsafe_AllocateMemory0 --duration 600\r通过上面的命令，能看到 MongoDB 和 netty 一直在申请使用内存。注意：早期的 mongodb client 确实有无法释放内存的 bug，但是在我们场景，长期观察会发现内存申请了逐渐释放了，没有持续增长。回到开头的 ContainerOOM 问题，可能一个原因是流量突增，MongoDB 申请了更多的内存导致 OOM，而不是因为内存不释放。\nts=2022-12-29 21:20:01;thread_name=ForkJoinPool.commonPool-worker-1;id=22;is_daemon=true;priority=1;TCCL=jdk.internal.loader.ClassLoaders$AppClassLoader@1d44bcfa @jdk.internal.misc.Unsafe.allocateMemory() at java.nio.DirectByteBuffer.\u0026lt;init\u0026gt;(DirectByteBuffer.java:125) at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:332) at sun.nio.ch.Util.getTemporaryDirectBuffer(Util.java:243) at java.net.Socket$SocketOutputStream.write(Socket.java:1035) at com.mongodb.internal.connection.SocketStream.write(SocketStream.java:99) at com.mongodb.internal.connection.InternalStreamConnection.sendMessage(InternalStreamConnection.java:426) at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:99) at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:444) ……………………………… at com.mongodb.MongoClientExt$1.execute(MongoClientExt.java:42) ……………………………… 另外，arthas 自带的 profiler 有时候经常追踪失败，可以切换到原始的 async-profiler，用他来追踪“其他”内存分配比较有效。\n总结 Java 进程内存占用：Total=heap + non-heap + 上面说的这个其他。\njemalloc jemalloc 是一个比 glibc malloc 更高效的内存池技术，在 Facebook 公司被大量使用，在 FreeBSD 和 FireFox 项目中使用了 jemalloc 作为默认的内存管理器。使用 jemalloc 可以使程序的内存管理性能提升，减少内存碎片。\n比如 Redis 内存分配默认使用的 jemalloc，早期版本安装 redis 是需要手动安装 jemalloc 的，现在 redis 应该是在编译期内置好了。\n原来使用 jemalloc 是为了分析内存占用，通过 jemalloc 输出当前内存分配情况，或者通过 diff 分析前后内存差，大概能看出内存都分给睡了，占了多少，是否有内存无法释放的情况。\n后来参考了这个文章，把 glibc 换成 jemalloc 带来性能提升，降低内存使用，决定一试。\nhow we’ve reduced memory usage without changing any code：https://blog.malt.engineering/java-in-k8s-how-weve-reduced-memory-usage-without-changing-any-code-cbef5d740ad\nDecreasing RAM Usage by 40% Using jemalloc with Python \u0026amp; Celery: https://zapier.com/engineering/celery-python-jemalloc/\n一个服务，运行一周，观察效果。\n使用 Jemalloc 之前： 使用 Jemalloc 之后（同时调低了 Pod 内存）： 注：以上结果未经生产长期检验。\n内存交还给操作系统 注意：下面的操作，生产环境不建议这么干。\n默认情况下，OpenJDK 不会主动向操作系统退还未用的内存（不严谨）。看第一张监控的图，会发现运行一段时间后，Pod 的内存使用量一直稳定在 80%\u0026ndash;90%不再波动。\n其实对于 Java 程序，浮动比较大的就是 heap 内存。其他区域 Code、Metaspace 基本稳定\n# 执行命令获取当前 heap 情况 jhsdb jmap --heap --pid $(pgrep java) #以下为输出 Attaching to process ID 7, please wait... Debugger attached successfully. Server compiler detected. JVM version is 17.0.5+8-LTS using thread-local object allocation. ZGC with 4 thread(s) Heap Configuration: MinHeapFreeRatio = 40 MaxHeapFreeRatio = 70 MaxHeapSize = 1287651328 (1228.0MB) NewSize = 1363144 (1.2999954223632812MB) MaxNewSize = 17592186044415 MB OldSize = 5452592 (5.1999969482421875MB) NewRatio = 2 SurvivorRatio = 8 MetaspaceSize = 22020096 (21.0MB) CompressedClassSpaceSize = 1073741824 (1024.0MB) MaxMetaspaceSize = 17592186044415 MB G1HeapRegionSize = 0 (0.0MB) Heap Usage: ZHeap used 310M, capacity 710M, max capacity 1228M Java 内存不交还，几种情况：\nXms 大于实际需要的内存，比如我们服务设置了 Xms768M，但是实际上只需要 256，高峰期也就 512，到不了 Xms 的值也就无所谓归还。 上面 jmap 的结果，可以看到 Java 默认的配置 MaxHeapFreeRatio=70，这个 70% Free 几乎很难达到。（另外注意 Xmx==Xms 的情况下这两个参数无效，因为他怎么扩缩都不会突破 Xms 和 Xmx 的限制）\nMinHeapFreeRatio = 40 空闲堆空间的最小百分比，计算公式为：HeapFreeRatio =(CurrentFreeHeapSize/CurrentTotalHeapSize) * 100，值的区间为 0 到 100，默认值为 40。如果 HeapFreeRatio \u0026lt; MinHeapFreeRatio，则需要进行堆扩容，扩容的时机应该在每次垃圾回收之后。 MaxHeapFreeRatio = 70 空闲堆空间的最大百分比，计算公式为：HeapFreeRatio =(CurrentFreeHeapSize/CurrentTotalHeapSize) * 100，值的区间为 0 到 100，默认值为 70。如果 HeapFreeRatio \u0026gt; MaxHeapFreeRatio，则需要进行堆缩容，缩容的时机应该在每次垃圾回收之后。 对于 ZGC，默认是交还给操作系统的。可通过 -XX:+ZUncommit -XX:ZUncommitDelay=300 这两个参数控制（不再使用的内存最多延迟 300s 归还给 OS，线下环境可以改小点）。\n经过调整后的服务，内存提交在 500\u0026ndash;800M 之间浮动，不再是一条直线。\n内存分析工具速览 使用 Java 自带工具 dump 内存。\njcmd \u0026lt;pid\u0026gt; GC.heap_dump \u0026lt;file-path\u0026gt; # 这个命令执行，JVM 会先触发 gc，然后再统计信息。 jmap -dump:live,format=b,file=/opt/tomcat/logs/dump.hprof \u0026lt;pid\u0026gt;\r使用 jmap 输出内存占用概览。\njmap -histo 1 | head -n 500\r使用 async-profiler 追踪 native 内存分配，输出火焰图。\nasync-profiler/bin/asprof -d 600 -e Unsafe_AllocateMemory0 -f /opt/tomcat/logs/unsafe_allocate.html \u0026lt;pid\u0026gt;\r使用 vmtouch 查看和清理 Linux 系统文件缓存。\n# 查看文件或文件夹占了多少缓存 vmtouch /files vmtouch /dir # 遍历文件夹输出详细占用 vmtouch -v /dir # 清空缓存 vmtouch -e /dir\rpmap 查看内存内容 使用 pmap 查看当前内存分配，如果找到了可疑的内存块，可以通过 gdb 尝试解析出内存块中的内容。\n# pmap 查看内存，先不要排序，便于找出连续的内存块（一般是 2 个一组） # pmap -x \u0026lt;pid\u0026gt; | sort -n -k3 pmap -x \u0026lt;pid\u0026gt; # 举例说明在上面发现有 7f6737dff000 开头的内存块可能异常，一般都是一个或多个一组，是连续的内存 cat /proc/\u0026lt;pid\u0026gt;/smaps \u0026gt; logs/smaps.txt gdb attach \u0026lt;pid\u0026gt; # dump 的起始地址，基于上面 smaps.txt 找到的内容，地址加上 0x 前缀 dump memory /opt/tomcat/logs/gdb-test.dump 0x7f6737dff000 0x7f6737e03000 # 尝试将 dump 文件内容转成可读的 string，其中 -10 是过滤长度大于 10 的，也可以不过滤 strings -10 /opt/tomcat/logs/gdb-test.dump # 如果幸运，能在上面的 strings 中找到你的 Java 类或 Bean 内容，如果不幸都是一堆乱码，可以尝试扩大 dump 内存块，多找几个连续的块试试\r识别 Linux 节点上的 cgroup 版本 cgroup 版本取决于正在使用的 Linux 发行版和操作系统上配置的默认 cgroup 版本。 要检查你的发行版使用的是哪个 cgroup 版本，请在该节点上运行 stat -fc %T /sys/fs/cgroup/ 命令：\nstat -fc %T /sys/fs/cgroup/\r对于 cgroup v2，输出为 cgroup2fs。\n对于 cgroup v1，输出为 tmpfs。\n问题原因分析和调整 回到开头问题，通过上面分析，2G 内存，RSS 其实占用 600M，为什么最终还是 ContainerOOM 了。\nkernel memory 为 0，排除 kernel 泄漏的原因。下面的参考资料里介绍了 kernel 泄露的两种场景。 Cache 很大，说明文件操作多。搜了一下代码，确实有很多 InputStream 调用没有显式关闭，而且有的 InputSteam Root 引用在 ThreadLocal 里，ThreadLocal 只 init 未 remove。 但是，ThreadLocal 的引用对象是线程池，池不回收，所以这部分可能会无法关闭，但是不会递增，但是 cache 也不能回收。 优化办法：ThreadLocal 中对象是线程安全的，无数据传递，直接干掉 ThreadLocal；显式关闭 InputStream。运行一周发现 cache 大约比优化前低 200\u0026ndash;500M。 ThreadLocal 引起内存泄露是 Java 中很经典的一个场景，一定要特别注意。 一般场景下，Java 程序都是堆内存占用高，但是这个服务堆内存其实在 200-500M 之间浮动，我们给他分了 768M，从来没有到过这个值，所以调低 Xms。留出更多内存给 JNI 使用。 线下环境内存分配切换到 jemalloc，长期观察大部分效果可以，但是对部分应用基本没有效果。 经过上述调整以后，线下环境 Pod 内存使用量由 1G 降到 600M 作用。线上环境内存使用量在 50%\u0026ndash;80%之间根据流量大小浮动，原来是 85% 居高不小。\n不同 JVM 参数内存占用对比 以下为少量应用实例总结出来的结果，应用的模型不同占用情况会有比较大差异，仅供对比参考。\n基础参数 中低流量时内存占用（Xmx 6G） 高流量时内存占用 Java 8 + G1 65% 85% Java 17 + G1 60% 75% Java 17 + ZGC 90% 95% Java 21 + ZGC + UseStringDeduplication 85% 90% Java 21 + ZGC + ZGenerational + UseStringDeduplication 65% 80% 总结：\nG1 比 ZGC 占用内存明显减少。 Java 21 ZGC 分代后确实能降低内存。 通过 -XX:+UseStringDeduplication 启用 String 去重后，有的应用能降低 10% 内存，有的几乎无变化。 参考资料 Java 进程内存分析工具：https://stackoverflow.com/questions/53576163/interpreting-jemaloc-data-possible-off-heap-leak/53598622#53598622\nJava 进程内存分布：https://cloud.tencent.com/developer/article/1666640\nJava Metaspace 详解：https://www.javadoop.com/post/metaspace\nhow we’ve reduced memory usage without changing any code：https://blog.malt.engineering/java-in-k8s-how-weve-reduced-memory-usage-without-changing-any-code-cbef5d740ad\nSpring Boot 引起的堆外内存泄漏排查及经验总结：https://tech.meituan.com/2019/01/03/spring-boot-native-memory-leak.html\nPod 进程内存缓存分析：https://zhuanlan.zhihu.com/p/449630026\nLinux 内存中的 Cache 真的能被回收么：https://cloud.tencent.com/developer/article/1115557\nLinux kernel memory 导致的 POD OOM killed: https://www.cnblogs.com/yannwang/p/13287963.html\ncgroup 内存泄露问题：https://www.cnblogs.com/leffss/p/15019898.html\n日志打印导致 page-cache 飙升问题解决： https://juejin.cn/post/6920957433947324423\nlogback 之 AsyncAppender 的原理、源码及避坑建议： https://developer.aliyun.com/article/1127879\nthe rocketmq_client.log file spent too much cache/buffer memory： https://github.com/apache/rocketmq/issues/3252\n","date":"2023-01-07","id":19,"permalink":"/blog/java-memory/","summary":"K8S 容器内 Java 进程内存分析，容器和 Jave OOM 问题定位。","tags":["k8s","Java"],"title":"K8S 容器内 Java 进程内存分析"},{"content":"为 VS Code Web 版 code-server 增加外部认证，并支持多用户，不同用户的 code-server 实例完全隔离。\n主要为了解决问题：\ncode-server 本身只支持配置文件形式的用户名密码认证（截止目前，以后也许会改进）。所以引入了外部认证系统，Google、GitHub、 okta、CAS、Keycloak 等理论上都是支持的。\ncode-server 默认没有数据隔离，所以又加了一层 auth proxy，为每个用户创建一个（或多个）code-server 实例，通过 proxy 代理到各自的实例，以实现用户间的数据隔离。\n使用开源 Auth Proxy，无需自己编码即可实现认证授权流程，比如 code flow with pkce 对大部分人来说读懂这个协议都很困难。\n此文档源码请参考：architecture-diagram\n使用组件 keycloak\nRedhat 开源 IAM 系统，目前也是 CNCF 项目，提供用户、组织服务，提供标准 OIDC。\noauth2-proxy\n认证代理，配合 keycloak 提供完整 OAuth2 Code Flow 认证流程。也可以试试 pomerium，看样子也不错。\n架构图如下。\n核心逻辑 架构图简单解读，所有过程官方文档都有详细说明，都是配置，以官方配置为准。\nkeycloak 创建 client，使用 OIDC 协议，作为 oauth2-proxy 的 provider。\ningress(nginx) 使用 auth_request 指令拦截所有请求，从 oauth2-proxy 进行代理认证，配置可参考 oauth2-proxy auth_request 指导。\nnginx.ingress.kubernetes.io/auth-signin: https://$host/oauth2/start?rd=$escaped_request_uri nginx.ingress.kubernetes.io/auth-url: https://$host/oauth2/auth\r认证通过后，将用户名/ID 作为标识，通过 Http Header （举例如 X-Forwarded-Preferred-Username) 传入 upstream。\ngateway(nginx) 从 Header 中获取用户标识，代理到此用户对应的 code-server 实例。\nlocation / { …… proxy_pass http://code-server-$http_x_forwarded_for_preferred_username; }\rcode-server 各个实例部署时，以免认证方式部署。\n每个 code-server 实例挂载不同的存储，实现完全隔离。\n","date":"2022-09-07","id":20,"permalink":"/blog/code-server/","summary":"为 VS Code Web 版 code-server 增加外部认证，并支持多用户，不同用户的 code-server 实例完全隔离。\n主要为了解决问题：\ncode-server 本身只支持配置文件形式的用户名密码认证（截止目前，以后也许会改进）。所以引入了外部认证系统，Google、GitHub、 okta、CAS、Keycloak 等理论上都是支持的。\ncode-server 默认没有数据隔离，所以又加了一层 auth proxy，为每个用户创建一个（或多个）code-server 实例，通过 proxy 代理到各自的实例，以实现用户间的数据隔离。\n使用开源 Auth Proxy，无需自己编码即可实现认证授权流程，比如 code flow with pkce 对大部分人来说读懂这个协议都很困难。\n此文档源码请参考：architecture-diagram\n使用组件 keycloak\nRedhat 开源 IAM 系统，目前也是 CNCF 项目，提供用户、组织服务，提供标准 OIDC。\noauth2-proxy\n认证代理，配合 keycloak 提供完整 OAuth2 Code Flow 认证流程。也可以试试 pomerium，看样子也不错。\n架构图如下。\n核心逻辑 架构图简单解读，所有过程官方文档都有详细说明，都是配置，以官方配置为准。\nkeycloak 创建 client，使用 OIDC 协议，作为 oauth2-proxy 的 provider。\ningress(nginx) 使用 auth_request 指令拦截所有请求，从 oauth2-proxy 进行代理认证，配置可参考 oauth2-proxy auth_request 指导。\nnginx.ingress.kubernetes.io/auth-signin: https://$host/oauth2/start?rd=$escaped_request_uri nginx.","tags":["devops"],"title":"使用 Visual Studio Code 搭建多用户远程 IDE"},{"content":"备考 CKA （Certified Kubernetes Administrator）过程，心得，遇见问题，CKA 真题。\n一句话总结：按照教程多练习，把控好时间就能通过，期望通过刷题通过考试的年代已经过去了，而且多练习对平时工作真的有用。\n备考环境 备考使用的系统和软件版本如下。\nUbuntu：20.04 Focal Fossa Kubernetes：1.20.7 kubeadm：1.20.7 安装和使用问题记录 kubeadm 安装问题 安装 kubeadm，国内安装使用阿里镜像源。\n$ cat /etc/apt/sources.list.d/kubernetes.list deb https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial main 踩坑：因为使用的是 ubuntu 20.04，代号 focal，专门去各个代理镜像源找kubernetes-focal都没有找到，后来发现 google 官方根本没发布对应的版本，只有kubernetes-xenial， k8s 官方文档里 ubuntu 也是用的这一个版本。可以用，就用他吧。\nkubeadm init 时指定使用阿里镜像源（解决国内连不上 k8s.gcr.io 的问题）、指定版本号（安装考试对应的版本，不一定是最新版本）。\n通过指定--image-repository，不需要手动下载镜像重新打 tag，kubeadm 自动使用指定的 repository。\nkubeadm init --image-repository=registry.aliyuncs.com/google_containers \\ --pod-network-cidr=10.244.0.0/16 \\ --kubernetes-version=v1.20.7\r解决 scheduler Unhealthy，controller-manager Unhealthy 第一次安装完成后通过 kubectl get cs命令，发现 scheduler Unhealthy，controller-manager Unhealthy。\n$ kubectl get cs NAME STATUS MESSAGE scheduler Unhealthy Get \u0026#34;http://127.0.0.1:10251/healthz\u0026#34;: dial tcp 127.0.0.1:10 controller-manager Unhealthy Get \u0026#34;http://127.0.0.1:10252/healthz\u0026#34;: dial tcp 127.0.0.1:10 查看本机端口，10251 和 10252 都没有启动。\n确认 schedule 和 controller-manager 组件配置是否禁用了非安全端口。\n查看配置文件，路径分别为：/etc/kubernetes/manifests/kube-scheduler.yaml 和 /etc/kubernetes/manifests/kube-controller-manager.yaml 将两个配置文件中 --port=0 注释掉（注释掉是否合适待商量）。\nspec: containers: - command: - kube-scheduler - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf - --bind-address=127.0.0.1 # 注释掉 port，其他行原样不要动 - --port=0\r解决 master 无法调度 我的环境是单节点，既当 master 又当 worker，kubeadm 安装完成后默认 master 节点是不参与调度的，pod 会一直 pending。\nkubectl describe node 发现 node 的 Taints 里有 node-role.kubernetes.io/master:NoSchedule。\n设置 k8s master 节点参与 POD 调度。\nkubectl taint nodes your-node-name node-role.kubernetes.io/master-\r考试心得 刷新浏览器会导致考试被终止。\n提前演练敲一遍，时间其实挺紧张。\n官方kubectl Cheat Sheet章节非常有用，必考。\n命令自动补全 source \u0026lt;(kubectl completion bash)。\n尽量使用命令创建 Pod、deployment、service。\nkubectl run podname --image=imagename --restart=Never -n namespace kubectl run \u0026lt;deploymentname\u0026gt; --image=\u0026lt;imagename\u0026gt; -n \u0026lt;namespace\u0026gt; kubectl expose \u0026lt;deploymentname\u0026gt; --port=\u0026lt;portNo.\u0026gt; --name=\u0026lt;svcname\u0026gt; 使用 dry-run。\nkubectl run \u0026lt;podname\u0026gt; --image=\u0026lt;imagename\u0026gt; --restart=Never --dry-run -o yaml \u0026gt; title.yaml 使用 kubectl -h 查看各个命令的帮助，很多都在 Examples 里。比如 kubectl expose -h。\nCKA 真题练习 真题会过时，别指望着刷刷题就通过考试，老老实实学一遍。\n将所有 pv 按照 name/capacity 排序。\n# sort by name kubectl get pv --sort-by=.metadata.name # sort by capacity kubectl get pv --sort-by=.spec.capacity.storage deployment 扩容。\nkubectl scale deployment test --replicas=3 Set the node named ek8s-node-1 as unavaliable and reschedule all the pods running on it.\nkubectl cordon ek8s-node-1 # drain node 的时候可能出错，根据错误提示加参数 kubectl drain ek8s-node-1 --delete-local-data --ignore-daemonsets --force Form the pod label name-cpu-loader,find pods running high CPU workloads and write the name of the pod consuming most CPU to the file /opt/KUTR00401/KURT00401.txt(which alredy exists).\n# 注意题目中并没有提 namespace，可以先看下这个 label 的 pod 在哪个 namespace，确定命令中要不要加 namespace kubectl top pods -l name=name-cpu-loader --sort-by=cpu echo \u0026#39;排名第一的 pod 名称\u0026#39; \u0026gt;\u0026gt;/opt/KUTR00401/KUTR00401.txt ","date":"2022-02-26","id":21,"permalink":"/blog/kubernetes-cka/","summary":"备考 CKA （Certified Kubernetes Administrator）过程，心得，遇见问题，CKA 真题。\n一句话总结：按照教程多练习，把控好时间就能通过，期望通过刷题通过考试的年代已经过去了，而且多练习对平时工作真的有用。\n备考环境 备考使用的系统和软件版本如下。\nUbuntu：20.04 Focal Fossa Kubernetes：1.20.7 kubeadm：1.20.7 安装和使用问题记录 kubeadm 安装问题 安装 kubeadm，国内安装使用阿里镜像源。\n$ cat /etc/apt/sources.list.d/kubernetes.list deb https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial main 踩坑：因为使用的是 ubuntu 20.04，代号 focal，专门去各个代理镜像源找kubernetes-focal都没有找到，后来发现 google 官方根本没发布对应的版本，只有kubernetes-xenial， k8s 官方文档里 ubuntu 也是用的这一个版本。可以用，就用他吧。\nkubeadm init 时指定使用阿里镜像源（解决国内连不上 k8s.gcr.io 的问题）、指定版本号（安装考试对应的版本，不一定是最新版本）。\n通过指定--image-repository，不需要手动下载镜像重新打 tag，kubeadm 自动使用指定的 repository。\nkubeadm init --image-repository=registry.aliyuncs.com/google_containers \\ --pod-network-cidr=10.244.0.0/16 \\ --kubernetes-version=v1.20.7\r解决 scheduler Unhealthy，controller-manager Unhealthy 第一次安装完成后通过 kubectl get cs命令，发现 scheduler Unhealthy，controller-manager Unhealthy。\n$ kubectl get cs NAME STATUS MESSAGE scheduler Unhealthy Get \u0026#34;http://127.","tags":[],"title":"备考 CKA 过程，CKA 真题分享"},{"content":"日常开发速查手册，程序员的瑞士军刀，常用优秀命令行工具和编程语句汇总。\n多一份复制粘贴，多一分开心。\n","date":"2023-09-07","id":22,"permalink":"/docs/tldr/","summary":"日常开发速查手册，程序员的瑞士军刀，常用优秀命令行工具","tags":[],"title":"TL;DR"},{"content":"从 Spring 到 Spring Boot，迁移升级快速入门以及各种踩坑记录。\n概述 从 Spring 到 Spring Boot，整体开发、运行方式主要变化。\n- 当前（老）模式 新模式（本地开发） 新模式（线上运行） 开发习惯 Spring + 外置 Tomcat Spring Boot（embed tomcat） Spring Boot War or Jar Java 版本 8、11、16、17 11、17、21（推荐） 11、17、21（推荐） Tomcat 版本 8.x、9.x 9.x 9.x（推荐）、10.x 说明：\n理论上完全兼容 Java11，但是要求业务方尽量使用 Java17 或 21。其他版本都是实验性质尽量兼容。 线上运行支持 Spring Boot jar 直接运行，但主要业务仍推荐以 war + tomcat 为主。如果希望以 java -jar 方式运行，参考下面的章节“jar 方式运行”描述。 目前 Spring Boot 主要推行版本是 2.7.x。 3.x 版本逐渐适配中，注意 3.x 要求 Java 最低版本是 17。 快速开始 线下支撑系统导航，点击 脚手架 进入 spring start 页面，按自己需求选择模块，生成自己业务模式初始化代码。 写（Copy）业务代码到项目里，修改 pom.xml 根据需要添加新的依赖。 查看本文档中 遇见问题及解决方案 章节，注意如果是老项目迁移，这一步很重要。 本地开发工具启动 main 方法。 上线发布系统，选择 tomcat9:openjdk17 镜像，并勾选 镜像 JDK 版本编译代码。 以上生成的一个最简略的代码结构，更多复杂使用方式参考下方主要 starter 使用说明。\n主要 starter 使用说明 文档会延后，代码不会骗人，更多说明参考各个项目源码的 README，README 会实时更新。\nfxiaoke-spring-cloud-parent 目前有两个公司级父 pom：\n旧：com.fxiaoke.common.fxiaoke-parent-pom 用于原 Spring + Tomcat 方式开发。 新：com.fxiaoke.cloud.fxiaoke-spring-cloud-parent 用于 Spring Boot/Cloud 方式开发。 注意：\nfxiaoke-spring-cloud-parent 导入了 fxiaoke-parent-pom，所以纷享包版本都是一致的，但是三方包比如 spring/netty/okhttp 会随 Spring Boot 版本。 Maven 项目 parent 统一使用公司新 parent pom，这里定义了 Spring Boot、Spring Cloud 以及内部定制的各种 support 和 starter 版本号。\n\u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;com.fxiaoke.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;fxiaoke-spring-cloud-parent\u0026lt;/artifactId\u0026gt; \u0026lt;!-- 注意使用最新版本，可以从脚手架里获取最新版本 --\u0026gt; \u0026lt;version\u0026gt;2.7.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;relativePath/\u0026gt; \u0026lt;/parent\u0026gt;\r主要升级项需关注：\n老项目切换到 Spring Boot 先分析实际生效的 maven dependency，关注下核心包版本是否有大的升级，是否可能对业务造成影响。 Spock and Groovy：Spock 由原来的 1.x 升级到 2.x 版本，同时 Groovy 升级到 4.x 版本，Junit4 升级到 Junit5。 spring-boot-starter-actuator 目前强制依赖 spring-boot-starter-actuator，容器镜像里使用它实现健康检查。 另外强制依赖 spring-boot-starter-web，因为有些基础组件依赖了 ServletContext。\n注意： actuator 的引入会带来一些额外收益，之前我们健康检测只检查服务端口是否有响应，而 actuator 默认还额外检查各个中间件的状态，业务方可根据需要自行增加或删除某些中间件的状态到健康检测服务，具体方式和更多高级应用参考 spring-boot-starter-actuator 官方文档。\ncms-spring-cloud-starter 配置中心 starter，类似 spring-cloud-consul/nacos/config，对接配置中心，实现配置文件动态加载、刷新，代替原 ReloadablePropertySourcesPlaceholderConfigurer。\n使用步骤：\n引入 starter。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.fxiaoke.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;cms-spring-cloud-starter\u0026lt;/artifactId\u0026gt; \u0026lt;!-- 版本号建议不写，使用 parent 定义好的版本 --\u0026gt; \u0026lt;/dependency\u0026gt;\r增加 src/main/resources/application.properties 文件，内容如下。\n# 当前模块名，必填，必须全局唯一，一般和 maven 子模块保持一致 spring.application.name=cms-starter-sample # 配置导入，这一行必须写。但是配置文件本身是否必须是通过 optional 控制的 spring.config.import=optional:cms:${spring.application.name}\r我们使用 spring.config.import 固定格式为 optional:cms:file-name。 optional 表示这个文件可选，配置中心不存在的时候也允许启动，cms 是固定字符代表对接 fs 配置中心。\n在 CMS 配置中心创建需要的配置文件，文件名为 spring-cloud-${spring.application.name}，其中${spring.application.name}替换成真正的文件名，注意当前版本自动追加了前缀spring-cloud-且不允许修改。\n代码中使用几种方式参考 sample 代码，文档查看 spring 官方ConfigurationProperties和 @Value 说明。\n配置变更后，如果想响应变更事件，实现自己逻辑，自定义类中implements ApplicationListener\u0026lt;RefreshScopeRefreshedEvent\u0026gt;\n配置加解密，在配置中心中有个加密功能框（如果看不到可能是没有权限），先使用本 starter 的秘钥加密，使用固定格式 ENC（加密后的内容）配置到文件里，在 java 里 get value 就是已经解密后的了。例如：\nsample.sensitive=ENC(xxx)\r响应配置更新：\n对于使用使用 ConfigurationProperties 映射的对象类，从对象中每次 get 的值都是刷新后的。推荐这种方式。\n@Data @Configuration @ConfigurationProperties(prefix = \u0026#34;sample\u0026#34;) public class SampleProperties { private String name; }\r@RefreshScope + @Value 获取 Value 注解的新值。\n@Service @RefreshScope public class ValueService { @Value(\u0026#34;${sample.over.value}\u0026#34;) @Getter private String watchValue; }\r监听 RefreshScopeRefreshedEvent 事件。\n@EventListener(RefreshScopeRefreshedEvent.class) public void handlerPropertiesChangeEvent(RefreshScopeRefreshedEvent event) { //此时配置 Bean 已刷新完成，处理自己的业务逻辑 }\rjar 方式运行 如果不使用外置 Tomcat，使用 java -jar 方式直接运行，首先打包模式为 jar 并在发布时增加环境变量 SPRING_BOOT_JAR_APP=true。\n与外置 Tomcat 模式差别：\njar 模式一个 pod 内只能部署一个模块，不支持多模块合并部署。 jar 模式不会自动把 jar 解压成文件夹（war 模式会），所以关于文件资源的读写要特别注意，参考下面的问题描述章节。 老项目迁移升级步骤 改 pom.xml：修改 parent，引入必须的 starter，删除所有关于 Spring/logback/junit 的依赖项（由 Spring Boot Starter 自动引入），插件切换到 spring-boot-maven-plugin。 原有的 xml 配置，可以改为注解形式，也可以不改直接 @ImportResource 使用。 注意配置扫描范围，原来 xml 中可能是配置是某几个包，Spring Boot 默认扫描 Application.java 所在包，范围可能扩大。 删除原来 web.xml 相关配置，如果有额外的 filter、servlet，需要额外定义 Bean 注入。 Unit Test 更换注解，目前默认 junit 版本是 junit5，原 junit4 注解有较大变更，详细请参考下面的参考资料。 迁移辅助工具 OpenRewrite\nOpenRewrite 快速入门请参考：使用 OpenRewrite 进行代码重构。\nEMT4J\n通过静态扫描指导从 Java 8 升级到 Java 17 需要注意的变更项。\ntomcat-jakartaee-migration\nTomcat 9 到 10 迁移辅助工具。\nspring-boot-migrator\nSpring Boot 迁移工具，通过扫描输出 从 Spring 到 Spring Boot，以及 Spring Boot 3 迁移指导意见。\nWar 配置转移 If you try to migrate a Java legacy application to Spring Boot you will find out that Spring Boot ignores the web.xml file when it is run as embedded container.\nwebapp web.xml 配置如何转移到 spring boot war 形式。 参考：https://www.baeldung.com/spring-boot-dispatcherservlet-web-xml\n遇见问题及解决方案 下面记录一些比较常见的问题，更多问题请参考下面章节中的参考资料，里面的问题很有参考价值。\ncom.google.common.io.Resources#getResource 无法获取到 jar 包内资源\n如果是 java -jar 模式运行， Thread.currentThread().getContextClassLoader().getResource(resourceName) 形式的调用都无法获取 jar 包内资源，可考虑使用 InputStream resourceFile = getClass().getResourceAsStream(resourceName); 方式代替。\nPostConstruct 和 PreDestroy 注解不生效\n参考链接 https://stackoverflow.com/questions/18161682/why-is-postconstruct-not-called 先逐个排除。\n我所遇到的原因：PostConstruct、PreDestroy 等注解可能存在多个实现或者过个版本，比如以下 jar 包都可能包含：\njavax.annotation-api-1.3.2.jar jakarta.annotation-api-1.3.5.jar jboss-annotations-api_1.3_spec-2.0.1.Final.jar 解决方法：排除依赖，只保留 jakarta.annotation-api 一种，且只能有一个版本。\nkafka 使用报错，日志类似如下：\nERROR c.f.s.SenderManager cannot send, org.apache.kafka.common.KafkaException: org.apache.kafka.clients.producer.internals.DefaultPartitioner is not an instance of org.apache.kafka.clients.producer.Partitioner\r原因：因为 classpath 下包含多个不同版本的 kafka-client.jar，检查依赖项，确保只引用一个版本。\n告警：SLF4J: Class path contains multiple SLF4J bindings.\n多个 jar 包含 SLF4J 实现，或引入了多个 logback 版本，请根据提示排除不需要的 jar 包。\nXML 中使用 AOP 注解，运行期报错如下（建议用到 AOP 的提前检查，因为运行期才会报错）：JoinPointMatch ClassNotFoundException\nCaused by: java.lang.ClassNotFoundException: org.aspectj.weaver.tools.JoinPointMatch at org.apache.catalina.loader.WebappClassLoaderBase.loadClass(WebappClassLoaderBase.java:1412) at org.apache.catalina.loader.WebappClassLoaderBase.loadClass(WebappClassLoaderBase.java:1220) ... 58 more\r依赖 spring aop，请确认是否引入 spring-boot-starter-aop。\n本地使用 Java 17 启动，类似如下报错。\nERROR o.s.b.SpringApplication Application run failed java.lang.reflect.InaccessibleObjectException: Unable to make protected final java.lang.Class java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int,java.security.ProtectionDomain) throws java.lang.ClassFormatError accessible: module java.base does not \u0026#34;opens java.lang\u0026#34; to unnamed module @443118b0 at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:354) at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:297) at java.base/java.lang.reflect.Method.checkCanSetAccessible(Method.java:199) at java.base/java.lang.reflect.Method.setAccessible(Method.java:193) at com.alibaba.dubbo.common.compiler.support.JavassistCompiler.doCompile(JavassistCompiler.java:123) [6 skipped] at com.alibaba.dubbo.common.compiler.support.AbstractCompiler.compile(AbstractCompiler.java:59) at com.alibaba.dubbo.common.compiler.support.AdaptiveCompiler.compile(AdaptiveCompiler.java:46)\r本地命令行中启动参数里主动追加以下参数（这些参数在发布系统的镜像里默认已经加了），IDEA 启动时设置到VM options里：\n--add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.math=ALL-UNNAMED --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.rmi/sun.rmi.transport=ALL-UNNAMED\rBean 重复定义错误，报错信息类似如下。\nThe bean \u0026#39;eieaConverterImpl\u0026#39;, defined in class path resource [spring/ei-ea-converter.xml], could not be registered. A bean with that name has already been defined in class path resource [spring/ei-ea-converter.xml] and overriding is disabled. Action: Consider renaming one of the beans or enabling overriding by setting spring.main.allow-bean-definition-overriding=true\r可能因为注解扫描范围增广或者有同样包多版本引入，导致扫描到多个。确认多处定义是否一致，如果不一致查看原项目哪个生效，以生效为准。如果一致，找到定义的地方查看是否能整个文件排除掉，实在不能在 application.properties 中设置 spring.main.allow-bean-definition-overriding=true 可解决。\n如下报错 class xxx is not visible from class loader，常见于 dubbo 服务。\n解决办法：不要用 spring-boot-devtools。 参考链接：https://blog.csdn.net/zhailuxu/article/details/79305661\ndubbo 服务 java.io.IOException: invalid constant type: 18，日志类似如下：\nWrapped by: java.lang.IllegalStateException: Can not create adaptive extenstion interface com.alibaba.dubbo.rpc.Protocol, cause: java.io.IOExc eption: invalid constant type: 18 at com.alibaba.dubbo.common.extension.ExtensionLoader.createAdaptiveExtension(ExtensionLoader.java:723) at com.alibaba.dubbo.common.extension.ExtensionLoader.getAdaptiveExtension(ExtensionLoader.java:455) ... 29 common frames omitted Wrapped by: java.lang.IllegalStateException: fail to create adaptive instance: java.lang.IllegalStateException: Can not create adaptive extens tion interface com.alibaba.dubbo.rpc.Protocol, cause: java.io.IOException: invalid constant type: 18 at com.alibaba.dubbo.common.extension.ExtensionLoader.getAdaptiveExtension(ExtensionLoader.java:459) at com.alibaba.dubbo.config.ServiceConfig.\u0026lt;clinit\u0026gt;(ServiceConfig.java:51) ... 28 common frames omitted 原因：缺少 javassist 或 javassist 版本太低。目前可用的版本是 javassist:javassist:3.27.0-GA。\nSpring Auto Configuration 常见排除：\nAn attempt was made to call a method that does not exist. The attempt was made from the following location: org.springframework.boot.autoconfigure.mongo.MongoPropertiesClientSettingsBuilderCustomizer.applyUuidRepresentation(MongoPropertiesClientSettingsBuilderCustomizer.java:58) The following method did not exist: \u0026#39;com.mongodb.MongoClientSettings$Builder com.mongodb.MongoClientSettings$Builder.uuidRepresentation(org.bson.UuidRepresentation)\u0026#39; The calling method\u0026#39;s class, org.springframework.boot.autoconfigure.mongo.MongoPropertiesClientSettingsBuilderCustomizer, was loaded from the following location: Spring 默认增加很多 Auto Configuration，使用 support 时可能触发 Auto Configuration 但又缺少配置，或者依赖版本与 Spring Boot 不匹配，可主动排除掉。\n@SpringBootApplication(exclude = {DataSourceAutoConfiguration.class, MongoDataAutoConfiguration.class})\r关注 Spring Boot 默认 Path 解析器变更，Spring Boot 2.6 版本以后默认由 ANT_PATH_MATCHER 变为 PATH_PATTERN_PARSER。\n双斜线 // 以前是可以匹配成功，目前版本会返回 404，比如 http://localhost:8080//actuator/health。\n默认禁用了后缀匹配，比如 GET /projects/spring-boot.json 将不能匹配到 @GetMapping(\u0026quot;/projects/spring-boot\u0026quot;)。\n据说，中文不主动进行 URLEncode 也会受影响，比如原来 http://localhost/卫星实验室 是能成功，目前也会 404。\nPATH_PATTERN_PARSER 只支持末尾 ** 匹配，不支持中间路径 ** 正则匹配，比如：/api/**/query 不支持。\n功能说明和切换方式请参考官方文档：https://docs.spring.io/spring-boot/docs/current/reference/html/web.html#web.servlet.spring-mvc.content-negotiation.\n参考资料 从 SpringMVC 迁移到 SpringBoot 的经验总结\nhttps://juejin.cn/post/6844903640361074696\nhttps://juejin.cn/post/6844903573453537294\nhttps://juejin.cn/post/7129751916002672654\n从 Java8 升级到 jdk17 的全过程记录\nhttps://juejin.cn/post/7258170075198259257\n从 JUnit 4 迁移到 JUnit 5\nhttps://zhuanlan.zhihu.com/p/144763642\n我服了！SpringBoot 升级后这服务我一个星期都没跑起来\nhttps://www.toutiao.com/article/7163602391366074916\nhttps://www.toutiao.com/article/7168780833636106760\nSpring Boot 2 到 Spring Boot 3 官方迁移指南\nhttps://github.com/spring-projects/spring-boot/wiki/Spring-Boot-3.0-Migration-Guide\nhttps://www.baeldung.com/spring-boot-3-migration\nSpring Boot 2.7.6 升级 3.1.0 爬坑指北\nhttps://juejin.cn/post/7237029359135408165\nSpring Boot 3.1 的新特性、升级说明以及核心功能的改进\nhttps://juejin.cn/post/7280787657013002301\nhttps://juejin.cn/post/7170907270631718920\nWhy is PostConstruct not called\nhttps://stackoverflow.com/questions/18161682/why-is-postconstruct-not-called\n","date":"2023-01-07","id":23,"permalink":"/blog/migrating-spring-to-spring-boot/","summary":"从 Spring 到 Spring Boot，迁移升级快速入门以及各种踩坑记录。\n概述 从 Spring 到 Spring Boot，整体开发、运行方式主要变化。\n- 当前（老）模式 新模式（本地开发） 新模式（线上运行） 开发习惯 Spring + 外置 Tomcat Spring Boot（embed tomcat） Spring Boot War or Jar Java 版本 8、11、16、17 11、17、21（推荐） 11、17、21（推荐） Tomcat 版本 8.x、9.x 9.x 9.x（推荐）、10.x 说明：\n理论上完全兼容 Java11，但是要求业务方尽量使用 Java17 或 21。其他版本都是实验性质尽量兼容。 线上运行支持 Spring Boot jar 直接运行，但主要业务仍推荐以 war + tomcat 为主。如果希望以 java -jar 方式运行，参考下面的章节“jar 方式运行”描述。 目前 Spring Boot 主要推行版本是 2.7.x。 3.x 版本逐渐适配中，注意 3.x 要求 Java 最低版本是 17。 快速开始 线下支撑系统导航，点击 脚手架 进入 spring start 页面，按自己需求选择模块，生成自己业务模式初始化代码。 写（Copy）业务代码到项目里，修改 pom.","tags":["Spring Boot","Java"],"title":"从 Spring 到 Spring Boot"},{"content":"Spring Boot 使用 Micrometer 集成 Prometheus 监控，5 分钟接入自定义监控指标，主要内容：\nMicrometer 介绍。 业务如何自定义指标，如何接入 Prometheus，实现方式和规范。 Micrometer 介绍 Micrometer 为 Java 平台上的性能数据收集提供了一个通用的 API，应用程序只需要使用 Micrometer 的通用 API 来收集性能指标，Micrometer 会负责完成与不同监控系统的适配工作。\nMicrometer 提供了多种度量指标类型（Timers、Guauges、Counters 等），同时支持接入不同的监控系统，例如 Influxdb、Graphite、Prometheus、OTLP 等。\n从 Spring Boot 2.x 开始使用 Micrometer 作为默认的监控门面接口， Think SLF4J, but for observability 。\nMicrometer 核心概念 Micrometer 中两个最核心的概念：计量器注册表 (MeterRegistry)，计量器 (Meter)。\nMeterRegistry\n内存注册表 (SimpleMeterRegistry): 在内存中保存每一个 Meter（指标）的最新值，并且不会将数据导出到任何地方。 组合注册表 (CompositeMeterRegistry): 可以添加多个注册表，用于将各个注册表组合起来，可以同时将指标发布到多个监控系统。Micrometer 提供了一个全局的 MeterRegistry，io.micrometer.core.instrument.Metrics 中持有一个静态 final 的 CompositeMeterRegistry 实例 globalRegistry。 普罗米修斯注册表 (PrometheusMeterRegistry): 当使用普罗米修斯监控时，引入 micrometer-registry-prometheus 依赖时会提供此种收集器，用于将指标数据转换为普罗米修斯识别的格式和导出数据等功能。 Meter（指标）\n监控数据的整个过程都是围绕着 Meter（指标）, 通过一个一个的 Meter（指标）数据来进行观察应用的状态。常用的指标如：\nCounter（计数器）: 单一计数指标，允许按固定数量递增，用来统计无上限数据。只允许递增。 Gauge（仪表盘）: 表示单个的变化的值，例如温度，气压。用于统计有上限可增可减的数据。在每次取样时，Gauge 会返回当前值。 Timer（计时器）: 通常用来记录事件的持续时间。Timer 会记录两类的数据，事件的数量和总的持续时间。 Tag（标签）\nMircrometer 通过 Tag（标签）实现了多维度的度量数据收集，通过 Tag 的命名可以推断出其指向的数据代表什么维度或是什么类型的度量指标。\n当前实现方式和要求 总体架构：Spring Boot Actuator + Micrometer + Prometheus + Granfana。\nSpring Boot Micrometer：提供监控门面 Api。 Spring Boot Actuator：提供监控指标采集服务，通过 /actuator/prometheus 获取数据。 Prometheus + Granfana：采集和存储数据，提供图表展示，根据告警规则发出告警。 总体实现步骤如下：\nSpring Boot Actuator 放开 prometheus http 访问，在配置文件中增加以下配置。\nmanagement.endpoint.prometheus.enabled=true management.endpoints.web.exposure.include=info,health,metrics,prometheus management.metrics.export.prometheus.enabled=true\r创建 Prometheus ServiceMonitor，从 /actuator/prometheus path 采集指标，如果涉及多个 war 合并部署到一个 tomcat 的，从多个 path 采集。\napiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: labels: app.kubernetes.io/component: metrics release: eye2 name: eye-consumer namespace: test spec: endpoints: - interval: 30s honorLabels: true path: /client-biz/actuator/prometheus port: metrics - interval: 30s honorLabels: true path: /gateway-biz/actuator/prometheus port: metrics jobLabel: eye-consumer selector: matchLabels: app: eye-consumer\r业务可通过 http://localhost:8080/actuator/metrics 查看指标是否已上报，通过 http://localhost:8080/actuator/prometheus 查看指标值。\n自定义 Metrics 指标 在 Spring Boot 中实现自定义指标非常简单，几种方式举例如下。\n像使用 slf4j 一样，使用 io.micrometer.core.instrument.Metrics 静态方式初始化一个指标，然后使用此指标直接操作。 使用 @Timed @Counted 注解。注意注解方式必须等方法调用后才能生成指标。而静态方法形式 io.micrometer.core.instrument.Metrics.counter立即就生成指标只是值为 0。另外注意 Spring 注解不支持 private、default 级别方法。 使用 Autowired MeterRegistry 创建自己的指标类型，适合一些动态 Tag 等高级定制场景。 import io.micrometer.core.annotation.Counted; import io.micrometer.core.instrument.Counter; import io.micrometer.core.instrument.Metrics; import org.springframework.stereotype.Service; @Service public class MicrometerSampleService { /** * 方式 1：像使用 slf4j 一样，使用 `io.micrometer.core.instrument.Metrics`静态方式初始化一个计数器，适用于名字和 Tag 固定的场景 */ private static final Counter failure = Metrics.counter(\u0026#34;fs.sms.send\u0026#34;, \u0026#34;result\u0026#34;, \u0026#34;failure\u0026#34;); @Autowired private MeterRegistry registry; private void sendSms() { try { // do something } catch (Exception e) { failure.increment(); } } /** * 方式 2：使用注解的方式，注意需要引入 spring-boot-starter-aop 依赖 */ @Counted(value = \u0026#34;fs.sms.send\u0026#34;, extraTags = {\u0026#34;provider\u0026#34;, \u0026#34;huawei\u0026#34;}) public void sendByHuawei() { this.sendSms(); } @Counted(value = \u0026#34;fs.sms.send\u0026#34;, extraTags = {\u0026#34;provider\u0026#34;, \u0026#34;ali\u0026#34;}) public void sendByAli() { this.sendSms(); } /** * 方式 3：使用 MeterRegistry，适合一些动态 Tag 等高级定制场景 * * @param result result */ public void countByResult(String result) { registry.counter(\u0026#34;fs.sms.send\u0026#34;, \u0026#34;result\u0026#34;, result).increment(); // or Counter.builder(\u0026#34;fs.sms.send\u0026#34;) .description(\u0026#34;send sms\u0026#34;) .tags(\u0026#34;result\u0026#34;, result) .register(registry) .increment(); } }\rSpring Boot 无法直接使用 @Timed @Counted 注解，需要引入切面支持，需要引入 spring-boot-starter-aop 依赖。\n@Configuration public class MicrometerAspectConfiguration { @Bean public CountedAspect countedAspect(MeterRegistry registry) { return new CountedAspect(registry); } @Bean public TimedAspect timedAspect(MeterRegistry registry) { return new TimedAspect(registry); } }\r为了方便大家使用，已经在我们的 starter 里自动注入了以上 Bean，大家只需要引入以下两个 starter。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.fxiaoke.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;metrics-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-aop\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt;\r自定义指标高级配置 Spring 默认注入的 MeterRegistry 是一个 CompositeMeterRegistry，如果想定制可注入自定义 MeterRegistryCustomizer Bean。\n@Configuration public class MicrometerConfiguration { @Bean MeterRegistryCustomizer\u0026lt;MeterRegistry\u0026gt; configurer() { return (registry) -\u0026gt; registry.config() .commonTags(\u0026#34;group\u0026#34;, \u0026#34;sample\u0026#34;) .commonTags(\u0026#34;application\u0026#34;, \u0026#34;sample\u0026#34;); } }\r如果只是想为当前应用增加 Tag，可直接通过配置文件增加，示例如下。\nmanagement.metrics.tags.biz=sample management.metrics.tags.application=${spring.application.name}\r自定义指标规范 指标和 Tag 命名约定使用英语句号分隔，全小写，Tag 可根据实际情况使用缩写。指标名在不同的 MeterRegistry 里会自动转换，比如在 Prometheus 会把 fs.sms.send 转换为 fs_sms_send。\n指标命名建议以 fs.application.action 为模板，避免与开源或其他项目组冲突。\n注意 Tag values 不能为 Null， 且必须是可枚举的某些固定类型便于统计。\n使用注解@Timed @Counted会默认增加 method、class、result、exception 这几个 Tag，注意不要与之冲突。\n公司和开源默认 Tag 如下，这些会被 ServiceMonitor 强制覆盖，业务不要自己定义。\nnamespace、application、service、container、pod、instance、job、endpoint、id 编码中如果需要 MeterRegistry，不允许引用具体实现（比如 Prometheus 的 io.prometheus.client.CollectorRegistry），而是使用 Micrometer 提供的统一接口 MeterRegistry。类比，在打印日志时不允许直接使用 logback 或 log4j api，而是使用 slf4j api.\n不要自己 new MeterRegistry，而是使用自动注入的或静态方法，因为我们可能随时在公司的 starter 增加自定义的配置。\n建议为指标加上 description 字段。\n最佳实践 合理规划 Tag，一个 Meter 具体类型需要通过名字和 Tag 作为它的唯一标识，这样做的好处是可以使用名字进行标记，通过不同的 Tag 去区分多种维度进行数据统计。\n反例 1（全部用 name 区分，无 Tag，重复计量）： Metrics.counter(\u0026#34;fs.sms.all\u0026#34;); Metrics.counter(\u0026#34;fs.sms.aliyun\u0026#34;); Metrics.counter(\u0026#34;fs.sms.huaweiyun\u0026#34;); 正例： Metrics.counter(\u0026#34;fs.sms.send\u0026#34;,\u0026#34;provider\u0026#34;,\u0026#34;ali\u0026#34;); Metrics.counter(\u0026#34;fs.sms.send\u0026#34;,\u0026#34;provider\u0026#34;,\u0026#34;huawei\u0026#34;); 避免无意义不可枚举的 Tag，混乱的 Tag 比无 Tag 更难管理。\n注意事项 注意引入的类名，有很多同名的类，使用 io.micrometer.core 包下的类。 ","date":"2023-08-07","id":24,"permalink":"/blog/spring-boot-micrometer/","summary":"Spring Boot 使用 Micrometer 集成 Prometheus 监控，5 分钟接入自定义监控指标，主要内容：\nMicrometer 介绍。 业务如何自定义指标，如何接入 Prometheus，实现方式和规范。 Micrometer 介绍 Micrometer 为 Java 平台上的性能数据收集提供了一个通用的 API，应用程序只需要使用 Micrometer 的通用 API 来收集性能指标，Micrometer 会负责完成与不同监控系统的适配工作。\nMicrometer 提供了多种度量指标类型（Timers、Guauges、Counters 等），同时支持接入不同的监控系统，例如 Influxdb、Graphite、Prometheus、OTLP 等。\n从 Spring Boot 2.x 开始使用 Micrometer 作为默认的监控门面接口， Think SLF4J, but for observability 。\nMicrometer 核心概念 Micrometer 中两个最核心的概念：计量器注册表 (MeterRegistry)，计量器 (Meter)。\nMeterRegistry\n内存注册表 (SimpleMeterRegistry): 在内存中保存每一个 Meter（指标）的最新值，并且不会将数据导出到任何地方。 组合注册表 (CompositeMeterRegistry): 可以添加多个注册表，用于将各个注册表组合起来，可以同时将指标发布到多个监控系统。Micrometer 提供了一个全局的 MeterRegistry，io.micrometer.core.instrument.Metrics 中持有一个静态 final 的 CompositeMeterRegistry 实例 globalRegistry。 普罗米修斯注册表 (PrometheusMeterRegistry): 当使用普罗米修斯监控时，引入 micrometer-registry-prometheus 依赖时会提供此种收集器，用于将指标数据转换为普罗米修斯识别的格式和导出数据等功能。 Meter（指标）","tags":["Spring Boot","Java"],"title":"Spring Boot 自定义监控指标"},{"content":"基于 Envoy + Java Agent 的智能路由服务实现方案介绍。\n核心需求 服务自动注册和发现，通过 Service Name 直接调用服务。当然基本的负载均衡策略、熔断降级限流等功能也要支持。 公司约定的路由策略，支持按照租户路由到特定环境的服务，比如 VIP、Gray、Sandbox 等。 多集群通信，同云内新老 K8S 集群路由打通，可通过 POD IP 互相通信。 跨云通信，支持通过 VPN 或代理，从专属云访问公有云服务。 整体架构 智能路由服务从逻辑上分为数据平面和控制平面，主要包含以下组件。\nNacos：服务注册中心，配置中心。 XDS Server：对接服务注册中心、配置中心，实现 CDS、LDS、RDS 协议将集群、服务、路由、灰度租户等配置下发到 Envoy。 Envoy + WASM Plugin：通过 Envoy 代理流量，自定义 WASM 插件实现按照租户、用户路由到不同服务，实现自定义负载均衡策略。 Java Agent：增强 Java 应用 Http Client，拦截 OkHttp、Apache Http Client、RestTemplate、OpenFeign 等客户端调用，将流量重定向到 Envoy，Envoy 再根据服务名路由到真实的 Pod，实现服务发现和灾备切换。 Nacos Service CRD：自定义 Nacos Service CRD，将 Service 注册到 Nacos 中作为一个永久实例，解决跨云、跨集群服务调用。比如跨云情况下注册的是一个公网地址或 VPN 可通的地址。 C4Context title 基于 Envoy + Java Agent 的智能路由服务 Enterprise_Boundary(dp, \"Data Plane\") { Container(appA, \"Application A\", \"Java,Agent\", \"Agent 拦截客户端重定向到 Envoy\") Container(envoy, \"Envoy Proxy\", \"Envoy,WASM\", \"代理所有入口流量\n基于租户、服务负载均衡\") Container(appB, \"Application B\", \"Java,Agent\", \"应用注册到服务中心\") Rel(appA, envoy, \"request by name\", \"http\") Rel(envoy, appB, \"http proxy \u0026 lb\", \"http\") UpdateRelStyle(appA, envoy, $offsetX=\"-40\",$offsetY=\"-40\") UpdateRelStyle(envoy, appB, $offsetX=\"-40\",$offsetY=\"-40\") } Enterprise_Boundary(cp, \"Control Plane\") { System(registry, \"服务注册\", \"服务注册，服务元数据\n配置管理，双向刷新\") Container(xdsServer, \"控制面板\", \"Java,Grpc\", \"对接服务注册中心配置中心\n实现 XDS 协议将配置下发到 Envoy\") System(config, \"配置管理\", \"服务注册，服务元数据\n配置管理，双向刷新\") Rel_L(xdsServer, registry, \"实时获取服务实例\") Rel_R(xdsServer, config, \"实时配置刷新\") UpdateRelStyle(xdsServer, registry, $offsetX=\"-40\",$offsetY=\"-20\") UpdateRelStyle(xdsServer, config, $offsetX=\"-40\",$offsetY=\"-20\") } Rel_U(xdsServer,envoy,\"通过 XDS 实现配置动态下发\",\"grpc\") Rel_D(appA, registry, \"服务注册，配置刷新\") Rel_D(appB, config, \"服务注册，配置刷新\") UpdateLayoutConfig($c4ShapeInRow=\"3\", $c4BoundaryInRow=\"1\") 方案对比 此方案原始的目标有几个：\n语言无关，可以不绑定 Java。目前看来这是个伪需求。 实现类似 Istio Ambient Mesh，剔除 Egress 流量代理，每个 Node 部署一个 Envoy Proxy，减少 Sidecar 资源开销。目前随着 Istio Ambient Mesh 成熟，这个优点反而也没有了。 对比 K8S 体系、Spring Cloud 体系，此方案优点：\n各服务实际上只需要注入服务中心，服务发现和负载均衡交给 Agent 和 Envoy，对于老的 Spring 项目也能直接使用，相对 Spring Cloud 轻量一些。 不需要引入 Istio 也能支持跨云、跨集群访问场景，Istio 相对还是比较重。 此方案缺点：\n本质上还是 Proxy 模式，Proxy 的所有缺点他都绕不过，比如：性能损耗、Http Keepalive 和 reset 响应。 对服务和配置实时刷新，要比 K8S、Spring Cloud 略微延迟，对高并发场景难免有中断。 单点故障影响范围太大，以及 Proxy 任何配置性错误都是致命的。 综上，随着 Istio Ambient Mesh 以及基于 eBPF 的 Cilium Mesh 的成熟，这个方案可能逐渐退出历史舞台。那么，使用 Spring Cloud 全家桶快速启动先让业务用起来，反而成了一个最简单的方案。\n功能实现方案 主要功能实现方式如下。\n服务注册 服务通过 nacos support 或 nacos starter 注册到服务中心，关注下 Nacos 数据模型：\nnamespace - group - service - cluster - instance: ip,port,metadata\rnamespace 和 group 的设计目的是为了实现隔离。\n同 service 下 instance 唯一标记：ip + port + cluster。metadata 不作为唯一性标记。 同 namespace 不同 group 下 service 可以完全相同。 Nacos 模型 开源原生默认值 我们方案 namespace 租户/环境 ，默认 public 固定自定义值 group 组织/虚拟分组，默认 DEFAULT 默认 DEFAULT，如果某个组确认自己的服务不给组用可自定义 group service spring.application.name 默认也是 spring.application.name，子模块名 cluster 虚拟，a k8s cluster/ a AZ 线上发布系统的“环境”英文名，实际是 k8s 的 namespace instance 一个 Java 应用，一个 jar 或一个 war 服务 一个 jar 或一个 war 服务 注册实例信息约束：\nserviceName: appName.modeleName 应用的子模块名，目前的字符和长度限制都能满足要求 clusterName: 发布系统定义的“环境”英文名 ip: pod ip port: container port enabled： true or false 用于主动控制上下线，下线保留实例不承载流量 weight：权重 metadata： - contextPath： 推荐等同子模块名，要求必须有且不能随意变更，可以是根目录 - preserved.register.source: 官方自动加，如 Spring Cloud、Dubbo。 - secure：官方开源客户端自动加 boolean 值，标记 http or https 协议 - protocol：通信协议，http/grpc/dubbo - version：版本标记，后续扩展路由策略支持多版本或限定版本。 - runtime：k8s info、node info。 对于专属云访问公有云，目前是通过 VPN 代理方式，那么在专属云访问公有云时，如何通过 Service 名字映射到公有云服务呢。 我们会在专属云内，手动（优化为自定义 K8S CRD，联动 GitOps 自动部署）在 Nacos 中增加一个永久性实例（关于永久性实例和临时实例的说明请参考官方文档），只是这个实例的 IP 不是 POD IP，是通过 VPN 可通的 IP。\n服务发现 调用方可使用 OkHttp、RestTemplate、OpenFeign 等客户端，直接使用服务名调用，比如：GET http://user-service/users。\n服务发现和负载均衡策略，其实依赖三个组件：XDS Server、Envoy + WASM、Java Agent。\n基本实现步骤：\nXDS Server 从 Nacos 实时获取最新的服务和实例列表，从配置中心获取服务灰度列表，通过 ADS 协议实时下发到 Envoy。 我们为 Envoy 定制了一个 WASM Plugin，这个 Plugin 支持通过租户路由到 Gray、VIP、Sandbox 等不同环境，Envoy 通过环境标记负载到真实的 POD 上。 Java Agent 增强 OkHttp 增加自定义 intercept，拦截服务名重定向到 Envoy，Envoy 根据服务名、路径、Header 路由到真实的后端服务。 我们的 RestTemplate、OpenFeign 客户端做了定制，底层连接器都切换到了 OkHttp。 客户端调用时，Header 中增加租户信息。 客户端调用时不需要关心服务的 contextPath，Envoy 会自动追加 contextPath。 路由和负载均衡规则 运行环境定义： Gray: foneshare-gray 用于常规发布的灰度过程；承载 QA 专用在线测试租户 Stage: foneshare-stage 用于大版本发布的灰度过程，承载 QA 专用在线测试租户、必要的真实租户 Urgent: foneshare-urgent 用于解决线上 vip、svip 面临的紧急 bug, 承载 bug 相关的 vip、svip 租户 Normal: foneshare、foneshare01、foneshare02、foneshare03、foneshare04 承载 vip 之外的租户 VIP: foneshare-vip 承载 vip 的租户 SVIP: 一个 foneshare-svip 承载多个 svip 租户 SSVIP: 一个环境承担一个租户，如 foneshare-yqsl 承载 ssvip 租户 Sandbox：所有以 _sandbox 结尾的租户都会路由到 Sandbox 环境。\n环境路由优先级从高到低：\nSSVIP \u0026gt; Gray \u0026gt; Urgent \u0026gt; Stage \u0026gt; SVIP \u0026gt; VIP \u0026gt; Sandbox \u0026gt; Normal\n对于同一租户，既配置在 vip 的灰度名单，同时又配置到 gray 的灰度名单，实际路由时走哪个环境：\nP1，最高优先级，gray、urgent、yqsl 等专属环境，tenantId 匹配任意环境就会返回； P2，第 2 优先级，stage； P3，第 3 优先级，svip； P4，第 4 优先级，vip； P5，第 5 优先级，sandbox； P6，默认 normal，上述环境均未匹配到后，路由到 normal 环境。\n按照上述顺序的原则，如一家租户既配置在 vip 同时又配置在 gray，实际会路由到 gray 环境。 注意：\n这里路由到 gray是指 gray 中存在对应的服务，如果 gray 中不存在服务，会按匹配到的优先级（vip）继续找，直到找到可用的服务。 命中条件：租户在这个环境的这个服务名单里（租户+服务唯一确定）。 沙箱用户比较特殊，是 EA 以 _sandbox 结尾。 Envoy 配置下发和更新 Envoy 通过文件系统或查询管理服务器发现其各种动态资源。这些发现服务及其相应的 API 统称为 xDS。\n我们的控制面 XDS Server 只实现了聚合发现服务（ADS），通过 ADS 使用单个流推送 CDS、EDS 和 RDS 资源，实现无中断的更新。\nXDS Server 缓存了各个 Envoy 的 grpc connection，同时响应 Nacos 和配置中心的更新事件，收到更新后再通过 grpc 推送到各个 Envoy 终端。\n使用限制 请求 Header 必须要包含 x-fs-ea 或 x-fs-ei 参数（推荐 x-fs-ea），否则无法支持按租户路由。 同一个服务在同一个环境，发布流程中的 contextPath 必须一致，因为是按环境类型统一路由只能有一个 contextPath。 配置文件变更需要 30s 后才能生效，这取决了 CMS 配置系统拉取方式。 新增加服务无法立即感知到（是新的服务，不是服务实例，实例能即时感知），我们是 10 分钟扫描一次服务，判断有无服务变更。 Nacos 服务端部署 部署形态：\n公有云一套，承接公有云新老集群。 每个专属云有独立的一套。 注意事项：\nNacos 需要放开 http（8848）和 grcp（9848、9849）端口。 认证：使用用户名密码模式。不同组创建独立的用户名密码，单独授权。默认的用户nacos不要用且权限不足。 连接时配置的 namespace 实际是 namespace id，可以考虑创建 namespace 时主动设置 namespace id 等同于 namespace name，便于定位，误删或迁移的时候能保证 ID 不变。 ","date":"2023-08-07","id":25,"permalink":"/docs/cloud/service-mesh-envoy/","summary":"基于 Envoy + Java Agent 的智能路由服务实现方案介绍。\n核心需求 服务自动注册和发现，通过 Service Name 直接调用服务。当然基本的负载均衡策略、熔断降级限流等功能也要支持。 公司约定的路由策略，支持按照租户路由到特定环境的服务，比如 VIP、Gray、Sandbox 等。 多集群通信，同云内新老 K8S 集群路由打通，可通过 POD IP 互相通信。 跨云通信，支持通过 VPN 或代理，从专属云访问公有云服务。 整体架构 智能路由服务从逻辑上分为数据平面和控制平面，主要包含以下组件。\nNacos：服务注册中心，配置中心。 XDS Server：对接服务注册中心、配置中心，实现 CDS、LDS、RDS 协议将集群、服务、路由、灰度租户等配置下发到 Envoy。 Envoy + WASM Plugin：通过 Envoy 代理流量，自定义 WASM 插件实现按照租户、用户路由到不同服务，实现自定义负载均衡策略。 Java Agent：增强 Java 应用 Http Client，拦截 OkHttp、Apache Http Client、RestTemplate、OpenFeign 等客户端调用，将流量重定向到 Envoy，Envoy 再根据服务名路由到真实的 Pod，实现服务发现和灾备切换。 Nacos Service CRD：自定义 Nacos Service CRD，将 Service 注册到 Nacos 中作为一个永久实例，解决跨云、跨集群服务调用。比如跨云情况下注册的是一个公网地址或 VPN 可通的地址。 C4Context title 基于 Envoy + Java Agent 的智能路由服务 Enterprise_Boundary(dp, \"","tags":["Spring Boot","Java","Envoy"],"title":"基于 Envoy 的智能路由服务"},{"content":"程序员的瑞士军刀，常用优秀命令行工具，上榜都是有理由的。\nprocs 比 ps 好用的工具，查看路径和端口一步到位，支持过滤。\nripgrep ripgrep 简称 rg，是一个面向行的搜索工具，Rust 编写，全平台支持，也是 VS Code 的默认搜索工具。它的搜索性能极高，在大项目中也有着出色的表现，并且默认可以忽略 .gitignore 文件中的内容，非常实用。\n除了作为一个高效的命令行工具使用外，整个项目的设计也不错，另外还是一个学习 Rust 的好项目。\nrg -h 开启探索之旅吧。\nTL;DR Too Long; Didn’t Read.\ntldr 根据二八原则，简化了烦琐的 man 指令帮助文档，仅列出常用的该指令的使用方法，让人一看就懂，大多数情况下，给出几个指令的使用 demo 可能正是我们想要的。\n举个例子看下实际运行效果，如下（太长，节选）。\n➜ ~ tldr docker List all docker containers (running and stopped): docker ps -a Start a container from an image, with a custom name: docker run --name container_name image Start or stop an existing container: docker start|stop container_name tldr 命令行有多种实现，比如官方推荐的有 npm 和 python。\n个人更喜欢 Rust 版本的实现 tealdeer，支持各系统包管理器和二进制安装，比如 homebrew。\nbrew install tealdeer\rfd fd，find 的替代品。\nbottom bottom，类似 Top 的酷炫系统监控工具，Inspired by gtop, gotop, and htop。\nbat bat，一只带翅膀的 cat，代替 cat 命令。\n","date":"2024-03-09","id":26,"permalink":"/docs/tldr/knife/","summary":"程序员的瑞士军刀，常用优秀命令行工具，上榜都是有理由的。\nprocs 比 ps 好用的工具，查看路径和端口一步到位，支持过滤。\nripgrep ripgrep 简称 rg，是一个面向行的搜索工具，Rust 编写，全平台支持，也是 VS Code 的默认搜索工具。它的搜索性能极高，在大项目中也有着出色的表现，并且默认可以忽略 .gitignore 文件中的内容，非常实用。\n除了作为一个高效的命令行工具使用外，整个项目的设计也不错，另外还是一个学习 Rust 的好项目。\nrg -h 开启探索之旅吧。\nTL;DR Too Long; Didn’t Read.\ntldr 根据二八原则，简化了烦琐的 man 指令帮助文档，仅列出常用的该指令的使用方法，让人一看就懂，大多数情况下，给出几个指令的使用 demo 可能正是我们想要的。\n举个例子看下实际运行效果，如下（太长，节选）。\n➜ ~ tldr docker List all docker containers (running and stopped): docker ps -a Start a container from an image, with a custom name: docker run --name container_name image Start or stop an existing container: docker start|stop container_name tldr 命令行有多种实现，比如官方推荐的有 npm 和 python。","tags":[],"title":"Knife"},{"content":"Linux Shell 日常编程实用句式速查，注意以下例子如无特殊说明都是 bash 语法。\n选择执行环境。 #!/usr/bin/env bash #!/usr/bin/bash\r指定变量默认值，取环境变量，取不到用默认值。 rep=${ENV_YOUR_KEY:-\u0026#34;my-default-value\u0026#34;}\r获取当前执行脚本的绝对路径，注意直接用 $0 或 pwd 获取的可能都不要你想要的。 current_dir=$(cd `dirname $0`;pwd)\r为当前目录包含子目录下所有 .sh 文件增加可执行权限。 chmod +x `find . -name \u0026#39;*.sh\u0026#39;`\r通过 tee 将提示信息显示到终端，并同时写入到文件。 log_file=/var/log/test.log echo \u0026#34;This line will echo to console and also write to log file.\u0026#34; | tee -a ${log_file}\r类似于 Java properties 中 key=value 形式的字符串，取 key 和 value 的值。 username_line=\u0026#34;username=test\u0026#34; #key is username key=${username_line%=*} #val is test val=${username_line#*=}\r实现 String trim 效果。 #trim string by echo val_trim=$(echo -n ${val})\r声明和循环数组。 apps=(foo bar) for app in ${apps[@]} do echo \u0026#34;$app\u0026#34; done\r文件 ls 转数组 # ls 转数组，根据需要 grep arrs=($(ls helmfiles/apps | grep -v .yaml))\r指定数组分割符，字符串转数组。 # 获取当前 helm list 命令输出结果，通过换行分割成数组 IFS=$\u0026#39;\\n\u0026#39; helm_list=($(helm list --no-headers)) for hm in ${helm_list[@]} do # 对每一行进行解析按 Tab 再分割成数组 IFS=$\u0026#39;\\t\u0026#39; hma=($hm) echo \u0026#34;helm upgrade --install ${hma[0]} ${hma[0]} --version ${hma[6]} --reset-values 2\u0026gt;\u0026amp;1\u0026#34; done\r文件按内容排序。 log=my-log-file # 原地排序覆盖原文件 sort -o ${log} ${log}\r判断字符串是否以某串开头，并去除指定前缀。 img=docker.io/nginx:1.20 repo=docker.io # if 判断字符串是否以 repo 开头 if [[ $img == $repo* ]]; then # 注意这里 +2 suffix=$(echo $img | cut -c$((${#repo}+2))-) # 输出 nginx:1.20 echo \u0026#34;$suffix\u0026#34; fi\r按多个关键词 或 过滤。 echo \u0026#34;nginx\u0026#34; | grep -E \u0026#34;nginx|tomcat\u0026#34; echo \u0026#34;tomcat\u0026#34; | grep -E \u0026#34;nginx|tomcat\u0026#34; echo \u0026#34;envoy\u0026#34; | grep -E \u0026#34;nginx|tomcat\u0026#34;\r获取当前时间，加减时区。 ct=$(TZ=UTC+8 date \u0026#34;+%Y%m%d%H%M\u0026#34;)\r检查是否以 root 用户执行。 # check if run as root user if [[ `id -u` -ne 0 ]]; then echo \u0026#34;You need root privileges to run this script.\u0026#34; fi\r","date":"2024-03-09","id":27,"permalink":"/docs/tldr/shell-coding/","summary":"Linux Shell 日常编程实用句式速查，注意以下例子如无特殊说明都是 bash 语法。\n选择执行环境。 #!/usr/bin/env bash #!/usr/bin/bash\r指定变量默认值，取环境变量，取不到用默认值。 rep=${ENV_YOUR_KEY:-\u0026#34;my-default-value\u0026#34;}\r获取当前执行脚本的绝对路径，注意直接用 $0 或 pwd 获取的可能都不要你想要的。 current_dir=$(cd `dirname $0`;pwd)\r为当前目录包含子目录下所有 .sh 文件增加可执行权限。 chmod +x `find . -name \u0026#39;*.sh\u0026#39;`\r通过 tee 将提示信息显示到终端，并同时写入到文件。 log_file=/var/log/test.log echo \u0026#34;This line will echo to console and also write to log file.\u0026#34; | tee -a ${log_file}\r类似于 Java properties 中 key=value 形式的字符串，取 key 和 value 的值。 username_line=\u0026#34;username=test\u0026#34; #key is username key=${username_line%=*} #val is test val=${username_line#*=}\r实现 String trim 效果。 #trim string by echo val_trim=$(echo -n ${val})\r声明和循环数组。 apps=(foo bar) for app in ${apps[@]} do echo \u0026#34;$app\u0026#34; done\r文件 ls 转数组 # ls 转数组，根据需要 grep arrs=($(ls helmfiles/apps | grep -v .","tags":[],"title":"Shell Coding"},{"content":"使用 backstage 创建开发者门户。\n","date":"2024-01-27","id":28,"permalink":"/docs/platform/backstage/","summary":"使用 backstage 创建开发者门户。","tags":[],"title":"开发者门户"},{"content":"作为一个以 Java 和 Spring 为主要技术栈的团队，在日常的软件开发中，我们经常会遇到一系列的组件升级和代码重构需求，在此过程我们期望能做到几个效果：\n项目级升级：整个项目（注此项目指 Maven Project）升级，而不是基于某个 Java 类或片段。 不同版本跨度变更：比如从 Spring 项目迁移到 Spring Boot，从 Spring Boot 2.x 升级到 3.x，从 Java 8 升级到 Java 21。 代码安全可靠：变更的代码一定是正确的，至少是逻辑正确的，至少不能像某 AI 助手一样设置一些根本不存在的属性。 经验产品化：最佳实践就是产品，比如我并不（想）了解 Spring Boot 3.2 具体有哪些变更，但希望能一键从 3.0 自动升级到 3.2，直接告诉有哪些变更。 自定义重构：对于某些自研代码，希望能自定义重构逻辑，一键自动重构。 基于以上背景，我们探索了 OpenRewrite、Spring Boot Migrator、Redhat Windup 和一众不便具名的 AI 代码助手，本文将分享我们使用 OpenRewrite、Spring Boot Migrator 进行代码重构和升级的一些使用经验和体验。\nOpenRewrite OpenRewrite 是一个开源的代码重写工具，旨在帮助开发人员自动化大规模重构代码。\n它提供了一套强大的 API 和插件系统，可以通过静态分析和代码转换技术来解析、修改和生成代码。OpenRewrite 支持多种编程语言，包括 Java、C#、 TypeScript、Python、Kubernetes 等。通过使用 OpenRewrite，开发人员可以轻松地进行代码重构、性能优化、代码风格调整和代码迁移等操作，从而提高代码质量和可维护性。OpenRewrite 的开源性质使得开发人员可以自由地定制和扩展其功能，以满足特定项目的需求。\n比如 Java 领域一些热门的应用场景：\nJava 版本升级：从 Java 8 到 Java 17，从 Java EE 到 Jakarta EE 。 Spring 框架迁移：从 Spring 5 到 Spring 6，从 Spring Boot 2 到 Spring Boot 3。 测试框架迁移： 从 Junit 4 到 Junit 5。 依赖管理：自动更新 Java 项目的 Maven 或 Gradle 依赖，确保使用最新和最安全的库版本。 代码清理和格式化：自动清理和格式化 Java 代码，确保符合项目或组织的编码标准和风格指南。 修复安全漏洞：自动识别和修复 Java 代码中的已知安全漏洞，如使用了有安全问题的库或方法。 代码异味检测和修复：识别并自动重构 Java 代码中的“代码异味”，以提高代码可维护性。 相比于时下火热的 AI 代码工具，比如 GitHub Copilot、Amazon CodeWhisperer，我认为 OpenRewrite 的优势主要有以下几点：\n可以直接处理大型项目，截止我写此文的今天还没一个成熟的 AI 助手能将整个项目一次性迁移或重构。 准确度较高，它通过预定义的规则来实现代码的精确修改，目前很多 AI 代码助手还在一本正经的瞎扯，尤其是凭空创造让人防不胜防。 可自定义扩展，对于自研代码不可能经过自动学习达到精确的效果，此时可自定义实现。 免费，可离线使用，便于 CI 集成。 值得一提的是，AWS 最近推出了 Amazon Q Code Transformation 专门做代码迁移和重构，需要开通 Amazon CodeWhisperer 企业版才能使用，可惜我没有钱去试用。\n快速入门 OpenRewrite 一个最核心的概念是 Recipe，由于我是国内首个翻译这个单词的人，以下将直译为食谱。一个 Recipe 可以理解为是一套声明好的规则，OpenRewrite 按照固定规则进行重构。\n一个 Recipe 可以包含多个 Recipe，可以层级累加，比如 UpgradeSpringBoot_3_2可能包含对 pom.xml、application.properties、java main source、java test source 的升级改动。\nOpenRewrite 支持使用 maven、gradle 以及使用 SaaS 服务 Moderne cli mod 这三种方法，maven、gradle 定制比较方便，dependency 管理也比较灵活，另外某些食谱只有 maven、gradle 支持，下面的例子都是使用 maven 执行。\n初次使用 OpenRewrite，可通过官方提供的 热门指导 找到自己感兴趣的按步骤操作。\n更多功能可通过 食谱分类 搜索，也可通过官网右上方的搜索按钮按照关键字搜索，也可通过 mvn rewrite:discover 列出可用的食谱。\n最佳实践和定制 OpenRewrite 对不同类型分别内置了一些最佳实践，可通过 best practices 关键字搜索。\n最佳实践包含了哪些规则呢，以 Spring Boot 3.x best practices 为例，在文档中能看到对应的食谱列表如下：\n--- type: specs.openrewrite.org/v1beta/recipe name: org.openrewrite.java.spring.boot3.SpringBoot3BestPractices displayName: Spring Boot 3.x best practices description: Applies best practices to Spring Boot 3 applications. tags: - spring - boot recipeList: - org.openrewrite.java.spring.boot2.SpringBoot2BestPractices - org.openrewrite.java.migrate.UpgradeToJava21 - org.openrewrite.java.spring.boot3.UpgradeSpringBoot_3_2\r而其中每个食谱又可能包含多个，比如上面的 org.openrewrite.java.spring.boot2.SpringBoot2BestPractices 又包含以下列表：\n--- type: specs.openrewrite.org/v1beta/recipe name: org.openrewrite.java.spring.boot2.SpringBoot2BestPractices displayName: Spring Boot 2.x best practices description: Applies best practices to Spring Boot 2 applications. tags: - spring - boot recipeList: - org.openrewrite.java.spring.NoRequestMappingAnnotation - org.openrewrite.java.spring.ImplicitWebAnnotationNames - org.openrewrite.java.spring.boot2.UnnecessarySpringExtension - org.openrewrite.java.spring.NoAutowiredOnConstructor - org.openrewrite.java.spring.boot2.RestTemplateBuilderRequestFactory - org.openrewrite.java.spring.boot2.ReplaceDeprecatedEnvironmentTestUtils\r如果官方的某个食谱不和你的胃口，比如想在最佳实践中去掉或新增某个自定义食谱怎么办，可以自己定义一个 rewrite.yml 文件，类似上面的 yaml 文件，在文件中自行组合食谱。注意 rewrite.yml 文件要放到 maven 项目的根目录下，更多关于文件的说明参考 Refactoring with declarative YAML recipes。\nCI 集成 通过 mvn rewrite:run 命令执行后，会直接修改代码。但是大部分 CI 场景下，我们可能只想做分析，不想真正改动代码。那么就可以把命令换成 mvn rewrite:dryRun，dryRun 会在控制台输出 warning 结果，并生成一个 rewrite.patch 文件。\n如果 dryRun 向控制台日志发出任何警告，或者生成了 rewrite.patch 文件，证明此次有需要变更的地方，则可视为 CI 失败。\nSpring Boot Migrator OpenRewrite 功能很强悍，但是仍缺失一项功能：将传统 Spring 项目一键迁移到 Spring Boot。\nSpring Boot Migrator 基于 OpenRewrite，是一个专门用于协助将传统的 Spring 应用迁移到基于 Spring Boot 的工具，同时也提供 Spring Boot 的升级。\nSpring Boot Migrator 程序运行起来后，通过 list 命令可以看到提供的 OpenRewrite recipes，其中主要关注的有：\ninitialize-spring-boot-migration: 将项目初始化为 Spring Boot 应用。 migrate-spring-xml-to-java-config：将 Spring xml 转换为 Java 注解。 spring-context-xml-import：使用 Import xml 文件的形式初始化 Bean。 boot-2.7-3.0-dependency-version-update：Spring Boot 2 升级到 Spring Boot 3。 migrator:\u0026gt; list Found these recipes: 1) remove-redundant-maven-compiler-plugin -\u0026gt; Remove standard maven-compiler plugin for applications with boot parent. 2) initialize-spring-boot-migration -\u0026gt; Initialize an application as Spring Boot application. 3) migrate-jndi-lookup -\u0026gt; Migrate JNDI lookup using InitialContext to Spring Boot 4) migrate-jpa-to-spring-boot -\u0026gt; Migrate JPA to Spring Boot 5) migrate-ejb-jar-deployment-descriptor -\u0026gt; Add or overrides @Stateless annotation as defined in ejb deployment descriptor 6) migrate-weblogic-ejb-deployment-descriptor -\u0026gt; Migrate weblogic-ejb-jar.xml deployment descriptor 7) mark-and-clean-remote-ejbs -\u0026gt; Search @Stateless EJBs implementing a @Remote interface 8) migrate-stateless-ejb -\u0026gt; Migration of stateless EJB to Spring components. 9) migrate-annotated-servlets -\u0026gt; Allow Spring Boot to deploy servlets annotated with @WebServlet 10) migrate-jax-ws -\u0026gt; Migrate Jax Web-Service implementation to Spring Boot bases Web-Service 11) migrate-jax-rs -\u0026gt; Any class has import starting with javax.ws.rs 12) migrate-mule-to-boot -\u0026gt; Migrate Mulesoft 3.9 to Spring Boot. 13) migrate-tx-to-spring-boot -\u0026gt; Migration of @TransactionAttribute to @Transactionsl 14) spring-context-xml-import -\u0026gt; Import Spring Framework xml bean configuration into Java configuration without converting them. 15) migrate-spring-xml-to-java-config -\u0026gt; Migrate Spring Framework xml bean configuration to Java configuration. 16) migrate-jms -\u0026gt; Convert JEE JMS app into Spring Boot JMS app 17) documentation-actions -\u0026gt; Create Documentation for Actions 18) migrate-jsf-2.x-to-spring-boot -\u0026gt; Use joinfaces to integrate JSF 2.x with Spring Boot. 19) cn-spring-cloud-config-server -\u0026gt; Externalize properties to Spring Cloud Config Server 20) boot-2.4-2.5-upgrade-report -\u0026gt; Create Upgrade Report for a Spring Boot 2.4 Application 21) boot-2.7-3.0-dependency-version-update -\u0026gt; Bump spring-boot-starter-parent from 2.7.x to 3.0.0 22) boot-autoconfiguration-update -\u0026gt; Create org.springframework.boot.autoconfigure.AutoConfiguration.imports file for new spring 2.7 23) boot-2.4-2.5-datasource-initializer -\u0026gt; Replace deprecated spring.datasource.* properties 24) boot-2.4-2.5-spring-data-jpa -\u0026gt; Rename JpaRepository methods getId() and calls to getOne() 25) boot-2.4-2.5-dependency-version-update -\u0026gt; Update Spring Boot dependencies from 2.4 to 2.5 26) boot-2.7-3.0-upgrade-report -\u0026gt; Create a report for Spring Boot Upgrade from 2.7.x to 3.0.0-M3 27) boot-2.4-2.5-sql-init-properties -\u0026gt; Replace deprecated spring.datasource.* properties 28) sbu30-report -\u0026gt; Create a report for Spring Boot Upgrade from 2.7.x to 3.0.x 29) sbu30-upgrade-dependencies -\u0026gt; Spring boot 3.0 Upgrade - Upgrade dependencies 30) sbu30-set-java-version -\u0026gt; Spring boot 3.0 Upgrade - Set java version property in build file 31) sbu30-add-milestone-repositories -\u0026gt; Spring boot 3.0 Upgrade - Add milestone repository for dependencies and plugins 32) sbu30-migrate-spring-data-properties -\u0026gt; Spring boot 3.0 Upgrade - Migrate \u0026#39;spring.data\u0026#39; properties to new property names 33) sbu30-remove-construtor-binding -\u0026gt; Spring boot 3.0 Upgrade - Remove redundant @ConstructorBinding annotations 34) sbu30-migrate-to-jakarta-packages -\u0026gt; Spring boot 3.0 Upgrade - Migrate javax packages to new jakarta packages 35) sbu30-johnzon-dependency-update -\u0026gt; Spring boot 3.0 Upgrade - Specify version number for johnzon-core 36) sbu30-225-logging-date-format -\u0026gt; Spring boot 3.0 Upgrade - Logging Date Format 37) sbu30-auto-configuration -\u0026gt; Move EnableAutoConfiguration Property from spring.factories to AutoConfiguration.imports 38) sbu30-upgrade-spring-cloud-dependency -\u0026gt; Upgrade Spring Cloud Dependencies 39) sbu30-upgrade-boot-version -\u0026gt; Spring boot 3.0 Upgrade - Upgrade Spring Boot version 40) sbu30-remove-image-banner -\u0026gt; Spring boot 3.0 Upgrade - Remove the image banner at src/main/resources 41) sbu30-paging-and-sorting-repository -\u0026gt; Spring boot 3.0 Upgrade - Add CrudRepository interface extension additionally to PagingAndSortingRepository 42) migrate-raml-to-spring-mvc -\u0026gt; Create Spring Boot @RestController from .raml files. 43) migrate-boot-2.3-2.4 -\u0026gt; Migrate from Spring Boot 2.3 to 2.4 44) upgrade-boot-1x-to-2x -\u0026gt; Migrate applications built on previous versions of Spring Boot to the latest Spring Boot 2.7 release. OpenRewrite 使用示例 下面使用 maven 方式演示几个例子，注意如果是多模块项目运行可能出错，参考官方说明：Running Rewrite on a multi-module Maven project。\n友情提醒：初次使用会下载很多依赖 jar 包，速度可能比较慢，切换到你最快的 maven 仓库。\n代码清理 先试一下 CodeCleanup 效果。\npom.xml 文件中增加以下配置，然后执行 mvn rewrite:run。\n\u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.openrewrite.maven\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;rewrite-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;5.23.1\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;activeRecipes\u0026gt; \u0026lt;recipe\u0026gt;org.openrewrite.staticanalysis.CodeCleanup\u0026lt;/recipe\u0026gt; \u0026lt;/activeRecipes\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.openrewrite.recipe\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;rewrite-static-analysis\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.3.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt;\r我的某个项目执行效果：\n删除了一些多余的空格、空行、多括号。 代码格式化，比如多个参数间缺少空格的加了空格。 颠倒了一个 equals 比较： method.equals(\u0026quot;CONNECT\u0026quot;) -\u0026gt; \u0026quot;CONNECT\u0026quot;.equals(method)。 长代码换行归整，三元运算格式化。 Spring 到 Spring Boot 下面介绍下我们将 Spring 项目迁移到 Spring Boot 的过程，以及中间遇到的一些问题。\n升级项 迁移前 迁移后 Runtime Spring + Tomcat Spring Boot 3.2 Spring Spring 4 Spring 6 Junit Junit 4 Junit 5 okhttp okhttp 3 okhttp 4 Java Java 8 Java 21 Java EE Java EE Jakarta EE 基本步骤：\n使用 Spring Boot Migrator 将 Spring 项目转换成 Spring Boot，使用 Java 17 启动 spring-boot-migrator 程序并根据提示一步一步操作，如果某个步骤出错，根据 stacktrace 命令查看错误信息，修复后继续 apply \u0026lt;recipe-number\u0026gt;。\n# 使用 Java 17 启动 spring-boot-migrator 程序 $ java -jar spring-boot-migrator.jar Get Started: ------------ Type \u0026#34;help\u0026#34; - to display a list all of the available commands. \u0026#34;scan \u0026lt;dir\u0026gt;\u0026#34; - to scan an application -------------------------------------------------------------------------------------------------- migrator:\u0026gt; scan my-spring-project-dir [ok] Found pom.xml. [ok] \u0026#39;sbm.gitSupportEnabled\u0026#39; is \u0026#39;true\u0026#39;, changes will be committed to branch [springboot] after each recipe. [ok] Found required source dir \u0026#39;src/main/java\u0026#39;. Maven 100% │██████████████████████████████████│ 2/2 (0:00:00 / 0:00:00) Applicable recipes: 1) remove-redundant-maven-compiler-plugin -\u0026gt; Remove standard maven-compiler plugin for applications with boot parent. 2) initialize-spring-boot-migration -\u0026gt; Initialize an application as Spring Boot application. 3) spring-context-xml-import -\u0026gt; Import Spring Framework xml bean configuration into Java configuration without converting them. 4) migrate-spring-xml-to-java-config -\u0026gt; Migrate Spring Framework xml bean configuration to Java configuration. 5) cn-spring-cloud-config-server -\u0026gt; Externalize properties to Spring Cloud Config Server 6) sbu30-set-java-version -\u0026gt; Spring boot 3.0 Upgrade - Set java version property in build file 7) sbu30-add-milestone-repositories -\u0026gt; Spring boot 3.0 Upgrade - Add milestone repository for dependencies and plugins 8) sbu30-225-logging-date-format -\u0026gt; Spring boot 3.0 Upgrade - Logging Date Format Run command \u0026#39;\u0026gt; apply \u0026lt;recipe-number\u0026gt;\u0026#39; to apply a recipe. ibss-site-dc:\u0026gt; apply remove-redundant-maven-compiler-plugin Applying recipe \u0026#39;remove-redundant-maven-compiler-plugin\u0026#39; [ok] Clean up redundant properties for maven compiler plugin. [ok] Clean up redundant maven compiler plugin. remove-redundant-maven-compiler-plugin successfully applied the following actions: (x) Clean up redundant properties for maven compiler plugin. (x) Clean up redundant maven compiler plugin. Applicable recipes: 1) initialize-spring-boot-migration -\u0026gt; Initialize an application as Spring Boot application. 2) spring-context-xml-import -\u0026gt; Import Spring Framework xml bean configuration into Java configuration without converting them. 3) migrate-spring-xml-to-java-config -\u0026gt; Migrate Spring Framework xml bean configuration to Java configuration. 4) cn-spring-cloud-config-server -\u0026gt; Externalize properties to Spring Cloud Config Server 5) sbu30-set-java-version -\u0026gt; Spring boot 3.0 Upgrade - Set java version property in build file 6) sbu30-add-milestone-repositories -\u0026gt; Spring boot 3.0 Upgrade - Add milestone repository for dependencies and plugins 7) sbu30-225-logging-date-format -\u0026gt; Spring boot 3.0 Upgrade - Logging Date Format Run command \u0026#39;\u0026gt; apply \u0026lt;recipe-number\u0026gt;\u0026#39; to apply a recipe. ibss-site-dc:\u0026gt; apply initialize-spring-boot-migration Applying recipe \u0026#39;initialize-spring-boot-migration\u0026#39; [ok] Add Spring Boot dependency management section to buildfile. [ok] Add Spring Boot starter class. [ok] Add initial unit test class to test Spring Boot Application Context startup. [ok] Set packaging to \u0026#39;jar\u0026#39; type if different initialize-spring-boot-migration successfully applied the following actions: (x) Add Spring Boot dependency management section to buildfile. (x) Add spring dependencies \u0026#39;spring-boot-starter\u0026#39; and \u0026#39;spring-boot-starter-test\u0026#39;. (x) Delete dependencies to artifacts transitively managed by Spring Boot. (x) Add Spring Boot Maven plugin. (x) Add Spring Boot starter class. (x) Add initial unit test class to test Spring Boot Application Context startup. (x) Set packaging to \u0026#39;jar\u0026#39; type if different Applicable recipes: 1) spring-context-xml-import -\u0026gt; Import Spring Framework xml bean configuration into Java configuration without converting them. 2) migrate-spring-xml-to-java-config -\u0026gt; Migrate Spring Framework xml bean configuration to Java configuration. 3) cn-spring-cloud-config-server -\u0026gt; Externalize properties to Spring Cloud Config Server 4) boot-2.7-3.0-dependency-version-update -\u0026gt; Bump spring-boot-starter-parent from 2.7.x to 3.0.0 5) boot-2.7-3.0-upgrade-report -\u0026gt; Create a report for Spring Boot Upgrade from 2.7.x to 3.0.0-M3 6) sbu30-report -\u0026gt; Create a report for Spring Boot Upgrade from 2.7.x to 3.0.x 7) sbu30-upgrade-dependencies -\u0026gt; Spring boot 3.0 Upgrade - Upgrade dependencies 8) sbu30-set-java-version -\u0026gt; Spring boot 3.0 Upgrade - Set java version property in build file 9) sbu30-add-milestone-repositories -\u0026gt; Spring boot 3.0 Upgrade - Add milestone repository for dependencies and plugins 10) sbu30-remove-construtor-binding -\u0026gt; Spring boot 3.0 Upgrade - Remove redundant @ConstructorBinding annotations 11) sbu30-225-logging-date-format -\u0026gt; Spring boot 3.0 Upgrade - Logging Date Format 12) sbu30-upgrade-spring-cloud-dependency -\u0026gt; Upgrade Spring Cloud Dependencies 13) sbu30-upgrade-boot-version -\u0026gt; Spring boot 3.0 Upgrade - Upgrade Spring Boot version Spring Boot Migrator 目前还并不成熟，以上步骤中很多 recipe 可能出错，所以我只用 Spring Boot Migrator 做一些基础初始化，其他任务使用 rewrite-maven-plugin，交给 openrewrite 执行。在 maven pom.xml 中增加以下配置后，执行 mvn rewrite:run。\n\u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.openrewrite.maven\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;rewrite-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;5.19.0\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;activeRecipes\u0026gt; \u0026lt;recipe\u0026gt;org.openrewrite.java.spring.boot2.UpgradeSpringBoot_2_7\u0026lt;/recipe\u0026gt; \u0026lt;recipe\u0026gt;org.openrewrite.java.testing.junit5.JUnit4to5Migration\u0026lt;/recipe\u0026gt; \u0026lt;recipe\u0026gt;org.openrewrite.okhttp.UpgradeOkHttp4\u0026lt;/recipe\u0026gt; \u0026lt;recipe\u0026gt;org.openrewrite.okhttp.ReorderRequestBodyCreateArguments\u0026lt;/recipe\u0026gt; \u0026lt;recipe\u0026gt;org.openrewrite.java.migrate.UpgradeToJava17\u0026lt;/recipe\u0026gt; \u0026lt;/activeRecipes\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.openrewrite.recipe\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;rewrite-spring\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;5.2.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.openrewrite.recipe\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;rewrite-testing-frameworks\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.2.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.openrewrite.recipe\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;rewrite-okhttp\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.1.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.openrewrite.recipe\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;rewrite-migrate-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.5.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/plugin\u0026gt;\r[INFO] Using active recipe(s) [org.openrewrite.java.spring.boot2.UpgradeSpringBoot_2_7, org.openrewrite.java.testing.junit5.JUnit4to5Migration] [INFO] Using active styles(s) [] [INFO] Validating active recipes... [INFO] Project [ibss-site-dc] Resolving Poms... [INFO] Project [ibss-site-dc] Parsing source files [INFO] Running recipe(s)... [WARNING] Changes have been made to pom.xml by: [WARNING] org.openrewrite.java.spring.boot2.UpgradeSpringBoot_2_7 [WARNING] org.openrewrite.java.spring.boot2.UpgradeSpringBoot_2_6 [WARNING] org.openrewrite.java.spring.boot2.UpgradeSpringBoot_2_5 [WARNING] org.openrewrite.java.spring.boot2.UpgradeSpringBoot_2_4 [WARNING] org.openrewrite.java.spring.boot2.SpringBoot2JUnit4to5Migration [WARNING] org.openrewrite.java.testing.junit5.JUnit4to5Migration [WARNING] org.openrewrite.maven.ExcludeDependency: {groupId=junit, artifactId=junit} [WARNING] org.openrewrite.java.dependencies.UpgradeDependencyVersion: {groupId=org.springframework.boot, artifactId=*, newVersion=2.7.x, overrideManagedVersion=false} [WARNING] Changes have been made to src/main/java/com/fsde/ibss/dc/controller/ClientController.java by: [WARNING] org.openrewrite.java.spring.boot2.UpgradeSpringBoot_2_7 [WARNING] org.openrewrite.java.spring.boot2.UpgradeSpringBoot_2_6 [WARNING] org.openrewrite.java.spring.boot2.UpgradeSpringBoot_2_5 [WARNING] org.openrewrite.java.spring.boot2.UpgradeSpringBoot_2_4 [WARNING] org.openrewrite.java.spring.boot2.UpgradeSpringBoot_2_3 [WARNING] org.openrewrite.java.spring.boot2.UpgradeSpringBoot_2_2 [WARNING] org.openrewrite.java.spring.boot2.UpgradeSpringBoot_2_1 [WARNING] org.openrewrite.java.spring.boot2.UpgradeSpringBoot_2_0 [WARNING] org.openrewrite.java.spring.boot2.SpringBoot2BestPractices [WARNING] org.openrewrite.java.spring.NoRequestMappingAnnotation [WARNING] org.openrewrite.java.spring.ImplicitWebAnnotationNames [WARNING] Changes have been made to src/test/java/com/fsde/ibss/dc/service/FileSaveTest.java by: [WARNING] org.openrewrite.java.spring.boot2.UpgradeSpringBoot_2_7 [WARNING] org.openrewrite.java.spring.boot2.UpgradeSpringBoot_2_6 [WARNING] org.openrewrite.java.spring.boot2.UpgradeSpringBoot_2_5 [WARNING] org.openrewrite.java.spring.boot2.UpgradeSpringBoot_2_4 [WARNING] org.openrewrite.java.spring.boot2.SpringBoot2JUnit4to5Migration [WARNING] org.openrewrite.java.testing.junit5.JUnit4to5Migration [WARNING] org.openrewrite.java.testing.junit5.UpdateTestAnnotation [WARNING] org.openrewrite.java.spring.boot2.RemoveObsoleteSpringRunners [WARNING] org.openrewrite.java.testing.junit5.JUnit5BestPractices [WARNING] org.openrewrite.java.testing.cleanup.RemoveTestPrefix [WARNING] org.openrewrite.java.testing.cleanup.TestsShouldNotBePublic [WARNING] Please review and commit the results. [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ 在上面步骤我是先升级到了 Spring Boot 2，然后再执行升级 Spring Boot 3 和 Jakarta EE。实际上直接升级到 Spring Boot 3 也是可以的。\n执行完成后可从 maven 中删除 rewrite-maven-plugin，也可留着下次做其他变更。\n试用总结 注意事项：\n我们项目配置都是通过远程配置中心管理的，所以升级过程中配置文件无法响应变更，解决办法是将配置从远端拷贝到本地 applicaiton.properties 中，执行完任务后根据变化再更新远程配置中心。 执行过程中可能会报错，比如 maven 插件升级后某些属性可能变更或失效，需要手动解决错误后，再执行 mvn rewrite:run， 可重复执行 mvn rewrite:run 命令，可随时加入新的 recipe 再执行，所以为了观察变更和保证执行成功，可分批次加入 recipe 多次执行。 OpenRewrite 不解析依赖的第三方 jar 包内容，所以第三方 jar 的问题需要自行识别，比如从 Java EE 到 Jakarta EE，只是替换规则中知道的 jar 包版本和当前项目的源代码，不会自动把 jar 包内的 javax. 替换为 jakarta.。 遇见问题和解决办法 ","date":"2024-01-09","id":29,"permalink":"/docs/platform/smart-code/","summary":"作为一个以 Java 和 Spring 为主要技术栈的团队，在日常的软件开发中，我们经常会遇到一系列的组件升级和代码重构需求，在此过程我们期望能做到几个效果：\n项目级升级：整个项目（注此项目指 Maven Project）升级，而不是基于某个 Java 类或片段。 不同版本跨度变更：比如从 Spring 项目迁移到 Spring Boot，从 Spring Boot 2.x 升级到 3.x，从 Java 8 升级到 Java 21。 代码安全可靠：变更的代码一定是正确的，至少是逻辑正确的，至少不能像某 AI 助手一样设置一些根本不存在的属性。 经验产品化：最佳实践就是产品，比如我并不（想）了解 Spring Boot 3.2 具体有哪些变更，但希望能一键从 3.0 自动升级到 3.2，直接告诉有哪些变更。 自定义重构：对于某些自研代码，希望能自定义重构逻辑，一键自动重构。 基于以上背景，我们探索了 OpenRewrite、Spring Boot Migrator、Redhat Windup 和一众不便具名的 AI 代码助手，本文将分享我们使用 OpenRewrite、Spring Boot Migrator 进行代码重构和升级的一些使用经验和体验。\nOpenRewrite OpenRewrite 是一个开源的代码重写工具，旨在帮助开发人员自动化大规模重构代码。\n它提供了一套强大的 API 和插件系统，可以通过静态分析和代码转换技术来解析、修改和生成代码。OpenRewrite 支持多种编程语言，包括 Java、C#、 TypeScript、Python、Kubernetes 等。通过使用 OpenRewrite，开发人员可以轻松地进行代码重构、性能优化、代码风格调整和代码迁移等操作，从而提高代码质量和可维护性。OpenRewrite 的开源性质使得开发人员可以自由地定制和扩展其功能，以满足特定项目的需求。\n比如 Java 领域一些热门的应用场景：\nJava 版本升级：从 Java 8 到 Java 17，从 Java EE 到 Jakarta EE 。 Spring 框架迁移：从 Spring 5 到 Spring 6，从 Spring Boot 2 到 Spring Boot 3。 测试框架迁移： 从 Junit 4 到 Junit 5。 依赖管理：自动更新 Java 项目的 Maven 或 Gradle 依赖，确保使用最新和最安全的库版本。 代码清理和格式化：自动清理和格式化 Java 代码，确保符合项目或组织的编码标准和风格指南。 修复安全漏洞：自动识别和修复 Java 代码中的已知安全漏洞，如使用了有安全问题的库或方法。 代码异味检测和修复：识别并自动重构 Java 代码中的“代码异味”，以提高代码可维护性。 相比于时下火热的 AI 代码工具，比如 GitHub Copilot、Amazon CodeWhisperer，我认为 OpenRewrite 的优势主要有以下几点：","tags":[],"title":"智能代码重构"},{"content":"","date":"2023-09-07","id":30,"permalink":"/docs/","summary":"","tags":[],"title":"Docs"},{"content":"","date":"2024-03-09","id":31,"permalink":"/categories/","summary":"","tags":[],"title":"Categories"},{"content":"","date":"2024-03-09","id":32,"permalink":"/contributors/","summary":"","tags":[],"title":"Contributors"},{"content":"","date":"2024-03-09","id":33,"permalink":"/tags/java/","summary":"","tags":[],"title":"Java"},{"content":"","date":"2024-03-09","id":34,"permalink":"/categories/java/","summary":"","tags":[],"title":"Java"},{"content":"","date":"2024-03-09","id":35,"permalink":"/contributors/l10178/","summary":"","tags":[],"title":"L10178"},{"content":"","date":"2024-03-09","id":36,"permalink":"/tags/spring-boot/","summary":"","tags":[],"title":"Spring Boot"},{"content":"","date":"2024-03-09","id":37,"permalink":"/categories/spring-boot/","summary":"","tags":[],"title":"Spring Boot"},{"content":"","date":"2024-03-09","id":38,"permalink":"/tags/","summary":"","tags":[],"title":"Tags"},{"content":"","date":"2024-02-26","id":39,"permalink":"/tags/tools/","summary":"","tags":[],"title":"Tools"},{"content":"","date":"2023-09-07","id":40,"permalink":"/privacy/","summary":"","tags":[],"title":"Privacy Policy"},{"content":"","date":"2023-09-07","id":41,"permalink":"/","summary":"","tags":[],"title":"卫星实验室"},{"content":"","date":"2023-09-07","id":42,"permalink":"/tags/k8s/","summary":"","tags":[],"title":"K8s"},{"content":"","date":"2023-09-07","id":43,"permalink":"/categories/k8s/","summary":"","tags":[],"title":"K8s"},{"content":"","date":"2023-08-07","id":44,"permalink":"/tags/envoy/","summary":"","tags":[],"title":"Envoy"},{"content":"","date":"2022-09-07","id":45,"permalink":"/tags/devops/","summary":"","tags":[],"title":"Devops"}]