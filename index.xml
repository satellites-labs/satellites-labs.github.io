<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>卫星实验室 on XLabs</title><link>https://www.xlabs.club/</link><description>Recent content in 卫星实验室 on XLabs</description><generator>Hugo</generator><language>zh</language><copyright>Copyright (c) 2020-2024 XLabs Club</copyright><lastBuildDate>Wed, 21 Aug 2024 20:41:31 +0800</lastBuildDate><atom:link href="https://www.xlabs.club/index.xml" rel="self" type="application/rss+xml"/><item><title>Introduction</title><link>https://www.xlabs.club/docs/cloud/introduction/</link><pubDate>Thu, 07 Sep 2023 16:04:48 +0200</pubDate><guid>https://www.xlabs.club/docs/cloud/introduction/</guid><description>云原生技术探索。</description></item><item><title>Introduction</title><link>https://www.xlabs.club/docs/guides/introduction/</link><pubDate>Thu, 07 Sep 2023 16:04:48 +0200</pubDate><guid>https://www.xlabs.club/docs/guides/introduction/</guid><description>卫星实验室，一个专注于研究 CRM（Customer Relationship Management，客户关系管理） 的开源组织。
此项目为卫星实验室主页 xlabs.club 的源码，在这里我们将分享 CRM 领域独有的建设经验，介绍如何以技术驱动 CRM 长期发展和高速增长。
加入我们，与我们共同共同探索 CRM 技术前沿，解决行业中的挑战。
主页内容 平台工程：我们的平台工程建设之路，关于 DevOps, DataOps, FinOps 以及 AIOps 的工程实践。 云原生：云原生技术探索，如何以云原生技术支撑起不断变化的复杂业务。 技术博客：研发踩坑记录，翻一翻总有惊喜。 awesome-x-ops：一些关于 AIOps、DataOps、DevOps、GitOps、FinOps 的优秀软件、博客、配套工具。 xlabs-ops：一些运维脚本和模板，如 Argo Workflows 模板仓库，是对官方 Examples 的组合、扩展。 License 本文档采用 CC BY-NC 4.0 许可协议。</description></item><item><title>Kubernetes</title><link>https://www.xlabs.club/docs/tldr/kubernetes/</link><pubDate>Thu, 07 Sep 2023 16:04:48 +0200</pubDate><guid>https://www.xlabs.club/docs/tldr/kubernetes/</guid><description>常用 Kubernetes 命令，复制，粘贴，这就是生活。
复制 secret 到另一个 namespace。 kubectl get secret mys --namespace=na -oyaml | grep -v &amp;#39;^\s*namespace:\s&amp;#39; | kubectl apply --namespace=nb -f -批量删除 pod。 kubectl get pods --all-namespaces | grep Evicted | awk &amp;#39;{print $2 &amp;#34; --namespace=&amp;#34; $1}&amp;#39; | xargs kubectl delete pod # Delete by label kubectl delete pod -n idaas-book -l app.kubernetes.io/name=idaas-book原地重启 Pod。 kubectl rollout restart deploy/xxx -n your-namespace命令行快速扩缩容。 # kubectl scale -h kubectl scale --replicas=1 deploy/xxx -n your-namespace密钥解密。 kubectl get secret my-creds -n mysql -o jsonpath=&amp;#34;{.</description></item><item><title>总体架构</title><link>https://www.xlabs.club/docs/platform/introduction/</link><pubDate>Thu, 07 Sep 2023 16:04:48 +0200</pubDate><guid>https://www.xlabs.club/docs/platform/introduction/</guid><description>我们的平台工程建设之路，介绍前期方案设计、中间踩坑历程。
原则 分享我们平台工程建设的一些基本原则。
以开发者为中心：赋能开发者，了解困难，解决问题，让开发者生活更轻松。 自动化：自动化手动和重复性任务，减少人为错误，提高效率。 标准化：标准化保持一致性，减少复杂性，减少团队认知负载，提供最佳实践和统一的编码结构。 模块化：松耦合且独立的模块，可独立开发、测试和部署。 弹性：可扩展水平扩缩容的能力，以及容错抗脆弱的能力。 安全：相比于微服务、云原生领域的安全，在平台工程里，更强调代码、基础设施、数据和其他资源的安全。 协作：平台工程师、开发人员、运维运营人员以及其他参与者之间的协作，提高生产力、促进创新并创造积极包容的工作环境。 持续改进：持续性反馈、评估、改进。 架构概述 为便于理解，我们仍然按照惯用架构模型，将架构分为 IaaS、CaaS、PaaS、Applications 这几个层级。
专业的运维人员作为 platform engineer 着重于 IaaS、CaaS、PaaS 建设，开发人员作为 application engineer 更专注于 PaaS、Applications 建设，为开发和运维提供工具、协作平台、基础应用。
C4Context title 平台工程总体架构 Boundary(users, "Users", "用户接入") { Person(superAdmin, "超级管理员") Person(admin, "平台管理员") Person(developer, "开发人员") Person(maintenance, "运维人员") } Boundary(console, "Console", "开发者平台") { Container(backstage, "Backstage","react","开发者门户") Container(apps, "应用管理平台","Application","容器管理、应用管理、配置管理、自动化测试") Container(ops, "统一运维平台","x-ops","数据库、中间件、日志、监控告警平台") Container(iam, "IAM", "Keycloak", "统一用户、组织、角色权限管理") } Boundary(paas, "PaaS", "PaaS") { ContainerDb(rds, "RDS", "PostgreSQL/MySQL", "PostgreSQL、MySQL 等关系型数据库") ContainerDb(clickhouse, "ClickHouse", "ClickHouse", "BI、Logging、Metrics 等列式数据库"</description></item><item><title>统一身份认证</title><link>https://www.xlabs.club/docs/platform/iam/</link><pubDate>Tue, 19 Dec 2023 22:26:42 +0800</pubDate><guid>https://www.xlabs.club/docs/platform/iam/</guid><description>统一身份认证（Identity and Access Management，身份认证和访问控制，简称 IAM）的技术选型和实践。
核心需求 集中管理：从一个地方管理账户和身份。 单点登录：允许用户使用一组凭据访问所有集成的系统和应用，避免记忆多个用户名和密码。 动态访问控制：基于角色和策略动态授予或撤销访问权限。 审计与合规：记录和监控访问活动，以支持合规性审计。 无缝快速集成：作为平台工程的一部分更强调“自助”，各个应用能够无缝快速接入，甚至有些应用只需要简单的权限能够不需要开发自动接入。 强化认证机制：采用多因素认证（MFA）等方法，确保只有授权用户才能访问系统和数据。 技术选项 为满足以上需求，在初期技术选项时主要关注以下几个开源组件。
keycloak: 全面的 IAM 解决方案 ，实现用户、权限管理，单点登录、MFA 等。 Dex: 身份代理，连接多个身份源，仅作为 OpenID Connect。 Ory: 包含多个独立的组件，组成一个全家桶的解决方案。 oauth2-proxy: 反向代理工具，专为提供 OAuth 2.0 身份验证和授权服务而设计，附带基于用户、分组、角色的权限管理。 Pomerium: Pomerium 不仅仅是一个 OAuth 2.0 代理，它还提供了细粒度的访问控制，能够根据用户、组、和其他上下文属性来决定访问权限。 以下为 keycloak 和 Dex 的简单对比。为什么不把 Ory 加进来，因为没有实际用过，不便于发表意见，如果你是一个 Ory 用户欢迎补充。
特性/工具 Keycloak Dex 类型 全面的 IAM 解决方案 身份代理 用户管理 支持内置用户管理 不直接管理用户，依赖外部身份提供者 协议支持 OpenID Connect、OAuth 2.0、SAML 2.0 OpenID Connect SSO 支持 依赖外部身份提供者实现 社交登录 支持多种社交登录选项 不直接支持，可通过连接外部身份提供者实现 角色管理 支持复杂的角色和权限管理 不直接支持 扩展性 高，适合各种规模和复杂性的需求 适合将多个身份源统一到一个认证流程的环境 使用场景 需要全面、集中式身份管理的组织 需要统一多个身份源认证，如在云原生环境中 用户界面 提供丰富的用户和管理员界面 主要是 API，没有详细的用户界面 适用性 适用于需要完整 IAM 解决方案的组织 适用于作为多个身份源代理，尤其在 Kubernetes 环境中 以下为 OAuth2 Proxy 和 Pomerium 的简单对比。</description></item><item><title>Koupleless 试用报告总结，踩坑记录分享</title><link>https://www.xlabs.club/blog/koupleless-first-boot/</link><pubDate>Mon, 27 May 2024 14:20:24 +0800</pubDate><guid>https://www.xlabs.club/blog/koupleless-first-boot/</guid><description>我们公司的主要应用都是以 Java 作为开发语言，这几年随着业务的高速增长，应用数目越来越多，CPU 内存资源占用越来越多，项目组之间开发合作效率也越来越低。
顺应这个时代降本增效的目的，我们希望寻找一个能解决当前几个核心问题的框架：
模块化开发、部署、资源共享的能力，减少 Cache、Class 等资源占用，有效降低内存占用。 更快更轻的依赖，应用能够更快的启动。 能够让各个项目组不改代码或少改代码即可接入，控制开发迁移的成本，毕竟很多历史老应用不是那么容易迁移。 基于以上背景，我们在 2022 年基于 SOFAArk 运行了一个版本，效果不太理想暂时搁置。今年 Koupleless 重新开源后做了一些增强和变更，开源社区活跃度有所提升，看宣传效果很好，我们决定重新评估是否可在公司内推广。
什么是 Koupleless Koupleless 是一种模块化 Serverless 技术解决方案，它能让普通应用低成本演进为 Serverless 研发模式，让代码与资源解耦，轻松独立维护， 与此同时支持秒级构建部署、合并部署、动态伸缩等能力为用户提供极致的研发运维体验，最终帮助企业实现降本增效。
Koupleless 是蚂蚁集团内部经过 5 年打磨成熟的研发框架和运维调度平台能力，相较于传统镜像化的应用模式研发、运维、运行阶段都有 10 倍左右的提升，总结起来 5 大特点：快、省、灵活部署、平滑演进、生产规模化验证。
以上都是官网的宣传，更多介绍请链接到官网查看。
在整个 Koupleless 平台里，需要四个组件：
研发工具 Arkctl, 提供模块创建、快速联调测试等能力。 运行组件 SOFAArk, Arklet，提供模块运维、模块生命周期管理，多模块运行环境。（这算两个组件？） 控制面组件 ModuleController，本质上是一个 K8S Operator，提供模块发布与运维能力。 我们公司有自己的发布系统、应用管理平台，很少允许运行额外的控制面组件，那么除去 ModuleController，我个人认为，Koupleless 约等于 SOFAArk。
Koupleless 增强了 SOFAArk 运维部署相关的功能，解决了 SOFAArk 在企业内无法开箱即用的问题。
应用接入遇见问题 基于官方文档我们改造接入了几个应用，分享几个我们遇见的问题。
对 Java 17 或 21 的支持不好。 虽然官方已经声称支持 Java 17，但是若用了 Java 17 的语法或新特性，无法编译通过。最后只好自编译 SOFAArk plugin 修改相关依赖解决。</description></item><item><title>容器镜像制作最佳实践，Dockerfile 实践经验和踩坑记录</title><link>https://www.xlabs.club/blog/docker-best-practices/</link><pubDate>Fri, 24 May 2024 20:56:08 +0800</pubDate><guid>https://www.xlabs.club/blog/docker-best-practices/</guid><description>整理了由 Docker 官方和社区推荐的用于构建高效镜像的最佳实践和方法，当然有些可能并不适用于你，请注意分辨。
使用官方镜像作为基础镜像。官方镜像经过了充分验证并集成了最佳实践。
# 正例： FROM node # 反例： FROM ubuntu RUN apt-get install -y node保持尽可能小的镜像大小，绝不安装无关依赖。
严格的版本化管理，使用确定性的标签，基础镜像禁用 latest。
使用 .dockerignore 文件排除文件干扰。
最经常变化的命令越往后执行，充分利用分层缓存机制。
Dockerfile 中每行命令产生一层，请合并命令执行，最大限度减少层数。
使用多阶段构建，减少所构建镜像的大小。
禁用 root 用户，使用独立的 use 和 group。
启用镜像安全扫描，并及时更新。
一个容器只专注做一件事情。
Java 应用程序不要使用 PID 为 1 的进程，使用 tini 或 dump-init 管理进程，避免僵尸进程。
以上都是一些基本的原则，但是实际工作的过程中，大家可能会像我一样纠结几个问题。
关于第 1 点，一定要使用官方镜像吗。未必，看情况。比如我们作为平台，涉及很多种开发语言，很多种组合场景，每个官方基础镜像可能都不同，就会自建基础镜像，以便统一操作系统、统一脚本和安全维护。为什么要统一操作系统，操作系统投的毒，就像出骨鱼片里未净的刺，给人一种不期待的伤痛。 为了镜像大小和安全，一定要使用 Alpine 或 distroless 镜像吗。我的建议是不要使用 Alpine 镜像，如有能力才使用 distroless 镜像。毕竟 libc 的坑，谁痛谁知道。 我们的镜像策略 Dockerfile 编写小技巧 使用 Heredocs 语法代替又长又臭的字符串拼接，当然 Heredocs 支持更多功能比如 run python、多文件内容拷贝，以下举例只是最常用的。</description></item><item><title>使用 Pulumi 部署 cert-manager 创建 K8S 自签名证书并信任证书</title><link>https://www.xlabs.club/blog/trust-cert-manager-selfsigned-tls/</link><pubDate>Mon, 29 Apr 2024 21:49:22 +0800</pubDate><guid>https://www.xlabs.club/blog/trust-cert-manager-selfsigned-tls/</guid><description>在搭建本地 Kubernetus 集群后，由于环境在内网，做不了域名验证，无法使用 Let&amp;rsquo;s Encrypt 颁发和自动更新证书，然而很多应用要求必须启用 HTTPS，只能用自签名 CA 证书，并由此 CA 继续颁发其他证书。
所以我们准备了以下工具，开始搭建。
Pulumi: 当前非常流行的 IaC 工具，值得一试。 cert-manager: 云原生证书管理，用于自动管理和颁发各种发行来源的 TLS 证书。它将确保证书有效并定期更新，并尝试在到期前的适当时间更新证书。 核心步骤和相关代码如下，更多源码请参考我们的 GitHub 项目 xlabs-ops。
使用 Pulumi 安装 cert-manager，生成自签名 CA 证书，根据自签名 CA 证书生成 cert-manager ClusterIssuer，都在如下代码了。
import * as pulumi from &amp;#34;@pulumi/pulumi&amp;#34;; import * as kubernetes from &amp;#34;@pulumi/kubernetes&amp;#34;; import * as tls from &amp;#34;@pulumi/tls&amp;#34;; // 部署 cert-manager Helm chart const certManagerRelease = new kubernetes.helm.v3.Release(&amp;#34;cert-manager&amp;#34;, { name: &amp;#34;cert-manager&amp;#34;, chart: &amp;#34;cert-manager&amp;#34;, version: &amp;#34;1.14.5&amp;#34;, namespace: &amp;#34;cert-manager&amp;#34;, createNamespace: true, timeout: 600, repositoryOpts: { repo: &amp;#34;https://charts.</description></item><item><title>Mac 搭建本地 K8S 开发环境方案选型</title><link>https://www.xlabs.club/blog/easiest-k8s-on-macos/</link><pubDate>Sat, 13 Apr 2024 15:20:43 +0800</pubDate><guid>https://www.xlabs.club/blog/easiest-k8s-on-macos/</guid><description>因为工作经常需要用到 K8S，而且有时因网络原因不能完全依赖公司网络，或者因为测试新功能不能直接发布到公司集群，所以就有了本地搭建 K8S 的需求。
另外如果你有以下需求，此文档中提到的方案也许有所帮助：
开发机器模拟 Arm、AMD64 等不同架构。 完全隔离的不同环境，比如为测试 docker、podman、buildkit、containd 等不同软件设置的独立环境。 CI/CD 流程中即用即消的轻量级虚拟机替代方案。 有限的资源模拟大批量的 K8S 节点。 以下介绍一下我用过的几种不同方案，有些纯属个人观点仅供参考。
Docker Desktop 并启用 Kubernetes 功能。
优点：最简单，开箱即用。
缺点：只支持单节点 K8S，且 K8S 部分功能不支持，不易定制。
Docker run K3D, K3D run K3S。
优点：简单，任何支持 docker 的工具（Rancher Desktop、Podman） 启动一个容器即可。
缺点：只支持 K3S。
multipass 启动虚拟机安装 K8S 或 K3S。
优点：multipass 可启动空白 ubuntu 虚拟机，或者启动已经安装好 minikube 的虚拟机。
缺点：只支持 ubuntu，虚拟机与宿主机同架构。
lima 启动虚拟机安装 K8S 或 K3S。
优点：支持虚拟多种 Linux，支持异构虚拟机，支持 contained 可代替 docker。
缺点：架构稍复杂，启动略慢，不如 multipass 稳定，不支持运行在 Windows。
以上方案，在网络畅通的情况下，均能在 10 分钟内启动一个单节点 K8S，所以整体方案都不复杂。</description></item><item><title>K8S 服务长连接负载不均衡问题分析和解决办法</title><link>https://www.xlabs.club/blog/tomcat-keepalive-load-balancer/</link><pubDate>Thu, 11 Apr 2024 21:05:46 +0800</pubDate><guid>https://www.xlabs.club/blog/tomcat-keepalive-load-balancer/</guid><description>问题背景，我们有一个 Http 服务在 K8S 内部署了 3 个 Pod，客户端使用 Service NodePort 进行连接，发现流量几乎都集中到了一个 Pod 上。
已知的情况是：
K8S Service 使用 round-robin 负载均衡策略。 客户端和服务端都启用了 Keep-Alive 长连接。 经过抓包分析，负载较高的 Pod 保持着较多 KeepAlive 长连接。将 kube-proxy 的 ipvs 转发模式设置为 Least-Connection，即倾向转发给连接数少的 Pod，可能会有所缓解，但也不一定，因为 ipvs 的负载均衡状态是分散在各个节点的，并没有收敛到一个地方，也就无法在全局层面感知哪个 Pod 上的连接数少，并不能真正做到 Least-Connection。
服务端主动要求断开长连接 客户端连接我们可能无法控制，那么如何从服务端主动断开长连接。
以 Tomcat 为例，它提供了 maxKeepAliveRequests 参数，到达此参数阈值后，Tomcat 会在 Response Header 中主动加一个 Connection: close，正常情况下客户端接收到此响应后会主动断开长连接。
对于其他不支持此参数的服务器，可以自定义 Filter 或者自定代码，到达某阈值后在 Response Header 中主动追加 Connection: close。
对于 Spring Boot 可通过 properties 配置。
# Spring Boot Tomcat server.tomcat.max-keep-alive-requests=100 # Spring Boot WebFlux server.</description></item></channel></rss>